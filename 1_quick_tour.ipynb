{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "ä½¿ç”¨ pipeline() æ˜¯åˆ©ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•çš„æ–¹å¼ã€‚ä½ èƒ½å¤Ÿå°† pipeline() å¼€ç®±å³ç”¨åœ°ç”¨äºè·¨ä¸åŒæ¨¡æ€çš„å¤šç§ä»»åŠ¡ã€‚æ¥çœ‹çœ‹å®ƒæ”¯æŒçš„ä»»åŠ¡åˆ—è¡¨ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| ä»»åŠ¡                 | æè¿°                                                         | æ¨¡æ€        | Pipeline æ ‡è¯†ç¬¦                                                                                   |\n",
    "|----------------------|--------------------------------------------------------------|-------------|------------------------------------------------------------------------------------------------------|\n",
    "| æ–‡æœ¬åˆ†ç±»            | ä¸ºç»™å®šçš„æ–‡æœ¬åºåˆ—æŒ‡å®šæ ‡ç­¾                                     | è‡ªç„¶è¯­è¨€å¤„ç† | `pipeline(task=\"sentiment-analysis\")`                                                                 |\n",
    "| æ–‡æœ¬ç”Ÿæˆ            | åœ¨ç»™å®šæç¤ºçš„æƒ…å†µä¸‹ç”Ÿæˆæ–‡æœ¬                                 | è‡ªç„¶è¯­è¨€å¤„ç† | `pipeline(task=\"text-generation\")`                                                                   |\n",
    "| æ€»ç»“                | ç”Ÿæˆæ–‡æœ¬æˆ–æ–‡æ¡£åºåˆ—çš„æ‘˜è¦                                     | è‡ªç„¶è¯­è¨€å¤„ç† | `pipeline(task=\"summarization\")`                                                                     |\n",
    "| å›¾åƒåˆ†ç±»            | ä¸ºå›¾åƒæŒ‡å®šæ ‡ç­¾                                               | è®¡ç®—æœºè§†è§‰   | `pipeline(task=\"image-classification\")`                                                               |\n",
    "| å›¾åƒåˆ†å‰²            | ä¸ºå›¾åƒçš„æ¯ä¸ªåƒç´ åˆ†é…æ ‡ç­¾ï¼ˆæ”¯æŒè¯­ä¹‰ã€å…¨æ™¯å’Œå®ä¾‹åˆ†å‰²ï¼‰       | è®¡ç®—æœºè§†è§‰   | `pipeline(task=\"image-segmentation\")`                                                                 |\n",
    "| å¯¹è±¡æ£€æµ‹            | é¢„æµ‹å›¾åƒä¸­å¯¹è±¡çš„è¾¹ç•Œæ¡†å’Œç±»åˆ«                                 | è®¡ç®—æœºè§†è§‰   | `pipeline(task=\"object-detection\")`                                                                   |\n",
    "| éŸ³é¢‘åˆ†ç±»            | ä¸ºæŸäº›éŸ³é¢‘æ•°æ®åˆ†é…æ ‡ç­¾                                       | éŸ³é¢‘         | `pipeline(task=\"audio-classification\")`                                                               |\n",
    "| è‡ªåŠ¨è¯­éŸ³è¯†åˆ«        | å°†è¯­éŸ³è½¬å½•ä¸ºæ–‡æœ¬                                             | éŸ³é¢‘         | `pipeline(task=\"automatic-speech-recognition\")`                                                       |\n",
    "| è§†è§‰é—®é¢˜å›ç­”        | å›ç­”ä¸€ä¸ªå…³äºå›¾åƒçš„é—®é¢˜ï¼Œç»™å‡ºä¸€ä¸ªå›¾åƒå’Œä¸€ä¸ªé—®é¢˜               | Multimodalå¤šå¼         | `pipeline(task=\"vqa\")`                                                                               |\n",
    "| æ–‡æ¡£é—®ç­”            | å›ç­”ä¸€ä¸ªå…³äºæ–‡æ¡£çš„é—®é¢˜ï¼Œç»™å‡ºä¸€ä¸ªæ–‡æ¡£å’Œä¸€ä¸ªé—®é¢˜               | Multimodalå¤šå¼         | `pipeline(task=\"document-question-answering\")`                                                       |\n",
    "| å›¾åƒå­—å¹•            | ä¸ºç»™å®šå›¾åƒç”Ÿæˆå­—å¹•                                           | Multimodalå¤šå¼         | `pipeline(task=\"image-to-text\")`                                                                      |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥å°†[pipeline()](https://huggingface.co/docs/transformers/v4.44.2/zh/main_classes/pipelines#transformers.pipeline)ç”¨äºä»»ä½•ä¸€ç§ä¸Šé¢æåˆ°çš„ä»»åŠ¡ï¼Œå¦‚æœæƒ³çŸ¥é“æ”¯æŒçš„ä»»åŠ¡çš„å®Œæ•´åˆ—è¡¨ï¼Œå¯ä»¥æŸ¥é˜…[pipeline API](https://huggingface.co/docs/transformers/v4.44.2/zh/main_classes/pipelines)ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª pipeline() å®ä¾‹å¹¶ä¸”æŒ‡å®šä½ æƒ³è¦å°†å®ƒç”¨äºçš„ä»»åŠ¡(`Task`)ã€‚åœ¨è¿™ç¯‡æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°† pipeline() ç”¨åœ¨ä¸€ä¸ªæƒ…æ„Ÿåˆ†æä»»åŠ¡(`task=\"sentiment-analysis\"`)ä¸Šä½œä¸ºç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\OneDrive\\WorkSpaces\\PythonProjects\\transformers-practice\\transformers-playground-windows-ctv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision af0f99b (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From e:\\OneDrive\\WorkSpaces\\PythonProjects\\transformers-practice\\transformers-playground-windows-ctv\\lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\OneDrive\\WorkSpaces\\PythonProjects\\transformers-practice\\transformers-playground-windows-ctv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'POSITIVE', 'score': 0.9997795224189758}]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec(\"tf-keras\") is None:\n",
    "    raise ImportError(f\"'tf-keras' is not installed. Please install it before running this script.\")\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# pipeline() ä¼šä¸‹è½½å¹¶ç¼“å­˜ä¸€ä¸ªç”¨äºæƒ…æ„Ÿåˆ†æçš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "\n",
    "# åœ¨ç›®æ ‡æ–‡æœ¬ä¸Šä½¿ç”¨ classifier\n",
    "classifier(\"We are very happy to show you the ğŸ¤— Transformers library.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¹Ÿå¯ä»¥æŠŠå¤šä¸ªè¾“å…¥æ”¾å…¥ä¸€ä¸ªåˆ—è¡¨ç„¶åä¼ ç»™pipeline()ï¼Œå®ƒå°†ä¼šè¿”å›ä¸€ä¸ªå­—å…¸åˆ—è¡¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label: POSITIVE, with score: 0.9998\n",
      "label: NEGATIVE, with score: 0.5309\n"
     ]
    }
   ],
   "source": [
    "results = classifier([\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"])\n",
    "for result in results:\n",
    "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline() ä¹Ÿå¯ä»¥éå†æ•´ä¸ªæ•°æ®é›†ã€‚åœ¨ä¸‹é¢è¿™ä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬é€‰æ‹©è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ä½œä¸ºä»»åŠ¡, åŠ è½½ [MInDS-14](https://huggingface.co/datasets/PolyAI/minds14) éŸ³é¢‘æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at facebook/wav2vec2-base-960h were not used when initializing Wav2Vec2ForCTC: ['wav2vec2.encoder.pos_conv_embed.conv.weight_g', 'wav2vec2.encoder.pos_conv_embed.conv.weight_v']\n",
      "- This IS expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Wav2Vec2ForCTC from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1', 'wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Generating train split: 563 examples [00:00, 6469.16 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec(\"soundfile\") is None:\n",
    "    raise ImportError(f\"'soundfile' is not installed. Please install it before running this script.\")\n",
    "\n",
    "speech_recognizer = pipeline(\"automatic-speech-recognition\", model=\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "from datasets import load_dataset, Audio\n",
    "dataset = load_dataset(\"PolyAI/minds14\", name=\"en-US\", split=\"train\", trust_remote_code=True)\n",
    "\n",
    "# ä½ éœ€è¦ç¡®ä¿æ•°æ®é›†ä¸­éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸ facebook/wav2vec2-base-960h è®­ç»ƒç”¨åˆ°çš„éŸ³é¢‘çš„é‡‡æ ·ç‡ä¸€è‡´\n",
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=speech_recognizer.feature_extractor.sampling_rate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å½“è°ƒç”¨ \"audio\" åˆ—æ—¶, éŸ³é¢‘æ–‡ä»¶å°†ä¼šè‡ªåŠ¨åŠ è½½å¹¶é‡é‡‡æ ·ã€‚ ä»å‰å››ä¸ªæ ·æœ¬ä¸­æå–åŸå§‹æ³¢å½¢æ•°ç»„ï¼Œå°†å®ƒä½œä¸ºåˆ—è¡¨ä¼ ç»™ pipelineï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I WOULD LIKE TO SET UP A JOINT ACCOUNT WITH MY PARTNER HOW DO I PROCEED WITH DOING THAT', \"FONDERING HOW I'D SET UP A JOIN TO HELL T WITH MY WIFE AND WHERE THE AP MIGHT BE\", \"I I'D LIKE TOY SET UP A JOINT ACCOUNT WITH MY PARTNER I'M NOT SEEING THE OPTION TO DO IT ON THE APSO I CALLED IN TO GET SOME HELP CAN I JUST DO IT OVER THE PHONE WITH YOU AND GIVE YOU THE INFORMATION OR SHOULD I DO IT IN THE AP AN I'M MISSING SOMETHING UQUETTE HAD PREFERRED TO JUST DO IT OVER THE PHONE OF POSSIBLE THINGS\", 'HOW DO I FURN A JOINA COUT']\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "\n",
    "if importlib.util.find_spec(\"librosa\") is None:\n",
    "    raise ImportError(f\"'librosa' is not installed. Please install it before running this script.\")\n",
    "\n",
    "result = speech_recognizer(dataset[:4][\"audio\"])\n",
    "print([d[\"text\"] for d in result])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå¯¹äºè¾“å…¥å¾ˆå¤§çš„å¤§å‹æ•°æ®é›†ï¼ˆå¦‚è¯­éŸ³æˆ–è§†è§‰ï¼‰ï¼Œåˆ™éœ€è¦ä¼ é€’ä¸€ä¸ªç”Ÿæˆå™¨è€Œä¸æ˜¯ç”¨ä¸€ä¸ªåˆ—è¡¨æ¥åŠ è½½å†…å­˜ä¸­çš„æ‰€æœ‰è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨ pipeline ä¸­ä½¿ç”¨å¦ä¸€ä¸ªæ¨¡å‹å’Œåˆ†è¯å™¨\n",
    "\n",
    "åœ¨ä¸Šæ–‡ä¸­æåˆ°ï¼Œè‹¥ pipeline() åœ¨æŒ‡å®šä»»åŠ¡åæœªæŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œåˆ™ä¼šä¸‹è½½å¹¶ç¼“å­˜ä¸€ä¸ªç”¨äºæƒ…æ„Ÿåˆ†æçš„é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨ã€‚\n",
    "\n",
    "å› ä¸º pipeline() å¯ä»¥å®¹çº³Hubä¸­çš„ä»»ä½•æ¨¡å‹ï¼Œä»è€Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°è®© pipeline() é€‚ç”¨äºå…¶ä»–ç”¨ä¾‹ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³è¦ä¸€ä¸ªèƒ½å¤Ÿå¤„ç†æ³•è¯­æ–‡æœ¬çš„æ¨¡å‹ï¼Œå¯ä»¥å…ˆä½¿ç”¨ Hub ä¸Šçš„ `Tags` ç­›é€‰åˆé€‚çš„æ¨¡å‹ã€‚\n",
    "\n",
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬é€‰æ‹©äº†ä¸€ä¸ªå¤šè¯­è¨€BERTæ¨¡å‹ï¼Œè¯¥æ¨¡å‹ç»è¿‡å¾®è°ƒï¼Œå¯ç”¨äºæ³•è¯­æ–‡æœ¬çš„æƒ…æ„Ÿåˆ†æï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å¤šè¯­è¨€BERTæ¨¡å‹\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ transformers åº“ä¸­ï¼ŒåŠ è½½é¢„è®­ç»ƒæ¨¡å‹ä¸»è¦æœ‰ä¸¤ç§æ–¹å¼ï¼š\n",
    "- **ä½¿ç”¨ pipeline() åŠ è½½ã€‚**\n",
    "\n",
    "    **ç‰¹ç‚¹**\n",
    "\n",
    "    1. é«˜é˜¶å°è£…ï¼špipeline å‡½æ•°æä¾›äº†ä¸€ä¸ªé«˜é˜¶å°è£…ï¼Œä½¿å¾—åŠ è½½å’Œä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹å˜å¾—éå¸¸ç®€å•ã€‚\n",
    "    2. æ˜“ç”¨æ€§ï¼šé€‚åˆå¿«é€Ÿå®éªŒå’ŒåŸå‹å¼€å‘ï¼Œå› ä¸ºå®ƒè‡ªåŠ¨å¤„ç†äº†å¾ˆå¤šç»†èŠ‚ï¼Œå¦‚æ¨¡å‹åŠ è½½ã€åˆ†è¯å™¨é…ç½®ç­‰ã€‚\n",
    "    3. å¤šåŠŸèƒ½ï¼šæ”¯æŒå¤šç§ä»»åŠ¡ç±»å‹ï¼ˆå¦‚æ–‡æœ¬åˆ†ç±»ã€é—®ç­”ã€å‘½åå®ä½“è¯†åˆ«ç­‰ï¼‰ï¼Œ**åªéœ€æŒ‡å®šä»»åŠ¡åç§°**å³å¯ã€‚\n",
    "    4. é»˜è®¤é…ç½®ï¼š**ä½¿ç”¨é»˜è®¤çš„æ¨¡å‹å’Œåˆ†è¯å™¨é…ç½®**ï¼Œæ— éœ€æ‰‹åŠ¨è°ƒæ•´ã€‚\n",
    "\n",
    "    **é€‚ç”¨åœºæ™¯**\n",
    "    \n",
    "    1. å¿«é€ŸéªŒè¯æ¨¡å‹æ•ˆæœã€‚\n",
    "    2. ç®€å•çš„åº”ç”¨ç¨‹åºå¼€å‘ã€‚\n",
    "    3. ä¸éœ€è¦è‡ªå®šä¹‰æ¨¡å‹æˆ–åˆ†è¯å™¨é…ç½®çš„æƒ…å†µã€‚\n",
    "- **ç›´æ¥åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨ã€‚**\n",
    "\n",
    "    **ç‰¹ç‚¹**\n",
    "\n",
    "    1. çµæ´»æ€§ï¼šå…è®¸æ›´ç»†ç²’åº¦çš„æ§åˆ¶å’Œè‡ªå®šä¹‰ï¼Œå¦‚ä¿®æ”¹åˆ†è¯å™¨çš„é…ç½®ã€è°ƒæ•´æ¨¡å‹çš„è¶…å‚æ•°ç­‰ã€‚\n",
    "    2. å¯æ‰©å±•æ€§ï¼šé€‚åˆ**éœ€è¦è‡ªå®šä¹‰æ¨¡å‹æˆ–åˆ†è¯å™¨**çš„æƒ…å†µï¼Œä¾‹å¦‚æ·»åŠ æ–°çš„ç‰¹æ®Šæ ‡è®°ã€è°ƒæ•´åˆ†è¯ç­–ç•¥ç­‰ã€‚\n",
    "    3. æ˜¾å¼æ§åˆ¶ï¼šåŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨çš„è¿‡ç¨‹æ˜¯æ˜¾å¼çš„ï¼Œä¾¿äºè°ƒè¯•å’Œç†è§£ã€‚\n",
    "\n",
    "    **é€‚ç”¨åœºæ™¯**\n",
    "\n",
    "    1. éœ€è¦è‡ªå®šä¹‰æ¨¡å‹æˆ–åˆ†è¯å™¨é…ç½®ã€‚\n",
    "    2. è¿›è¡Œæ›´æ·±å…¥çš„æ¨¡å‹ç ”ç©¶å’Œè°ƒä¼˜ã€‚\n",
    "    3. å¼€å‘å¤æ‚çš„åº”ç”¨ç¨‹åºï¼Œéœ€è¦æ›´ç²¾ç»†çš„æ§åˆ¶ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œä½¿ç”¨`AutoModelForSequenceClassification`å’Œ`AutoTokenizer`ç›´æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹åŠå…¶å…³è”çš„åˆ†è¯å™¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\OneDrive\\WorkSpaces\\PythonProjects\\transformers-practice\\transformers-playground-windows-ctv\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Pytorch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ pipeline() ä¸­æŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨`tokenizer`ï¼Œç°åœ¨æ‚¨å¯ä»¥å°†åˆ†ç±»å™¨`classifier`æ¥å¤„ç†æ³•è¯­æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
    "# åœ¨åå°ï¼Œ`AutoModelForSequenceClassification`å’Œ`AutoTokenizer`ç±»ä¸€èµ·å·¥ä½œï¼Œä¸ºä¸Šé¢ä½¿ç”¨çš„ pipeline() æä¾›åŠ¨åŠ›ã€‚\n",
    "\n",
    "classifier(\"Nous sommes trÃ¨s heureux de vous prÃ©senter la bibliothÃ¨que ğŸ¤— Transformers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æ²¡æœ‰æ‰¾åˆ°é€‚åˆä½ çš„æ¨¡å‹ï¼Œå°±éœ€è¦åœ¨ä½ çš„æ•°æ®ä¸Šå¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹äº†ã€‚\n",
    "\n",
    "æŸ¥çœ‹[å¾®è°ƒæ•™ç¨‹](https://huggingface.co/docs/transformers/v4.44.2/zh/training)æ¥å­¦ä¹ æ€æ ·è¿›è¡Œå¾®è°ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auto Class\n",
    "\n",
    "`AutoClass`æ˜¯ä¸€ç§å¿«æ·æ–¹å¼ï¼Œå¯ä»¥ä»é¢„è®­ç»ƒæ¨¡å‹çš„åç§°æˆ–è·¯å¾„ä¸­è‡ªåŠ¨æ£€ç´¢å…¶æ¶æ„ï¼Œä½ åªéœ€ä¸ºä»»åŠ¡åŠå…¶å…³è”çš„é¢„å¤„ç†ç±»é€‰æ‹©é€‚å½“çš„`AutoClass`ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬å›åˆ°ä¸Šä¸€èŠ‚çš„ç¤ºä¾‹ï¼Œçœ‹çœ‹å¦‚ä½•ä½¿ç”¨ AutoClass å¤åˆ¶ pipeline() çš„ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoTokenizer\n",
    "\n",
    "åˆ†è¯å™¨è´Ÿè´£é¢„å¤„ç†æ–‡æœ¬ï¼Œå°†æ–‡æœ¬è½¬æ¢ä¸ºç”¨äºè¾“å…¥æ¨¡å‹çš„æ•°å­—æ•°ç»„ã€‚æœ‰å¤šä¸ªç”¨æ¥ç®¡ç†åˆ†è¯è¿‡ç¨‹çš„è§„åˆ™ï¼ŒåŒ…æ‹¬å¦‚ä½•æ‹†åˆ†å•è¯å’Œåœ¨ä»€ä¹ˆæ ·çš„çº§åˆ«ä¸Šæ‹†åˆ†å•è¯ã€‚\n",
    "\n",
    "æ³¨æ„ï¼šå®ä¾‹åŒ–çš„åˆ†è¯å™¨è¦ä¸æ¨¡å‹çš„åç§°ç›¸åŒ, ä»¥ç¡®ä¿å’Œæ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨ç›¸åŒçš„åˆ†è¯è§„åˆ™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "# ä½¿ç”¨AutoTokenizeråŠ è½½ä»¤ç‰ŒåŒ–å™¨ï¼Œå®ä¾‹åŒ–çš„åˆ†è¯å™¨è¦ä¸æ¨¡å‹çš„åç§°ç›¸åŒ\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†æ–‡æœ¬ä¼ å…¥åˆ†è¯å™¨ tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer(\"We are very happy to show you the ğŸ¤— Transformers library.\")\n",
    "print(encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ†è¯å™¨è¿”å›äº†å«æœ‰å¦‚ä¸‹å†…å®¹çš„å­—å…¸:\n",
    "- input_idsï¼šè¿™æ˜¯ä¸€ä¸ªæ•´æ•°åºåˆ—ï¼Œæ¯ä¸ªæ•´æ•°ä»£è¡¨è¯æ±‡è¡¨ä¸­çš„ä¸€ä¸ªç‰¹å®štokenã€‚**æ¨¡å‹ä½¿ç”¨è¿™ä¸ªåºåˆ—ä½œä¸ºè¾“å…¥æ¥ç†è§£å’Œå¤„ç†æ–‡æœ¬æ•°æ®**ã€‚\n",
    "- token_type_idsï¼štoken_type_ids æ˜¯ä¸€ä¸ªä¸ input_ids ç›¸åŒé•¿åº¦çš„åºåˆ—ï¼Œå®ƒè¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­æ¯ä¸ª token çš„ç±»å‹ã€‚è¿™ä¸ªå­—æ®µä¸»è¦ç”¨äº**åŒºåˆ†ä¸åŒç±»å‹çš„è¾“å…¥åºåˆ—**ï¼Œå°¤å…¶æ˜¯åœ¨å¤„ç†ç”±å¤šä¸ªæ–‡æœ¬åºåˆ—ç»„æˆçš„è¾“å…¥æ—¶ï¼Œä¾‹å¦‚é—®ç­”ç³»ç»Ÿä¸­çš„é—®é¢˜å’Œç­”æ¡ˆï¼Œæˆ–è€…æ–‡æœ¬åˆ†ç±»ä»»åŠ¡ä¸­çš„æ–‡æœ¬å’Œæ ‡ç­¾ã€‚\n",
    "- attention_maskï¼šè¿™ä¸ªåºåˆ—è¡¨ç¤ºè¾“å…¥åºåˆ—ä¸­å“ªäº› token æ˜¯å®é™…çš„æ–‡æœ¬tokenï¼Œå“ªäº›æ˜¯å¡«å……ï¼ˆpaddingï¼‰tokenã€‚åœ¨å¤„ç†æ‰¹é‡æ•°æ®æ—¶ï¼Œ**ç”±äºä¸åŒåºåˆ—çš„é•¿åº¦å¯èƒ½ä¸åŒï¼Œé€šå¸¸éœ€è¦ä½¿ç”¨å¡«å……tokenæ¥ä½¿æ‰€æœ‰åºåˆ—é•¿åº¦ä¸€è‡´**ã€‚`attention_mask` ä¸­çš„ `1 è¡¨ç¤ºå®é™…çš„æ–‡æœ¬token`ï¼Œè€Œ `0 è¡¨ç¤ºå¡«å……token`ï¼Œè¿™æ ·æ¨¡å‹å°±å¯ä»¥å¿½ç•¥å¡«å……tokenã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ†è¯å™¨ä¹Ÿå¯ä»¥æ¥å—åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œå¹¶å¡«å……å’Œæˆªæ–­æ–‡æœ¬ï¼Œè¿”å›å…·æœ‰ç»Ÿä¸€é•¿åº¦çš„æ‰¹æ¬¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pt_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"pt\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "tf_batch = tokenizer(\n",
    "    [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n",
    "    padding=True,\n",
    "    truncation=True,\n",
    "    max_length=512,\n",
    "    return_tensors=\"tf\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŸ¥é˜…[é¢„å¤„ç†æ•°æ®æ•™ç¨‹](https://huggingface.co/docs/transformers/v4.44.2/zh/preprocessing)æ¥è·å¾—æœ‰å…³åˆ†è¯çš„æ›´è¯¦ç»†çš„ä¿¡æ¯ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨ `AutoFeatureExtractor` å’Œ `AutoProcessor` æ¥å¤„ç†å›¾åƒï¼ŒéŸ³é¢‘ï¼Œè¿˜æœ‰å¤šæ¨¡å¼è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AutoModel\n",
    "\n",
    "ğŸ¤— Transformers æä¾›äº†ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„å®ä¾‹. è¿™è¡¨ç¤ºä½ å¯ä»¥åƒåŠ è½½ AutoTokenizer ä¸€æ ·åŠ è½½ AutoModelã€‚\n",
    "\n",
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ**AutoModel æ˜¯ä¸€ä¸ªé€šç”¨çš„æ¥å£ï¼Œå®ƒå¯ä»¥åŠ è½½å„ç§ç±»å‹çš„é¢„è®­ç»ƒæ¨¡å‹ï¼Œä½†å¯èƒ½éœ€è¦é¢å¤–çš„é…ç½®æ‰èƒ½ç”¨äºç‰¹å®šä»»åŠ¡ã€‚**\n",
    "\n",
    "è€Œ `AutoModelForSequenceClassification` æ˜¯ä¸“é—¨ä¸ºæ–‡æœ¬æˆ–åºåˆ—çš„åˆ†ç±»ä»»åŠ¡å®šåˆ¶çš„æ¥å£ï¼Œå·²è‡ªåŠ¨ä¸ºä½ é…ç½®å¥½äº†ä¸€ä¸ªé€‚ç”¨äºåºåˆ—åˆ†ç±»ä»»åŠ¡çš„æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ [ä»»åŠ¡æ‘˜è¦](https://huggingface.co/docs/transformers/v4.44.2/zh/task_summary) æŸ¥æ‰¾ AutoModel æ”¯æŒçš„ä»»åŠ¡."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨å¯ä»¥æŠŠä¹‹å‰é¢„å¤„ç†å¥½çš„è¾“å…¥æ‰¹æ¬¡ç›´æ¥ä¼ é€’æ¨¡å‹ã€‚ä½ éœ€è¦é€šè¿‡ ** æ¥è§£åŒ…å­—å…¸:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pt_outputs = pt_model(**pt_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "tf_outputs = tf_model(**tf_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**logits**\n",
    "- å®šä¹‰ï¼šlogitsæ˜¯æ¨¡å‹è¾“å‡ºçš„åŸå§‹åˆ†æ•°ï¼Œé€šå¸¸æ˜¯é€šè¿‡ä¸€ä¸ªçº¿æ€§å±‚ï¼ˆå…¨è¿æ¥å±‚ï¼‰å¾—åˆ°çš„ã€‚è¿™äº›åˆ†æ•°åœ¨æ²¡æœ‰ç»è¿‡ä»»ä½•æ¿€æ´»å‡½æ•°å¤„ç†ä¹‹å‰ï¼Œè¡¨ç¤ºæ¨¡å‹å¯¹æ¯ä¸ªç±»åˆ«çš„åŸå§‹é¢„æµ‹å€¼ã€‚\n",
    "- ä½œç”¨ï¼šlogitsæœ¬èº«å¹¶ä¸ç›´æ¥è¡¨ç¤ºæ¦‚ç‡ï¼Œè€Œæ˜¯éœ€è¦é€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚softmaxï¼‰è½¬æ¢ä¸ºæ¦‚ç‡ã€‚\n",
    "**æ¿€æ´»ç»“æœ**\n",
    "- å®šä¹‰ï¼šæ¿€æ´»ç»“æœé€šå¸¸æ˜¯æŒ‡å°†logitsé€šè¿‡æ¿€æ´»å‡½æ•°ï¼ˆå¦‚softmaxã€sigmoidç­‰ï¼‰å¤„ç†åçš„è¾“å‡ºã€‚æ¿€æ´»å‡½æ•°çš„ä½œç”¨æ˜¯å°†logitsè½¬æ¢ä¸ºæŸç§å½¢å¼çš„æ¦‚ç‡æˆ–å†³ç­–è¾¹ç•Œã€‚\n",
    "- ä½œç”¨ï¼šæ¿€æ´»å‡½æ•°å¯ä»¥ä½¿æ¨¡å‹çš„è¾“å‡ºæ›´æ˜“äºè§£é‡Šå’Œä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œsoftmaxå‡½æ•°å¯ä»¥å°†logitsè½¬æ¢ä¸ºå¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "æ¨¡å‹åœ¨ `pt_outputs.logits` è¾“å‡ºæœ€ç»ˆçš„æ¿€æ´»ç»“æœ. è¿™é‡Œåœ¨ logits ä¸Šåº”ç”¨ `softmax` å‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡:\n",
    "- `dim = -1`ï¼šè¡¨ç¤ºåœ¨æœ€åä¸€ä¸ªç»´åº¦ä¸Šè¿›è¡Œsoftmaxæ“ä½œï¼Œé€šå¸¸åœ¨å¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œæœ€åä¸€ä¸ªç»´åº¦æ˜¯ç±»åˆ«ç»´åº¦ã€‚\n",
    "- åœ¨å¤šç±»åˆ†ç±»ä»»åŠ¡ä¸­ï¼Œsoftmax å‡½æ•°å°† logits è½¬æ¢ä¸ºæ¦‚ç‡åˆ†å¸ƒï¼Œèƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬äº†è§£æ¨¡å‹å¯¹æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹ç½®ä¿¡åº¦ï¼Œä»è€Œåšå‡ºæ›´å‡†ç¡®çš„å†³ç­–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from torch import nn\n",
    "\n",
    "pt_predictions = nn.functional.softmax(pt_outputs.logits, dim=-1)\n",
    "print(pt_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pt_predictions` æ˜¯ä¸€ä¸ªæ¦‚ç‡åˆ†å¸ƒï¼Œæ¯ä¸ªå…ƒç´ è¡¨ç¤ºå¯¹åº”ç±»åˆ«çš„æ¦‚ç‡ï¼Œæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡å€¼è¡¨ç¤ºæ¨¡å‹å¯¹è¯¥ç±»åˆ«çš„ç½®ä¿¡åº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "\n",
    "tf_predictions = tf.nn.softmax(tf_outputs.logits, dim=-1)\n",
    "print(tf_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹ï¼ˆPyTorch æˆ– TensorFlowï¼‰åœ¨æœ€ç»ˆçš„æ¿€æ´»å‡½æ•°ï¼ˆæ¯”å¦‚ softmaxï¼‰ä¹‹å‰è¾“å‡ºå¼ é‡ï¼Œ å› ä¸ºæœ€ç»ˆçš„æ¿€æ´»å‡½æ•°å¸¸å¸¸ä¸æŸå¤±(loss)èåˆã€‚\n",
    "\n",
    "æ¨¡å‹çš„è¾“å‡ºæ˜¯ç‰¹æ®Šçš„æ•°æ®ç±»ï¼Œæ‰€ä»¥å®ƒä»¬çš„å±æ€§å¯ä»¥åœ¨ IDE ä¸­è¢«è‡ªåŠ¨è¡¥å…¨ã€‚æ¨¡å‹çš„è¾“å‡ºå°±åƒä¸€ä¸ªå…ƒç»„æˆ–å­—å…¸ï¼ˆä½ å¯ä»¥é€šè¿‡æ•´æ•°ã€åˆ‡ç‰‡æˆ–å­—ç¬¦ä¸²æ¥ç´¢å¼•å®ƒï¼‰ï¼Œä¸º None çš„å±æ€§ä¼šè¢«å¿½ç•¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä¿å­˜æ¨¡å‹\n",
    "\n",
    "å½“ä½ çš„æ¨¡å‹å¾®è°ƒå®Œæˆ(é¢„è®­ç»ƒå®Œæˆ)ï¼Œå¯ä»¥ä½¿ç”¨ `PreTrainedModel.save_pretrained()` æŠŠæ¨¡å‹å’Œæ¨¡å‹çš„åˆ†è¯å™¨ä¿å­˜ä¸‹æ¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pt_save_directory = \"./pt_save_pretrained\"\n",
    "tokenizer.save_pretrained(pt_save_directory)\n",
    "pt_model.save_pretrained(pt_save_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "tf_save_directory = \"./tf_save_pretrained\"\n",
    "tokenizer.save_pretrained(tf_save_directory)\n",
    "tf_model.save_pretrained(tf_save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å†æ¬¡ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ—¶ï¼Œå¯ä»¥ä½¿ç”¨ `PreTrainedModel.from_pretrained()` åŠ è½½å®ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ğŸ¤— Transformers æœ‰ä¸€ä¸ªç‰¹åˆ«é…·çš„åŠŸèƒ½ï¼Œä¿å­˜ä¸€ä¸ªæ¨¡å‹åï¼Œåœ¨åŠ è½½æ—¶å¯ä»¥é€‰æ‹©åœ°å°†å®ƒåŠ è½½ä¸º PyTorch æˆ– TensorFlow æ¨¡å‹ã€‚`from_pt` æˆ– `from_tf` å‚æ•°å¯ä»¥å°†æ¨¡å‹ä»ä¸€ä¸ªæ¡†æ¶è½¬æ¢ä¸ºå¦ä¸€ä¸ªæ¡†æ¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow To Pytorch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True) # from_tf å°† TensorFlow æ¨¡å‹åŠ è½½ä¸º PyTorch æ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch To TensorFlow\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pt_save_directory)\n",
    "tf_model = TFAutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è‡ªå®šä¹‰æ¨¡å‹æ„å»º\n",
    "\n",
    "ä½ å¯ä»¥ä¿®æ”¹æ¨¡å‹çš„é…ç½®ç±»æ¥æ”¹å˜æ¨¡å‹çš„æ„å»ºæ–¹å¼ã€‚é…ç½®æ¨¡å‹çš„å±æ€§ï¼Œæ¯”å¦‚éšè—å±‚æˆ–è€…æ³¨æ„åŠ›å¤´çš„æ•°é‡ã€‚å½“ä½ æ ¹æ®è‡ªå®šä¹‰çš„é…ç½®ç±»åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œæ¨¡å‹çš„å±æ€§éƒ½æ˜¯éšæœºåˆå§‹åŒ–çš„ï¼Œä½ éœ€è¦å…ˆè®­ç»ƒæ¨¡å‹æ‰èƒ½å¾—åˆ°æœ‰æ„ä¹‰çš„ç»“æœã€‚\n",
    "\n",
    "è‡ªå®šä¹‰æ¨¡å‹æ„å»ºé€šè¿‡å¯¼å…¥ `AutoConfig` æ¥åŠ è½½ä½ æƒ³ä¿®æ”¹çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ `AutoConfig.from_pretrained()` ä¸­ï¼Œä½ èƒ½å¤ŸæŒ‡å®šæƒ³è¦ä¿®æ”¹çš„å±æ€§ï¼Œæ¯”å¦‚æ³¨æ„åŠ›å¤´çš„æ•°é‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "my_config = AutoConfig.from_pretrained(\"distilbert/distilbert-base-uncased\", n_heads=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ `AutoModel.from_config()` æ ¹æ®ä½ çš„è‡ªå®šä¹‰é…ç½®åˆ›å»ºä¸€ä¸ªæ¨¡å‹:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from transformers import AutoModel\n",
    "\n",
    "my_model = AutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "from transformers import TFAutoModel\n",
    "\n",
    "my_model = TFAutoModel.from_config(my_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trainer - PyTorch ä¼˜åŒ–è®­ç»ƒå¾ªç¯\n",
    "\n",
    "æ‰€æœ‰çš„æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„ `[torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module)`ï¼Œæ‰€ä»¥ä½ å¯ä»¥åœ¨ä»»ä½•å…¸å‹çš„è®­ç»ƒæ¨¡å‹ä¸­ä½¿ç”¨å®ƒä»¬ã€‚\n",
    "\n",
    "å½“ä½ ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ—¶ï¼ŒğŸ¤— Transformers ä¸º PyTorch æä¾›äº†ä¸€ä¸ª `Trainer` ç±»ï¼Œå®ƒåŒ…å«äº†åŸºç¡€çš„è®­ç»ƒå¾ªç¯å¹¶ä¸”ä¸ºè¯¸å¦‚åˆ†å¸ƒå¼è®­ç»ƒï¼Œæ··åˆç²¾åº¦ç­‰ç‰¹æ€§å¢åŠ äº†é¢å¤–çš„åŠŸèƒ½ã€‚\n",
    "\n",
    "æ ¹æ®äºä½ çš„ä»»åŠ¡, ä½ å¯ä»¥ä¼ é€’ä»¥ä¸‹çš„å‚æ•°ç»™ `Trainer`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ä½¿ç”¨ `PreTrainedModel` æˆ–è€… `torch.nn.Module` æ¥å¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **`TrainingArguments` å«æœ‰ä½ å¯ä»¥ä¿®æ”¹çš„æ¨¡å‹è¶…å‚æ•°**ï¼Œæ¯”å¦‚å­¦ä¹ ç‡ï¼Œæ‰¹æ¬¡å¤§å°å’Œè®­ç»ƒæ—¶çš„è¿­ä»£æ¬¡æ•°ã€‚å¦‚æœä½ æ²¡æœ‰æŒ‡å®šè®­ç»ƒå‚æ•°ï¼Œé‚£ä¹ˆå®ƒä¼šä½¿ç”¨é»˜è®¤å€¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"path/to/save/folder/\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. åŠ è½½ä¸€ä¸ªé¢„å¤„ç†ç±»ï¼Œæ¯”å¦‚åˆ†è¯å™¨ï¼Œç‰¹å¾æå–å™¨æˆ–è€…å¤„ç†å™¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. åŠ è½½ä¸€ä¸ªæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")  # doctest: +IGNORE_RESULT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†ç”¨äºåˆ†è¯çš„å‡½æ•°ï¼Œå¹¶ä¸”ä½¿ç”¨ `map` å°†åˆ†è¯å™¨åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. åˆ›å»ºä¸€ä¸ªæ•°æ®é›†é¢„å¤„ç†å·¥å…· `DataCollatorWithPadding`\n",
    "\n",
    "- `DataCollatorWithPadding` æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„æ•°æ®é¢„å¤„ç†å·¥å…·ï¼Œèƒ½å¤Ÿè‡ªåŠ¨å¤„ç†æ‰¹æ¬¡æ•°æ®çš„å¡«å……å’Œæ©ç ç”Ÿæˆï¼Œæå¤§åœ°ç®€åŒ–äº†NLPä»»åŠ¡ä¸­çš„æ•°æ®é¢„å¤„ç†æ­¥éª¤ã€‚é€šè¿‡ä½¿ç”¨è¿™ä¸ªç±»ï¼Œå¯ä»¥ç¡®ä¿æ¨¡å‹è¾“å…¥çš„ä¸€è‡´æ€§ï¼Œæé«˜è®­ç»ƒæ•ˆç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨æŠŠæ‰€æœ‰çš„ç±»ä¼ ç»™ `Trainer`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ") \n",
    "# ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè°ƒç”¨ train() è¿›è¡Œè®­ç»ƒ\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºåƒç¿»è¯‘æˆ–æ‘˜è¦è¿™äº›ä½¿ç”¨åºåˆ—åˆ°åºåˆ—æ¨¡å‹çš„ä»»åŠ¡ï¼Œç”¨ `Seq2SeqTrainer` å’Œ `Seq2SeqTrainingArguments` æ¥æ›¿ä»£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥é€šè¿‡å­ç±»åŒ– `Trainer` å¹¶é‡å†™å…¶ä¸­çš„æ–¹æ³•æ¥è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚è¿™æ ·ä½ å°±å¯ä»¥**è‡ªå®šä¹‰åƒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨ç­‰è¿™æ ·çš„ç‰¹æ€§**ã€‚æŸ¥é˜… [Trainer å‚è€ƒæ‰‹å†Œ](https://huggingface.co/docs/transformers/v4.44.2/en/main_classes/trainer#transformers.Trainer)äº†è§£å“ªäº›æ–¹æ³•èƒ½å¤Ÿè¢«å­ç±»åŒ–ã€‚\n",
    "\n",
    "å¦ä¸€ä¸ªè‡ªå®šä¹‰è®­ç»ƒå¾ªç¯çš„æ–¹å¼æ˜¯é€šè¿‡[å›è°ƒ(Callbacks)](https://huggingface.co/docs/transformers/main_classes/callback)ã€‚ä½ å¯ä»¥**ä½¿ç”¨å›è°ƒæ¥ä¸å…¶ä»–åº“é›†æˆ**ï¼ŒæŸ¥çœ‹è®­ç»ƒå¾ªç¯æ¥æŠ¥å‘Šè¿›åº¦æˆ–æå‰ç»“æŸè®­ç»ƒã€‚\n",
    "\n",
    "**æ³¨æ„ï¼šå›è°ƒå¹¶ä¸ä¼šä¿®æ”¹è®­ç»ƒå¾ªç¯ã€‚å¦‚æœæƒ³è‡ªå®šä¹‰æŸå¤±å‡½æ•°ç­‰ï¼Œå°±éœ€è¦å­ç±»åŒ– Trainer äº†ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ Tensorflow è®­ç»ƒ\n",
    "\n",
    "æ‰€æœ‰æ¨¡å‹éƒ½æ˜¯æ ‡å‡†çš„ `[tf.keras.Model](https://www.tensorflow.org/api_docs/python/tf/keras/Model)`ï¼Œæ‰€ä»¥ä½ å¯ä»¥**é€šè¿‡ `Keras API` å®ç°åœ¨ Tensorflow ä¸­è®­ç»ƒ**ã€‚\n",
    "\n",
    "ğŸ¤— Transformers æä¾›äº† `prepare_tf_dataset()` æ–¹æ³•æ¥è½»æ¾åœ°å°†æ•°æ®é›†åŠ è½½ä¸º `tf.data.Dataset`ï¼Œè¿™æ ·ä½ å°±å¯ä»¥ä½¿ç”¨ Keras çš„ `compile` å’Œ `fit` æ–¹æ³•é©¬ä¸Šå¼€å§‹è®­ç»ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ä½¿ç”¨ `TFPreTrainedModel` æˆ–è€… `tf.keras.Model` æ¥å¼€å§‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. åŠ è½½ä¸€ä¸ªæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"rotten_tomatoes\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. åŠ è½½ä¸€ä¸ªé¢„å¤„ç†ç±»ï¼Œæ¯”å¦‚åˆ†è¯å™¨ï¼Œç‰¹å¾æå–å™¨æˆ–è€…å¤„ç†å™¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. åˆ›å»ºä¸€ä¸ªç»™æ•°æ®é›†ç”¨äºåˆ†è¯çš„å‡½æ•°ï¼Œå¹¶ä¸”ä½¿ç”¨ `map` å°†åˆ†è¯å™¨åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset):\n",
    "    return tokenizer(dataset[\"text\"])\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. å°†æ•°æ®é›†å’Œåˆ†è¯å™¨ä¼ ç»™ `prepare_tf_dataset()`å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ã€‚å¦‚æœä½ éœ€è¦çš„è¯ï¼Œä¹Ÿå¯ä»¥åœ¨è¿™é‡Œæ”¹å˜æ‰¹æ¬¡å¤§å°å’Œæ˜¯å¦æ‰“ä¹±æ•°æ®é›†"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "tf_dataset = model.prepare_tf_dataset(\n",
    "    dataset, batch_size=16, shuffle=True, tokenizer=tokenizer\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ä¸€åˆ‡å‡†å¤‡å°±ç»ªåï¼Œè°ƒç”¨ `compile` å’Œ `fit` å¼€å§‹è®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "from transformers.keras.optimizers import Adam\n",
    "\n",
    "model.compile(optimizer=Adam(3e-5))\n",
    "model.fit(dataset) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ¥ä¸‹æ¥åšä»€ä¹ˆ\n",
    "\n",
    "ç°åœ¨ä½ å·²ç»å®Œæˆäº† ğŸ¤— Transformers çš„å¿«é€Ÿä¸Šæ‰‹æ•™ç¨‹ï¼Œæ¥çœ‹çœ‹æˆ‘ä»¬çš„æŒ‡å—å¹¶ä¸”å­¦ä¹ å¦‚ä½•åšä¸€äº›æ›´å…·ä½“çš„äº‹æƒ…ï¼Œæ¯”å¦‚å†™ä¸€ä¸ªè‡ªå®šä¹‰æ¨¡å‹ï¼Œä¸ºæŸä¸ªä»»åŠ¡å¾®è°ƒä¸€ä¸ªæ¨¡å‹ä»¥åŠå¦‚ä½•ä½¿ç”¨è„šæœ¬æ¥è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "å¦‚æœä½ æœ‰å…´è¶£äº†è§£æ›´å¤š ğŸ¤— Transformers çš„æ ¸å¿ƒç« èŠ‚ï¼Œé‚£å°±å–æ¯å’–å•¡ç„¶åæ¥çœ‹çœ‹æˆ‘ä»¬çš„æ¦‚å¿µæŒ‡å—å§ï¼"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
