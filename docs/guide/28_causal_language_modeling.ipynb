{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35cccc02",
   "metadata": {},
   "source": [
    "# å› æœè¯­è¨€æ¨¡å‹\n",
    "\n",
    "å› æœè¯­è¨€æ¨¡å‹æ˜¯è¯­è¨€æ¨¡å‹çš„ä¸€ç§ç±»å‹ï¼Œå®ƒç”¨äºé¢„æµ‹åºåˆ—ä¸­ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå¹¶ä¸”æ¨¡å‹åªèƒ½å…³æ³¨å·¦ä¾§çš„æ ‡è®°ã€‚è¿™æ„å‘³ç€æ¨¡å‹æ— æ³•çœ‹åˆ°æœªæ¥çš„æ ‡è®°ã€‚GPT-2 å°±æ˜¯ä¸€ä¸ªå› æœè¯­è¨€æ¨¡å‹çš„ä¾‹å­ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "1. åœ¨ [ELI5](https://huggingface.co/datasets/eli5) æ•°æ®é›†çš„ [r/askscience](https://www.reddit.com/r/askscience/) å­é›†ä¸Šå¾®è°ƒ [DistilGPT2](https://huggingface.co/distilbert/distilgpt2)ã€‚\n",
    "2. ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e548135",
   "metadata": {},
   "source": [
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee60dd3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932bea26",
   "metadata": {},
   "source": [
    "\n",
    "æˆ‘ä»¬å»ºè®®æ‚¨ç™»å½•åˆ°æ‚¨çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6174f6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1feb00",
   "metadata": {},
   "source": [
    "\n",
    "## åŠ è½½ ELI5 æ•°æ®é›†\n",
    "\n",
    "é¦–å…ˆï¼Œä½¿ç”¨ ğŸ¤— Datasets åº“åŠ è½½ [ELI5-Category](https://huggingface.co/datasets/eli5_category) æ•°æ®é›†çš„å‰ 5000 ä¸ªç¤ºä¾‹ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸å·¥ä½œï¼Œç„¶åå†åœ¨å®Œæ•´æ•°æ®é›†ä¸ŠèŠ±è´¹æ›´å¤šæ—¶é—´è¿›è¡Œè®­ç»ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954d7f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9824094f",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) æ–¹æ³•å°†æ•°æ®é›†çš„ `train` åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d75834",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73de33e",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99d65658",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e97ff41",
   "metadata": {},
   "source": [
    "\n",
    "è™½ç„¶è¿™å¯èƒ½çœ‹èµ·æ¥å¾ˆå¤šï¼Œä½†æ‚¨çœŸæ­£æ„Ÿå…´è¶£çš„æ˜¯ `text` å­—æ®µã€‚è¯­è¨€å»ºæ¨¡ä»»åŠ¡é…·çš„åœ°æ–¹åœ¨äºæ‚¨ä¸éœ€è¦æ ‡ç­¾ï¼ˆä¹Ÿç§°ä¸ºæ— ç›‘ç£ä»»åŠ¡ï¼‰ï¼Œå› ä¸ºä¸‹ä¸€ä¸ªè¯å°±æ˜¯æ ‡ç­¾ã€‚\n",
    "\n",
    "## é¢„å¤„ç†\n",
    "\n",
    "ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ DistilGPT2 åˆ†è¯å™¨æ¥å¤„ç† `text` å­å­—æ®µï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb66440d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f24e30f7",
   "metadata": {},
   "source": [
    "\n",
    "æ‚¨ä¼šæ³¨æ„åˆ°ä¸Šé¢çš„ç¤ºä¾‹ä¸­ï¼Œ`text` å­—æ®µå®é™…ä¸ŠåµŒå¥—åœ¨ `answers` ä¸­ã€‚è¿™æ„å‘³ç€æ‚¨éœ€è¦ä½¿ç”¨ [`flatten`](https://huggingface.co/docs/datasets/process#flatten) æ–¹æ³•ä»å…¶åµŒå¥—ç»“æ„ä¸­æå– `text` å­å­—æ®µï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7edcc72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "eli5 = eli5.flatten()\n",
    "eli5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50329e28",
   "metadata": {},
   "source": [
    "\n",
    "æ¯ä¸ªå­å­—æ®µç°åœ¨éƒ½æ˜¯ä¸€ä¸ªå•ç‹¬çš„åˆ—ï¼Œå¦‚ `answers` å‰ç¼€æ‰€ç¤ºï¼Œ`text` å­—æ®µç°åœ¨æ˜¯ä¸€ä¸ªåˆ—è¡¨ã€‚ä¸è¦åˆ†åˆ«å¯¹æ¯ä¸ªå¥å­è¿›è¡Œåˆ†è¯ï¼Œè€Œæ˜¯å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥è”åˆå¯¹å®ƒä»¬è¿›è¡Œåˆ†è¯ã€‚\n",
    "\n",
    "è¿™æ˜¯ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œç”¨äºè¿æ¥æ¯ä¸ªç¤ºä¾‹çš„å­—ç¬¦ä¸²åˆ—è¡¨å¹¶åˆ†è¯ç»“æœï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2666e87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83353e37",
   "metadata": {},
   "source": [
    "\n",
    "è¦ä½¿ç”¨æ­¤é¢„å¤„ç†å‡½æ•°å¤„ç†æ•´ä¸ªæ•°æ®é›†ï¼Œè¯·ä½¿ç”¨ ğŸ¤— Datasets çš„ [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) æ–¹æ³•ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `batched=True` æ¥åŠ é€Ÿ `map` å‡½æ•°ï¼Œä»¥ä¾¿ä¸€æ¬¡å¤„ç†æ•°æ®é›†ä¸­çš„å¤šä¸ªå…ƒç´ ï¼Œå¹¶é€šè¿‡ `num_proc` å¢åŠ è¿›ç¨‹æ•°ã€‚åˆ é™¤æ‚¨ä¸éœ€è¦çš„ä»»ä½•åˆ—ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb138fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_eli5 = eli5.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    num_proc=4,\n",
    "    remove_columns=eli5[\"train\"].column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d92aee5",
   "metadata": {},
   "source": [
    "\n",
    "æ­¤æ•°æ®é›†åŒ…å«æ ‡è®°åºåˆ—ï¼Œä½†å…¶ä¸­ä¸€äº›åºåˆ—çš„é•¿åº¦è¶…è¿‡äº†æ¨¡å‹çš„æœ€å¤§è¾“å…¥é•¿åº¦ã€‚\n",
    "\n",
    "æ‚¨ç°åœ¨å¯ä»¥ä½¿ç”¨ç¬¬äºŒä¸ªé¢„å¤„ç†å‡½æ•°æ¥ï¼š\n",
    "\n",
    "- è¿æ¥æ‰€æœ‰åºåˆ—\n",
    "- å°†è¿æ¥çš„åºåˆ—åˆ†å‰²æˆç”± `block_size` å®šä¹‰çš„è¾ƒçŸ­å—ï¼Œ`block_size` åº”è¯¥æ—¢å°äºæœ€å¤§è¾“å…¥é•¿åº¦ï¼ŒåˆçŸ­åˆ°è¶³ä»¥é€‚åˆæ‚¨çš„ GPU RAMã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3bb016",
   "metadata": {},
   "outputs": [],
   "source": [
    "block_size = 128\n",
    "\n",
    "def group_texts(examples):\n",
    "    # è¿æ¥æ‰€æœ‰æ–‡æœ¬ã€‚\n",
    "    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # æˆ‘ä»¬ä¸¢å¼ƒå°çš„ä½™æ•°ï¼Œå¦‚æœæ¨¡å‹æ”¯æŒï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ å¡«å……è€Œä¸æ˜¯ä¸¢å¼ƒï¼Œæ‚¨å¯ä»¥è‡ªå®šä¹‰è¿™éƒ¨åˆ†ä»¥æ»¡è¶³æ‚¨çš„éœ€æ±‚ã€‚\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # æŒ‰ block_size åˆ†å—ã€‚\n",
    "    result = {\n",
    "        k: [t[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159ec28c",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨ `group_texts` å‡½æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01f0441",
   "metadata": {},
   "outputs": [],
   "source": [
    "lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d35ad9",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ä½¿ç”¨ [DataCollatorForLanguageModeling](/docs/transformers/main/en/main_classes/data_collator#transformers.DataCollatorForLanguageModeling) åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚åœ¨æ•´ç†è¿‡ç¨‹ä¸­ï¼Œå°†å¥å­åŠ¨æ€å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„é•¿åº¦æ¯”å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦æ›´æœ‰æ•ˆã€‚\n",
    "\n",
    "Pytorch\n",
    "\n",
    "éšè— Pytorch å†…å®¹\n",
    "\n",
    "ä½¿ç”¨åºåˆ—ç»“æŸæ ‡è®°ä½œä¸ºå¡«å……æ ‡è®°ï¼Œå¹¶å°† `mlm=False`ã€‚è¿™å°†ä½¿ç”¨è¾“å…¥ä½œä¸ºå‘å³ç§»åŠ¨ä¸€ä¸ªå…ƒç´ ä½œä¸ºæ ‡ç­¾ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e402d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088145d4",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "éšè— TensorFlow å†…å®¹\n",
    "\n",
    "ä½¿ç”¨åºåˆ—ç»“æŸæ ‡è®°ä½œä¸ºå¡«å……æ ‡è®°ï¼Œå¹¶å°† `mlm=False`ã€‚è¿™å°†ä½¿ç”¨è¾“å…¥ä½œä¸ºå‘å³ç§»åŠ¨ä¸€ä¸ªå…ƒç´ ä½œä¸ºæ ‡ç­¾ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36bb800",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55855e1e",
   "metadata": {},
   "source": [
    "\n",
    "## è®­ç»ƒ\n",
    "\n",
    "Pytorch\n",
    "\n",
    "éšè— Pytorch å†…å®¹\n",
    "\n",
    "å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ [åŸºæœ¬æ•™ç¨‹](../training#train-with-pytorch-trainer)ï¼\n",
    "\n",
    "æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ [AutoModelForCausalLM](/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForCausalLM) åŠ è½½ DistilGPT2ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16259077",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3acf5346",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "1. åœ¨ [TrainingArguments](/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚\n",
    "2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨ã€‚\n",
    "3. è°ƒç”¨ [train()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) æ¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae693b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=lm_dataset[\"train\"],\n",
    "    eval_dataset=lm_dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacabde0",
   "metadata": {},
   "source": [
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [evaluate()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.evaluate) æ–¹æ³•è¯„ä¼°æ‚¨çš„æ¨¡å‹å¹¶è·å–å…¶å›°æƒ‘åº¦ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7092b3e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8254a3e4",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åä½¿ç”¨ [push_to_hub()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9dba47",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c75696",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "éšè— TensorFlow å†…å®¹\n",
    "\n",
    "å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ [åŸºæœ¬æ•™ç¨‹](../training#train-a-tensorflow-model-with-keras)ï¼\n",
    "\n",
    "è¦åœ¨ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹ï¼Œé¦–å…ˆè®¾ç½®ä¼˜åŒ–å™¨å‡½æ•°ã€å­¦ä¹ ç‡è®¡åˆ’å’Œä¸€äº›è®­ç»ƒè¶…å‚æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da7815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer, AdamWeightDecay\n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad1ab0a8",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åæ‚¨å¯ä»¥ä½¿ç”¨ [TFAutoModelForCausalLM](/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForCausalLM) åŠ è½½ DistilGPT2ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b925e474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55f04f6",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ [prepare_tf_dataset()](/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset) å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8eebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    lm_dataset[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    lm_dataset[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c85256b",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚è¯·æ³¨æ„ï¼ŒTransformers æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨æƒ³è¦æŒ‡å®šä¸€ä¸ªï¼Œå¦åˆ™ä¸éœ€è¦æŒ‡å®šæŸå¤±å‡½æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d76ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=optimizer)  # æ— éœ€æŒ‡å®šæŸå¤±å‡½æ•°"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e709633",
   "metadata": {},
   "source": [
    "\n",
    "æ‚¨å¯ä»¥é€šè¿‡åœ¨ [PushToHubCallback](/docs/transformers/main/en/main_classes/keras_callbacks#transformers.PushToHubCallback) ä¸­æŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨çš„æ¨é€ä½ç½®æ¥å®ç°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9399ca36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_eli5_clm-model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c6fea4",
   "metadata": {},
   "source": [
    "\n",
    "æœ€åï¼Œæ‚¨å‡†å¤‡å¥½å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€çºªå…ƒæ•°å’Œå›è°ƒæ¥å¾®è°ƒæ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bafcbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8667a8c7",
   "metadata": {},
   "source": [
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œæ‚¨çš„æ¨¡å‹å°†è‡ªåŠ¨ä¸Šä¼ åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨å®ƒï¼\n",
    "\n",
    "æœ‰å…³å¦‚ä½•å¾®è°ƒæ¨¡å‹è¿›è¡Œå› æœè¯­è¨€å»ºæ¨¡çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„ [PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb) æˆ– [TensorFlow ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ã€‚\n",
    "\n",
    "## æ¨ç†\n",
    "\n",
    "å¤ªå¥½äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼\n",
    "\n",
    "æƒ³å‡ºä¸€ä¸ªæ‚¨æƒ³ä»ä¸­ç”Ÿæˆæ–‡æœ¬çš„æç¤ºï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870b860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Somatic hypermutation allows the immune system to\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb70a91c",
   "metadata": {},
   "source": [
    "\n",
    "å°è¯•ä½¿ç”¨æ‚¨å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†å…¶ç”¨äº [pipeline()](/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline)ã€‚ä½¿ç”¨æ‚¨çš„æ¨¡å‹å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºæ–‡æœ¬ç”Ÿæˆçš„ `pipeline`ï¼Œå¹¶å°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e746ca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline(\"text-generation\", model=\"username/my_awesome_eli5_clm-model\")\n",
    "generator(prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df7381c",
   "metadata": {},
   "source": [
    "\n",
    "Pytorch\n",
    "\n",
    "éšè— Pytorch å†…å®¹\n",
    "\n",
    "å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å°† `input_ids` ä½œä¸º PyTorch å¼ é‡è¿”å›ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7a08d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dbc1fae",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ [generate()](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationMixin.generate) æ–¹æ³•ç”Ÿæˆæ–‡æœ¬ã€‚æœ‰å…³æ§åˆ¶ç”Ÿæˆçš„ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œå‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies) é¡µé¢ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d2e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "model = AutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aca13ad",
   "metadata": {},
   "source": [
    "\n",
    "å°†ç”Ÿæˆçš„æ ‡è®° ID è§£ç å›æ–‡æœ¬ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72896c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e79db398",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "éšè— TensorFlow å†…å®¹\n",
    "\n",
    "å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯å¹¶å°† `input_ids` ä½œä¸º TensorFlow å¼ é‡è¿”å›ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1d20872",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2566e",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ `~transformers.generation_tf_utils.TFGenerationMixin.generate` æ–¹æ³•åˆ›å»ºæ‘˜è¦ã€‚æœ‰å…³æ§åˆ¶ç”Ÿæˆçš„ä¸åŒæ–‡æœ¬ç”Ÿæˆç­–ç•¥å’Œå‚æ•°çš„æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [æ–‡æœ¬ç”Ÿæˆç­–ç•¥](../generation_strategies) é¡µé¢ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf7e725",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n",
    "outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1992ba1f",
   "metadata": {},
   "source": [
    "\n",
    "å°†ç”Ÿæˆçš„æ ‡è®° ID è§£ç å›æ–‡æœ¬ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa53572",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
