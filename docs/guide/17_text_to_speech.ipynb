{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬è½¬è¯­éŸ³ (Text-to-speech, TTS)\n",
    "\n",
    "æ–‡æœ¬è½¬è¯­éŸ³ï¼ˆTTSï¼‰æ˜¯å°†æ–‡å­—è½¬æ¢æˆè‡ªç„¶è¯­éŸ³çš„ä»»åŠ¡ï¼Œè¿™ç§è¯­éŸ³å¯ä»¥æ”¯æŒå¤šç§è¯­è¨€å’Œå¤šä¸ªè¯´è¯è€…ã€‚ç›®å‰ï¼ŒğŸ¤— Transformers ä¸­å·²ç»æä¾›äº†å¤šç§æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹ï¼Œæ¯”å¦‚ [Bark](https://huggingface.co/docs/transformers/main/en/model_doc/bark)ã€[MMS](https://huggingface.co/docs/transformers/main/en/model_doc/mms)ã€[VITS](https://huggingface.co/docs/transformers/main/en/model_doc/vits) å’Œ [SpeechT5](https://huggingface.co/docs/transformers/main/en/model_doc/speecht5)ã€‚\n",
    "\n",
    "ä½ å¯ä»¥é€šè¿‡ `text-to-audio` pipelineï¼ˆæˆ–è€…å®ƒçš„åˆ«åâ€”â€”`text-to-speech`ï¼‰è½»æ¾ç”ŸæˆéŸ³é¢‘ã€‚\n",
    "\n",
    "æœ‰äº›æ¨¡å‹ï¼Œæ¯”å¦‚ Bark è¿˜å¯ä»¥ç”Ÿæˆéè¯­è¨€è¯­éŸ³ï¼Œæ¯”å¦‚ç¬‘å£°ã€å¹æ¯å’Œå“­æ³£ï¼Œç”šè‡³å¯ä»¥æ·»åŠ éŸ³ä¹ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªä½¿ç”¨ Bark è¿›è¡Œâ€œæ–‡æœ¬è½¬è¯­éŸ³â€çš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-to-speech\", model=\"suno/bark-small\")\n",
    "text = \"[clears throat] This is a test ... and I just took a long pause.\"\n",
    "output = pipe(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™é‡Œæœ‰ä¸€ä¸ªä»£ç ç‰‡æ®µï¼Œä½ å¯ä»¥åœ¨ notebook ä¸­è¿è¡Œå®ƒæ¥è¯•å¬ç”Ÿæˆçš„éŸ³é¢‘ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(output[\"audio\"], rate=output[\"sampling_rate\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æƒ³äº†è§£æ›´å¤šå…³äº Bark å’Œå…¶ä»–é¢„è®­ç»ƒ TTS æ¨¡å‹çš„ç¤ºä¾‹ï¼Œè¯·å‚è€ƒæˆ‘ä»¬çš„[éŸ³é¢‘è¯¾ç¨‹](https://huggingface.co/learn/audio-course/chapter6/pre-trained_models)ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æƒ³è¦å¾®è°ƒä¸€ä¸ª TTS æ¨¡å‹ï¼Œç›®å‰ ğŸ¤— Transformers ä¸­å¯ç”¨çš„æ–‡æœ¬è½¬è¯­éŸ³æ¨¡å‹åªæœ‰ [SpeechT5](https://huggingface.co/docs/transformers/main/en/tasks/model_doc/speecht5) å’Œ [FastSpeech2Conformer](https://huggingface.co/docs/transformers/main/en/tasks/model_doc/fastspeech2_conformer)ï¼Œä¸è¿‡æœªæ¥ä¼šæ·»åŠ æ›´å¤šæ¨¡å‹ã€‚\n",
    "\n",
    "SpeechT5 æ˜¯åœ¨è¯­éŸ³è½¬æ–‡æœ¬å’Œæ–‡æœ¬è½¬è¯­éŸ³æ•°æ®çš„ç»„åˆä¸Šè¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œè¿™ä½¿å¾—å®ƒèƒ½å¤Ÿå­¦ä¹ ä¸€ä¸ªç»Ÿä¸€çš„éšå«è¡¨ç¤ºç©ºé—´ï¼Œè¿™ä¸ªç©ºé—´ç”±æ–‡æœ¬å’Œè¯­éŸ³å…±äº«ã€‚è¿™æ„å‘³ç€åŒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹å¯ä»¥é’ˆå¯¹ä¸åŒä»»åŠ¡è¿›è¡Œå¾®è°ƒã€‚æ­¤å¤–ï¼ŒSpeechT5 é€šè¿‡ `x-vector` è¯´è¯è€…åµŒå…¥æ”¯æŒå¤šä¸ªè¯´è¯è€…ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—çš„å…¶ä½™éƒ¨åˆ†å°†å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "1. å°†åŸæœ¬åœ¨è‹±è¯­è¯­éŸ³ä¸Šè®­ç»ƒçš„ [SpeechT5](https://huggingface.co/docs/transformers/main/en/model_doc/speecht5) å¾®è°ƒåˆ° [VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) æ•°æ®é›†çš„è·å…°è¯­ï¼ˆnlï¼‰å­é›†ä¸Šã€‚\n",
    "2. ä½¿ç”¨ä½ å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œæœ‰ä¸¤ç§æ–¹æ³•ï¼šä½¿ç”¨ pipeline æˆ–ç›´æ¥ä½¿ç”¨ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install datasets soundfile speechbrain accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éœ€è¦ä»æºä»£ç å®‰è£… ğŸ¤—Transformersï¼Œå› ä¸ºå¹¶éæ‰€æœ‰ SpeechT5 çš„åŠŸèƒ½éƒ½å·²åˆå¹¶åˆ°å®˜æ–¹ç‰ˆæœ¬ä¸­ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/transformers.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‹¥è¦æŒ‰ç…§æœ¬æŒ‡å—æ“ä½œï¼Œä½ è¿˜éœ€è¦ä¸€ä¸ª GPUã€‚å¦‚æœä½ åœ¨ notebook ä¸­å·¥ä½œï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤æ¥æ£€æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„ GPUï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äº AMD GPUï¼Œå¯ä»¥ä½¿ç”¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!rocm-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æˆ‘ä»¬é¼“åŠ±ä½ ç™»å½•ä½ çš„ Hugging Face è´¦æˆ·ï¼Œä»¥ä¾¿ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¾“å…¥ä½ çš„ä»¤ç‰Œè¿›è¡Œç™»å½•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŠ è½½æ•°æ®é›†\n",
    "\n",
    "[VoxPopuli](https://huggingface.co/datasets/facebook/voxpopuli) æ˜¯ä¸€ä¸ªå¤§è§„æ¨¡çš„å¤šè¯­ç§è¯­éŸ³è¯­æ–™åº“ï¼Œæ•°æ®æ¥æºäº2009-2020å¹´æ¬§æ´²è®®ä¼šæ´»åŠ¨å½•éŸ³ã€‚å®ƒåŒ…å«15ç§æ¬§æ´²è¯­è¨€çš„æ ‡è®°éŸ³é¢‘-è½¬å½•æ•°æ®ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨è·å…°è¯­å­é›†ï¼Œä½ ä¹Ÿå¯ä»¥é€‰æ‹©å…¶ä»–å­é›†ã€‚\n",
    "\n",
    "è¯·æ³¨æ„ï¼ŒVoxPopuli æˆ–ä»»ä½•å…¶ä»–è‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®é›†å¯èƒ½ä¸æ˜¯è®­ç»ƒ TTS æ¨¡å‹çš„æœ€ä½³é€‰æ‹©ã€‚é‚£äº›å¯¹ ASR æœ‰ç›Šçš„ç‰¹æ€§ï¼Œæ¯”å¦‚è¿‡å¤šçš„èƒŒæ™¯å™ªéŸ³ï¼Œåœ¨ TTS ä¸­é€šå¸¸æ˜¯ä¸å¸Œæœ›å‡ºç°çš„ã€‚æ‰€ä»¥ï¼Œæ‰¾åˆ°é«˜è´¨é‡ã€å¤šè¯­ç§ã€å¤šè¯´è¯è€…çš„ TTS æ•°æ®é›†ç¡®å®ç›¸å½“æœ‰æŒ‘æˆ˜æ€§ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬åŠ è½½æ•°æ®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Audio\n",
    "\n",
    "dataset = load_dataset(\"facebook/voxpopuli\", \"nl\", split=\"train\")\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20968 ä¸ªç¤ºä¾‹åº”è¯¥è¶³å¤Ÿç”¨äºå¾®è°ƒã€‚SpeechT5 è¦æ±‚éŸ³é¢‘æ•°æ®çš„é‡‡æ ·ç‡ä¸º 16 kHzï¼Œæ‰€ä»¥è¯·ç¡®ä¿æ•°æ®é›†ä¸­çš„ç¤ºä¾‹æ»¡è¶³è¿™ä¸€è¦æ±‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é¢„å¤„ç†æ•°æ®\n",
    "\n",
    "é¦–å…ˆï¼Œå®šä¹‰è¦ä½¿ç”¨çš„æ¨¡å‹ checkpoint å¹¶åŠ è½½ç›¸åº”çš„å¤„ç†å™¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5Processor\n",
    "\n",
    "checkpoint = \"microsoft/speecht5_tts\"\n",
    "processor = SpeechT5Processor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¸º SpeechT5 åˆ†è¯è¿›è¡Œæ–‡æœ¬æ¸…ç†\n",
    "\n",
    "é¦–å…ˆï¼Œæ¸…ç†æ–‡æœ¬æ•°æ®ã€‚ä½ éœ€è¦ä½¿ç”¨å¤„ç†å™¨çš„åˆ†è¯å™¨éƒ¨åˆ†æ¥å¤„ç†æ–‡æœ¬ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = processor.tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ•°æ®é›†ç¤ºä¾‹åŒ…å« `raw_text` å’Œ `normalized_text` ç‰¹å¾ã€‚åœ¨å†³å®šä½¿ç”¨å“ªä¸ªç‰¹å¾ä½œä¸ºæ–‡æœ¬è¾“å…¥æ—¶ï¼Œè€ƒè™‘åˆ° `SpeechT5` çš„åˆ†è¯å™¨æ²¡æœ‰ç”¨äºæ•°å­—çš„ä»»ä½•æ ‡è®°ã€‚åœ¨ `normalized_text` ä¸­ï¼Œæ•°å­—è¢«å†™æˆæ–‡æœ¬å½¢å¼ã€‚å› æ­¤ï¼Œå®ƒæ›´é€‚åˆä½œä¸ºè¾“å…¥æ–‡æœ¬ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨ `normalized_text`ã€‚\n",
    "\n",
    "ç”±äº `SpeechT5` æ˜¯åœ¨è‹±è¯­ä¸Šè®­ç»ƒçš„ï¼Œå®ƒå¯èƒ½æ— æ³•è¯†åˆ«è·å…°è¯­æ•°æ®é›†ä¸­çš„æŸäº›å­—ç¬¦ã€‚å¦‚æœä¿ç•™åŸæ ·ï¼Œè¿™äº›å­—ç¬¦å°†è¢«è½¬æ¢ä¸º `<unk>` æ ‡è®°ã€‚ç„¶è€Œï¼Œåœ¨è·å…°è¯­ä¸­ï¼ŒæŸäº›å­—ç¬¦å¦‚ `Ã ` ç”¨äºå¼ºè°ƒéŸ³èŠ‚ã€‚ä¸ºäº†ä¿ç•™æ–‡æœ¬çš„æ„ä¹‰ï¼Œæˆ‘ä»¬å¯ä»¥å°†è¿™ä¸ªå­—ç¬¦æ›¿æ¢ä¸ºæ™®é€šçš„ `a`ã€‚\n",
    "\n",
    "ä¸ºäº†è¯†åˆ«ä¸æ”¯æŒçš„æ ‡è®°ï¼Œä½¿ç”¨ `SpeechT5Tokenizer` æå–æ•°æ®é›†ä¸­çš„æ‰€æœ‰å”¯ä¸€å­—ç¬¦ï¼Œè¯¥åˆ†è¯å™¨ä»¥å­—ç¬¦ä½œä¸ºæ ‡è®°ã€‚ä¸ºæ­¤ï¼Œç¼–å†™ `extract_all_chars` æ˜ å°„å‡½æ•°ï¼Œå°†æ‰€æœ‰ç¤ºä¾‹çš„è½¬å½•æ–‡æœ¬è¿æ¥æˆä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå­—ç¬¦é›†åˆã€‚ç¡®ä¿åœ¨ `dataset.map()` ä¸­è®¾ç½® `batched=True` å’Œ `batch_size=-1`ï¼Œä»¥ä¾¿æ‰€æœ‰è½¬å½•æ–‡æœ¬ä¸€æ¬¡æ€§æä¾›ç»™æ˜ å°„å‡½æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_chars(batch):\n",
    "    all_text = \" \".join(batch[\"normalized_text\"])\n",
    "    vocab = list(set(all_text))\n",
    "    return {\"vocab\": [vocab], \"all_text\": [all_text]}\n",
    "\n",
    "\n",
    "vocabs = dataset.map(\n",
    "    extract_all_chars,\n",
    "    batched=True,\n",
    "    batch_size=-1,\n",
    "    keep_in_memory=True,\n",
    "    remove_columns=dataset.column_names,\n",
    ")\n",
    "\n",
    "dataset_vocab = set(vocabs[\"vocab\"][0])\n",
    "tokenizer_vocab = {k for k, _ in tokenizer.get_vocab().items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ æœ‰ä¸¤å¥—å­—ç¬¦é›†ï¼šä¸€å¥—æ˜¯æ•°æ®é›†ä¸­çš„è¯æ±‡ï¼Œå¦ä¸€å¥—æ˜¯åˆ†è¯å™¨ä¸­çš„è¯æ±‡ã€‚\n",
    "\n",
    "ä¸ºäº†è¯†åˆ«æ•°æ®é›†ä¸­ä»»ä½•ä¸æ”¯æŒçš„å­—ç¬¦ï¼Œä½ å¯ä»¥å–è¿™ä¸¤ä¸ªé›†åˆçš„å·®é›†ã€‚ç»“æœé›†åˆå°†åŒ…å«é‚£äº›åœ¨æ•°æ®é›†ä¸­ä½†ä¸åœ¨åˆ†è¯å™¨ä¸­çš„å­—ç¬¦ï¼Œå³ä¸æ”¯æŒçš„å­—ç¬¦é›†åˆã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_vocab - tokenizer_vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†å¤„ç†ä¸Šä¸€æ­¥ä¸­å¾—åˆ°çš„ä¸æ”¯æŒçš„å­—ç¬¦ï¼Œéœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°å°†è¿™äº›å­—ç¬¦æ˜ å°„åˆ°æœ‰æ•ˆçš„æ ‡è®°ã€‚è¯·æ³¨æ„ï¼Œåˆ†è¯å™¨ä¸­ç©ºæ ¼å·²ç»ç”¨ â– æ›¿æ¢ï¼Œä¸éœ€è¦å•ç‹¬å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacements = [\n",
    "    (\"Ã \", \"a\"),\n",
    "    (\"Ã§\", \"c\"),\n",
    "    (\"Ã¨\", \"e\"),\n",
    "    (\"Ã«\", \"e\"),\n",
    "    (\"Ã­\", \"i\"),\n",
    "    (\"Ã¯\", \"i\"),\n",
    "    (\"Ã¶\", \"o\"),\n",
    "    (\"Ã¼\", \"u\"),\n",
    "]\n",
    "\n",
    "\n",
    "def cleanup_text(inputs):\n",
    "    for src, dst in replacements:\n",
    "        inputs[\"normalized_text\"] = inputs[\"normalized_text\"].replace(src, dst)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "dataset = dataset.map(cleanup_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ å·²ç»å¤„ç†äº†æ–‡æœ¬ä¸­çš„ç‰¹æ®Šå­—ç¬¦ï¼ˆä¸æ”¯æŒçš„å­—ç¬¦ï¼‰ï¼Œæ¥ä¸‹æ¥è¯¥å°†æ³¨æ„åŠ›è½¬å‘éŸ³é¢‘æ•°æ®äº†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯´è¯è€…\n",
    "VoxPopuli æ•°æ®é›†åŒ…å«å¤šä½è¯´è¯è€…çš„è¯­éŸ³ï¼Œä½†æ•°æ®é›†ä¸­æœ‰å¤šå°‘ä½è¯´è¯è€…å‘¢ï¼Ÿ\n",
    "\n",
    "ä¸ºäº†ç¡®å®šè¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬å¯ä»¥ç»Ÿè®¡å…·æœ‰ç‹¬ç‰¹ç‰¹å¾çš„è¯´è¯è€…çš„æ•°é‡ä»¥åŠæ¯ä½è¯´è¯è€…ä¸ºæ•°æ®é›†è´¡çŒ®çš„ç¤ºä¾‹æ•°é‡ã€‚è€ƒè™‘åˆ°æ•°æ®é›†ä¸­ä¸€å…±æœ‰ 20,968 ä¸ªç¤ºä¾‹ï¼Œè¿™äº›ä¿¡æ¯å°†å¸®åŠ©æˆ‘ä»¬æ›´å¥½åœ°ç†è§£è¯´è¯è€…å’Œç¤ºä¾‹åœ¨æ•°æ®ä¸­çš„åˆ†å¸ƒæƒ…å†µã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "speaker_counts = defaultdict(int)\n",
    "\n",
    "for speaker_id in dataset[\"speaker_id\"]:\n",
    "    speaker_counts[speaker_id] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€šè¿‡ç»˜åˆ¶ç›´æ–¹å›¾ï¼Œä½ å¯ä»¥äº†è§£æ¯ä½è¯´è¯è€…æœ‰å¤šå°‘æ•°æ®ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(speaker_counts.values(), bins=20)\n",
    "plt.ylabel(\"Speakers\")\n",
    "plt.xlabel(\"Examples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç›´æ–¹å›¾æ˜¾ç¤ºï¼Œæ•°æ®é›†ä¸­å¤§çº¦ä¸‰åˆ†ä¹‹ä¸€çš„è¯´è¯è€…æ‹¥æœ‰å°‘äº 100 ä¸ªç¤ºä¾‹ï¼Œè€Œå¤§çº¦ 10 ä½è¯´è¯è€…æ‹¥æœ‰è¶…è¿‡ 500 ä¸ªç¤ºä¾‹ã€‚\n",
    "\n",
    "ä¸ºäº†æé«˜è®­ç»ƒæ•ˆç‡å¹¶å¹³è¡¡æ•°æ®é›†ï¼Œæˆ‘ä»¬å¯ä»¥å°†æ•°æ®é™åˆ¶åœ¨æ‹¥æœ‰ 100 åˆ° 400 ä¸ªç¤ºä¾‹çš„è¯´è¯è€…èŒƒå›´å†…ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_speaker(speaker_id):\n",
    "    return 100 <= speaker_counts[speaker_id] <= 400\n",
    "\n",
    "\n",
    "dataset = dataset.filter(select_speaker, input_columns=[\"speaker_id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬æ£€æŸ¥ä¸€ä¸‹è¿˜å‰©ä¸‹å¤šå°‘ä½è¯´è¯è€…ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(dataset[\"speaker_id\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è®©æˆ‘ä»¬çœ‹çœ‹è¿˜å‰©ä¸‹å¤šå°‘ä¸ªç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ ç°åœ¨å¤§çº¦æœ‰10,000 ä¸ªç¤ºä¾‹ï¼Œæ¥è‡ªå¤§çº¦ 40 ä½ç‹¬ç‰¹çš„è¯´è¯è€…ï¼Œè¿™åº”è¯¥æ˜¯è¶³å¤Ÿçš„ã€‚\n",
    "\n",
    "è¯·æ³¨æ„ï¼Œå¦‚æœç¤ºä¾‹è¾ƒé•¿ï¼Œé‚£äº›åªæœ‰å°‘é‡ç¤ºä¾‹çš„è¯´è¯è€…å®é™…ä¸Šå¯èƒ½ä¼šæ‹¥æœ‰æ›´å¤šçš„éŸ³é¢‘æ•°æ®ã€‚ç„¶è€Œï¼Œç¡®å®šæ¯ä½è¯´è¯è€…çš„æ€»éŸ³é¢‘é‡éœ€è¦éå†æ•´ä¸ªæ•°æ®é›†ï¼Œè¿™æ˜¯ä¸€ä¸ªæ¯”è¾ƒè€—æ—¶çš„è¿‡ç¨‹ï¼Œæ¶‰åŠåŠ è½½å’Œè§£ç æ¯ä¸ªéŸ³é¢‘æ–‡ä»¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨è¿™é‡Œè·³è¿‡è¿™ä¸€æ­¥ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯´è¯è€…åµŒå…¥\n",
    "ä¸ºäº†è®© TTS æ¨¡å‹èƒ½å¤ŸåŒºåˆ†å¤šä½è¯´è¯è€…ï¼Œä½ éœ€è¦ä¸ºæ¯ä¸ªç¤ºä¾‹åˆ›å»ºä¸€ä¸ªè¯´è¯è€…åµŒå…¥ã€‚è¯´è¯è€…åµŒå…¥æ˜¯æ¨¡å‹çš„é¢å¤–è¾“å…¥ï¼Œç”¨äºæ•æ‰ç‰¹å®šè¯´è¯è€…çš„å£°éŸ³ç‰¹å¾ã€‚\n",
    "\n",
    "ä¸ºäº†ç”Ÿæˆè¿™äº›è¯´è¯è€…åµŒå…¥ï¼Œè¿™é‡Œä½¿ç”¨ `SpeechBrain` çš„é¢„è®­ç»ƒæ¨¡å‹ [spkrec-xvect-voxceleb](https://huggingface.co/speechbrain/spkrec-xvect-voxceleb)ã€‚\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ªå‡½æ•° `create_speaker_embedding()`ï¼Œå®ƒæ¥å—ä¸€ä¸ªè¾“å…¥éŸ³é¢‘æ³¢å½¢ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªåŒ…å«ç›¸åº”è¯´è¯è€…åµŒå…¥çš„ 512 å…ƒç´ å‘é‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from speechbrain.inference.classifiers import EncoderClassifier\n",
    "\n",
    "spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "speaker_model = EncoderClassifier.from_hparams(\n",
    "    source=spk_model_name,\n",
    "    run_opts={\"device\": device},\n",
    "    savedir=os.path.join(\"/tmp\", spk_model_name),\n",
    ")\n",
    "\n",
    "\n",
    "def create_speaker_embedding(waveform):\n",
    "    with torch.no_grad():\n",
    "        speaker_embeddings = speaker_model.encode_batch(torch.tensor(waveform))\n",
    "        speaker_embeddings = torch.nn.functional.normalize(speaker_embeddings, dim=2)\n",
    "        speaker_embeddings = speaker_embeddings.squeeze().cpu().numpy()\n",
    "    return speaker_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œ`speechbrain/spkrec-xvect-voxceleb` æ¨¡å‹æ˜¯åœ¨ `VoxCeleb` æ•°æ®é›†çš„è‹±è¯­è¯­éŸ³ä¸Šè®­ç»ƒçš„ï¼Œè€Œæœ¬æŒ‡å—ä¸­çš„è®­ç»ƒç¤ºä¾‹æ˜¯è·å…°è¯­ã€‚å°½ç®¡æˆ‘ä»¬ç›¸ä¿¡è¿™ä¸ªæ¨¡å‹ä»èƒ½ä¸ºæˆ‘ä»¬çš„è·å…°è¯­æ•°æ®é›†ç”Ÿæˆåˆç†çš„è¯´è¯è€…åµŒå…¥ï¼Œä½†è¿™ç§å‡è®¾åœ¨æ‰€æœ‰æƒ…å†µä¸‹å¯èƒ½å¹¶ä¸æˆç«‹ã€‚\n",
    "\n",
    "ä¸ºäº†è·å¾—æœ€ä½³ç»“æœï¼Œæˆ‘ä»¬å»ºè®®é¦–å…ˆåœ¨ç›®æ ‡è¯­éŸ³ä¸Šè®­ç»ƒä¸€ä¸ª `X-vector` æ¨¡å‹ã€‚è¿™å°†ç¡®ä¿æ¨¡å‹èƒ½å¤Ÿæ›´å¥½åœ°æ•æ‰è·å…°è¯­ä¸­ç‹¬ç‰¹çš„å£°éŸ³ç‰¹å¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å¤„ç†æ•°æ®é›†\n",
    "\n",
    "æœ€åï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ•°æ®è½¬æ¢æˆæ¨¡å‹éœ€è¦çš„æ ¼å¼ã€‚æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªåä¸º`prepare_dataset`çš„å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°æ¥æ”¶ä¸€ä¸ªæ ·æœ¬ï¼Œç„¶åç”¨`SpeechT5Processor`å·¥å…·å¯¹è¾“å…¥çš„æ–‡å­—è¿›è¡Œç¼–ç ï¼ŒåŒæ—¶æŠŠç›®æ ‡éŸ³é¢‘è½¬æ¢æˆ`å¯¹æ•°æ¢…å°”é¢‘è°±å›¾`ã€‚æ­¤å¤–ï¼Œè¿˜è¦åŠ å…¥`speaker embeddings`ä½œä¸ºé¢å¤–çš„è¾“å…¥ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "\n",
    "    example = processor(\n",
    "        text=example[\"normalized_text\"],\n",
    "        audio_target=audio[\"array\"],\n",
    "        sampling_rate=audio[\"sampling_rate\"],\n",
    "        return_attention_mask=False,\n",
    "    )\n",
    "\n",
    "    # strip off the batch dimension\n",
    "    example[\"labels\"] = example[\"labels\"][0]\n",
    "\n",
    "    # use SpeechBrain to obtain x-vector\n",
    "    example[\"speaker_embeddings\"] = create_speaker_embedding(audio[\"array\"])\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†ç¡®è®¤æ•°æ®å¤„ç†æ˜¯å¦æ­£ç¡®ï¼Œæˆ‘ä»¬å¯ä»¥æŸ¥çœ‹ä¸€ä¸ªå…·ä½“çš„æ ·æœ¬è¿›è¡ŒéªŒè¯ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example = prepare_dataset(dataset[0])\n",
    "list(processed_example.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¯´è¯è€…çš„åµŒå…¥ä¿¡æ¯åº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å«512ä¸ªå…ƒç´ çš„å‘é‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_example[\"speaker_embeddings\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ ‡è®°åº”è¯¥æ˜¯ä¸€ä¸ªåŒ…å«80ä¸ªæ¢…å°”é¢‘æ®µçš„æ—¥å¿—æ¢…å°”é¢‘è°±å›¾ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(processed_example[\"labels\"].T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°è´´å£«ï¼šå¦‚æœä½ è§‰å¾—è¿™ä¸ªé¢‘è°±å›¾çœ‹èµ·æ¥å¾ˆä¹±ï¼Œå¯èƒ½æ˜¯å› ä¸ºä½ ä¹ æƒ¯äº†ä½é¢‘åœ¨åº•éƒ¨ã€é«˜é¢‘åœ¨é¡¶éƒ¨çš„æ˜¾ç¤ºæ–¹å¼ã€‚ä¸è¿‡ï¼Œåœ¨ä½¿ç”¨`matplotlib`åº“ç»˜åˆ¶é¢‘è°±å›¾æ—¶ï¼Œyè½´æ˜¯ç¿»è½¬çš„ï¼Œæ‰€ä»¥é¢‘è°±å›¾æ˜¾ç¤ºçš„æ˜¯é¢ å€’çš„ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¿™ä¸ªå¤„ç†å‡½æ•°åº”ç”¨åˆ°æ•´ä¸ªæ•°æ®é›†ä¸Šã€‚è¿™ä¸ªè¿‡ç¨‹å¤§æ¦‚éœ€è¦5åˆ°10åˆ†é’Ÿçš„æ—¶é—´ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(prepare_dataset, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯èƒ½ä¼šçœ‹åˆ°ä¸€ä¸ªè­¦å‘Šï¼Œæç¤ºæ•°æ®é›†ä¸­æœ‰äº›æ ·æœ¬çš„è¾“å…¥é•¿åº¦è¶…è¿‡äº†æ¨¡å‹èƒ½å¤„ç†çš„æœ€å¤§é™åˆ¶ï¼ˆ600ä¸ªæ ‡è®°ï¼‰ã€‚æˆ‘ä»¬éœ€è¦ä»æ•°æ®é›†ä¸­ç§»é™¤è¿™äº›æ ·æœ¬ã€‚ä¸ºäº†æ›´ç¨³å¦¥åœ°å¤„ç†ï¼Œç¡®ä¿æ‰¹é‡å¤„ç†æ›´é«˜æ•ˆï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥ç­›é€‰ï¼Œç§»é™¤æ‰€æœ‰è¶…è¿‡200ä¸ªæ ‡è®°çš„æ ·æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_not_too_long(input_ids):\n",
    "    input_length = len(input_ids)\n",
    "    return input_length < 200\n",
    "\n",
    "\n",
    "dataset = dataset.filter(is_not_too_long, input_columns=[\"input_ids\"])\n",
    "len(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªåŸºæœ¬çš„è®­ç»ƒ/æµ‹è¯•æ•°æ®æ‹†åˆ†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.train_test_split(test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ•°æ®æ•´ç†å™¨\n",
    "ä¸ºäº†æŠŠå¤šä¸ªæ ·æœ¬åˆå¹¶æˆä¸€ä¸ªæ‰¹æ¬¡å¤„ç†ï¼Œæˆ‘ä»¬éœ€è¦å®šä¹‰ä¸€ä¸ªè‡ªå®šä¹‰çš„æ•°æ®æ•´ç†å™¨ã€‚è¿™ä¸ªæ•´ç†å™¨ä¼šç”¨å¡«å……æ ‡è®°æ¥è¡¥é½è¾ƒçŸ­çš„åºåˆ—ï¼Œç¡®ä¿æ‰€æœ‰æ ·æœ¬é•¿åº¦ä¸€è‡´ã€‚å¯¹äºé¢‘è°±å›¾æ ‡ç­¾ï¼Œå¡«å……éƒ¨åˆ†ä¼šç”¨ç‰¹æ®Šå€¼-100æ¥ä»£æ›¿ã€‚è¿™ä¸ªç‰¹æ®Šå€¼å‘Šè¯‰æ¨¡å‹åœ¨è®¡ç®—é¢‘è°±å›¾æŸå¤±æ—¶ï¼Œå¿½ç•¥è¿™äº›å¡«å……éƒ¨åˆ†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Any, Dict, List, Union\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class TTSDataCollatorWithPadding:\n",
    "    processor: Any\n",
    "\n",
    "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids = [{\"input_ids\": feature[\"input_ids\"]} for feature in features]\n",
    "        label_features = [{\"input_values\": feature[\"labels\"]} for feature in features]\n",
    "        speaker_features = [feature[\"speaker_embeddings\"] for feature in features]\n",
    "\n",
    "        # collate the inputs and targets into a batch\n",
    "        batch = processor.pad(input_ids=input_ids, labels=label_features, return_tensors=\"pt\")\n",
    "\n",
    "        # replace padding with -100 to ignore loss correctly\n",
    "        batch[\"labels\"] = batch[\"labels\"].masked_fill(batch.decoder_attention_mask.unsqueeze(-1).ne(1), -100)\n",
    "\n",
    "        # not used during fine-tuning\n",
    "        del batch[\"decoder_attention_mask\"]\n",
    "\n",
    "        # round down target lengths to multiple of reduction factor\n",
    "        if model.config.reduction_factor > 1:\n",
    "            target_lengths = torch.tensor([len(feature[\"input_values\"]) for feature in label_features])\n",
    "            target_lengths = target_lengths.new(\n",
    "                [length - length % model.config.reduction_factor for length in target_lengths]\n",
    "            )\n",
    "            max_length = max(target_lengths)\n",
    "            batch[\"labels\"] = batch[\"labels\"][:, :max_length]\n",
    "\n",
    "        # also add in the speaker embeddings\n",
    "        batch[\"speaker_embeddings\"] = torch.tensor(speaker_features)\n",
    "\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨`SpeechT5`æ¨¡å‹ä¸­ï¼Œè§£ç å™¨éƒ¨åˆ†çš„è¾“å…¥é•¿åº¦ä¼šè¢«å‡åŠã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå®ƒä¼šä»ç›®æ ‡åºåˆ—ä¸­æ¯éš”ä¸€ä¸ªæ—¶é—´æ­¥ä¸¢å¼ƒä¸€ä¸ªå…ƒç´ ã€‚ç„¶åï¼Œè§£ç å™¨ä¼šé¢„æµ‹ä¸€ä¸ªé•¿åº¦æ˜¯åŸæ¥ä¸¤å€çš„åºåˆ—ã€‚ç”±äºåŸå§‹ç›®æ ‡åºåˆ—çš„é•¿åº¦å¯èƒ½æ˜¯å¥‡æ•°ï¼Œæ‰€ä»¥æ•°æ®æ•´ç†å™¨ä¼šç¡®ä¿å°†æ‰¹æ¬¡çš„æœ€å¤§é•¿åº¦è°ƒæ•´ä¸º2çš„å€æ•°ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = TTSDataCollatorWithPadding(processor=processor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒæ¨¡å‹\n",
    "ä»ä¹‹å‰ç”¨äºåŠ è½½å¤„ç†å™¨çš„åŒä¸€ä¸ª checkpoint ä¸­åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SpeechT5ForTextToSpeech\n",
    "\n",
    "model = SpeechT5ForTextToSpeech.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`use_cache=True`é€‰é¡¹ä¸æ¢¯åº¦ checkpointing åŠŸèƒ½ä¸å…¼å®¹ã€‚ä¸ºäº†é¡ºåˆ©è¿›è¡Œè®­ç»ƒï¼Œæˆ‘ä»¬éœ€è¦å…³é—­è¿™ä¸ªé€‰é¡¹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.use_cache = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰è®­ç»ƒå‚æ•°ã€‚åœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—ä»»ä½•è¯„ä¼°æŒ‡æ ‡ã€‚æˆ‘ä»¬åªå…³æ³¨æŸå¤±å€¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"speecht5_finetuned_voxpopuli_nl\",  # change to a repo name of your choice\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=1e-5,\n",
    "    warmup_steps=500,\n",
    "    max_steps=4000,\n",
    "    gradient_checkpointing=True,\n",
    "    fp16=True,\n",
    "    eval_strategy=\"steps\",\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,\n",
    "    logging_steps=25,\n",
    "    report_to=[\"tensorboard\"],\n",
    "    load_best_model_at_end=True,\n",
    "    greater_is_better=False,\n",
    "    label_names=[\"labels\"],\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ›å»ºä¸€ä¸ª`Trainer`å¯¹è±¡ï¼Œå¹¶å°†æ¨¡å‹ã€æ•°æ®é›†å’Œæ•°æ®æ•´ç†å™¨ä¼ é€’ç»™å®ƒè¿›è¡Œå®ä¾‹åŒ–ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Seq2SeqTrainer\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    args=training_args,\n",
    "    model=model,\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    processing_class=processor,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å‡†å¤‡å·¥ä½œå®Œæˆåï¼Œä½ å°±å¯ä»¥å¼€å§‹è®­ç»ƒäº†ï¼è®­ç»ƒè¿‡ç¨‹å¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ã€‚æ ¹æ®ä½ çš„GPUæ€§èƒ½ï¼Œå¯åŠ¨è®­ç»ƒæ—¶å¯èƒ½ä¼šé‡åˆ°CUDAâ€œå†…å­˜ä¸è¶³â€çš„é”™è¯¯ã€‚å¦‚æœå‡ºç°è¿™ç§æƒ…å†µï¼Œä½ å¯ä»¥å°†`per_device_train_batch_size`å‡åŠï¼ŒåŒæ—¶å°†`gradient_accumulation_steps`åŠ å€æ¥è¡¥å¿ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¸ºäº†ç¡®ä¿checkpointèƒ½ä¸pipelineä¸€èµ·ä½¿ç”¨ï¼Œè®°å¾—æŠŠå¤„ç†å™¨å’Œcheckpointä¸€èµ·ä¿å­˜ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.save_pretrained(\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å°†æœ€ç»ˆæ¨¡å‹ä¸Šä¼ åˆ°ğŸ¤— Hubï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹æ¨ç†\n",
    "\n",
    "### ä½¿ç”¨pipelineè¿›è¡Œæ¨ç†\n",
    "\n",
    "å¤ªæ£’äº†ï¼Œç°åœ¨ä½ å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥å¼€å§‹ç”¨å®ƒæ¥è¿›è¡Œæ¨ç†äº†ï¼é¦–å…ˆï¼Œæˆ‘ä»¬æ¥äº†è§£ä¸€ä¸‹å¦‚ä½•å°†å®ƒä¸ç›¸åº”çš„ pipeline ç»“åˆä½¿ç”¨ã€‚è®©æˆ‘ä»¬ç”¨ä½ çš„ checkpoint åˆ›å»ºä¸€ä¸ª`text-to-speech`çš„ pipeline ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(\"text-to-speech\", model=\"YOUR_ACCOUNT_NAME/speecht5_finetuned_voxpopuli_nl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é€‰æ‹©ä¸€æ®µä½ æƒ³è¦ç”¨è·å…°è¯­æœ—è¯»çš„æ–‡å­—ï¼Œæ¯”å¦‚ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦å°† SpeechT5 ä¸ pipeline ä¸€èµ·ä½¿ç”¨ï¼Œä½ éœ€è¦ä¸€ä¸ª`speaker embeddings`ã€‚æˆ‘ä»¬å¯ä»¥ä»æµ‹è¯•æ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ä¸­è·å–å®ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"test\"][304]\n",
    "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ å¯ä»¥å°†æ–‡æœ¬å’Œè¯´è¯è€…åµŒå…¥ä¼ é€’ç»™ pipelineï¼Œå®ƒä¼šè‡ªåŠ¨å¤„ç†å‰©ä¸‹çš„æ­¥éª¤ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forward_params = {\"speaker_embeddings\": speaker_embeddings}\n",
    "output = pipe(text, forward_params=forward_params)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åä½ å¯ä»¥è†å¬ç”Ÿæˆçš„ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "Audio(output['audio'], rate=output['sampling_rate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰‹åŠ¨è¿è¡Œæ¨ç†\n",
    "\n",
    "ä½ ä¹Ÿå¯ä»¥åœ¨ä¸ä½¿ç”¨ pipeline çš„æƒ…å†µä¸‹å®ç°ç›¸åŒçš„æ¨ç†ç»“æœï¼Œä½†è¿™éœ€è¦æ›´å¤šæ­¥éª¤ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä» ğŸ¤— Hub åŠ è½½æ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SpeechT5ForTextToSpeech.from_pretrained(\"YOUR_ACCOUNT/speecht5_finetuned_voxpopuli_nl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»æµ‹è¯•æ•°æ®é›†ä¸­æŒ‘é€‰ä¸€ä¸ªä¾‹å­ï¼Œè·å–`speaker embeddings`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = dataset[\"test\"][304]\n",
    "speaker_embeddings = torch.tensor(example[\"speaker_embeddings\"]).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å®šä¹‰è¾“å…¥æ–‡æœ¬å¹¶å°†å…¶æ ‡è®°åŒ–(tokenize)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"hallo allemaal, ik praat nederlands. groetjes aan iedereen!\"\n",
    "inputs = processor(text=text, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ä½ çš„æ¨¡å‹åˆ›å»ºé¢‘è°±å›¾ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectrogram = model.generate_speech(inputs[\"input_ids\"], speaker_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿˜å¯ä»¥å¯è§†åŒ–é¢‘è°±å›¾ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.imshow(spectrogram.T)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼Œä½¿ç”¨å£°ç å™¨ï¼ˆvocoderï¼‰å°†é¢‘è°±å›¾è½¬æ¢ä¸ºéŸ³é¢‘ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    speech = vocoder(spectrogram)\n",
    "\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(speech.numpy(), rate=16000)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ ¹æ®æˆ‘ä»¬çš„ç»éªŒï¼Œè¦ä»è¿™ä¸ªæ¨¡å‹ä¸­è·å¾—æ»¡æ„çš„ç»“æœå¯èƒ½æœ‰ä¸€å®šéš¾åº¦ã€‚`speaker embeddings`çš„è´¨é‡ä¼¼ä¹æ˜¯ä¸€ä¸ªå…³é”®å› ç´ ã€‚ç”±äºSpeechT5æ˜¯ä½¿ç”¨è‹±è¯­xå‘é‡è¿›è¡Œé¢„è®­ç»ƒçš„ï¼Œæ‰€ä»¥åœ¨ä½¿ç”¨è‹±è¯­çš„`speaker embeddings`æ—¶è¡¨ç°æœ€ä½³ã€‚å¦‚æœåˆæˆçš„è¯­éŸ³æ•ˆæœä¸ä½³ï¼Œå»ºè®®å°è¯•ä½¿ç”¨ä¸åŒçš„`speaker embeddings`ã€‚\n",
    "\n",
    "å¢åŠ è®­ç»ƒæ—¶é—´ä¹Ÿå¯èƒ½æå‡ç»“æœçš„è´¨é‡ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåˆæˆçš„è¯­éŸ³æ˜æ˜¾æ˜¯è·å…°è¯­è€Œéè‹±è¯­ï¼Œå¹¶ä¸”ç¡®å®æ•æ‰åˆ°äº† speaker çš„è¯­éŸ³ç‰¹å¾ï¼ˆä¸ç¤ºä¾‹ä¸­çš„åŸå§‹éŸ³é¢‘ç›¸æ¯”ï¼‰ã€‚\n",
    "\n",
    "å¦ä¸€ä¸ªå¯ä»¥å°è¯•çš„ç‚¹æ˜¯æ¨¡å‹çš„é…ç½®ã€‚ä¾‹å¦‚ï¼Œå¯ä»¥å°è¯•è®¾ç½®`config.reduction_factor = 1`ï¼Œçœ‹çœ‹æ˜¯å¦ä¼šæœ‰æ‰€æ”¹å–„ã€‚\n",
    "\n",
    "æœ€åï¼Œå¿…é¡»è€ƒè™‘åˆ°ä¼¦ç†é—®é¢˜ã€‚è™½ç„¶TTSæŠ€æœ¯æœ‰è®¸å¤šæœ‰ç›Šçš„åº”ç”¨ï¼Œä½†ä¹Ÿå¯èƒ½è¢«ç”¨äºæ¶æ„ç›®çš„ï¼Œæ¯”å¦‚åœ¨æœªç»çŸ¥æƒ…æˆ–åŒæ„çš„æƒ…å†µä¸‹æ¨¡ä»¿æŸäººçš„å£°éŸ³ã€‚è¯·åŠ¡å¿…è°¨æ…å’Œè´Ÿè´£ä»»åœ°ä½¿ç”¨TTSæŠ€æœ¯ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
