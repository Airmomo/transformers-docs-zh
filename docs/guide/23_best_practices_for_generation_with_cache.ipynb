{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44b9ec3f",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ç¼“å­˜ä¼˜åŒ–ç”Ÿæˆçš„æœ€ä½³å®è·µ\n",
    "\n",
    "é«˜æ•ˆçš„ç¼“å­˜å¯¹äºä¼˜åŒ–å„ç§ç”Ÿæˆä»»åŠ¡ï¼ˆåŒ…æ‹¬æ–‡æœ¬ç”Ÿæˆã€ç¿»è¯‘ã€æ‘˜è¦å’Œå…¶ä»–åŸºäº Transformer çš„åº”ç”¨ç¨‹åºï¼‰ä¸­æ¨¡å‹çš„æ€§èƒ½è‡³å…³é‡è¦ã€‚æœ‰æ•ˆçš„ç¼“å­˜æœ‰åŠ©äºå‡å°‘è®¡ç®—æ—¶é—´å¹¶æé«˜å“åº”é€Ÿåº¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å®æ—¶æˆ–èµ„æºå¯†é›†å‹åº”ç”¨ç¨‹åºä¸­ã€‚\n",
    "\n",
    "Transformers æ”¯æŒå„ç§ç¼“å­˜æ–¹æ³•ï¼Œåˆ©ç”¨â€œCacheâ€ç±»æ¥æŠ½è±¡å’Œç®¡ç†ç¼“å­˜é€»è¾‘ã€‚æœ¬æ–‡æ¦‚è¿°äº†ä½¿ç”¨è¿™äº›ç±»ä»¥æœ€å¤§åŒ–æ€§èƒ½å’Œæ•ˆç‡çš„æœ€ä½³å®è·µã€‚è¯·æŸ¥çœ‹ [API æ–‡æ¡£](./internal/generation_utils) ä¸­æ‰€æœ‰å¯ç”¨çš„ `Cache` ç±»ã€‚\n",
    "\n",
    "## ä»€ä¹ˆæ˜¯ç¼“å­˜ä»¥åŠä¸ºä»€ä¹ˆæˆ‘ä»¬åº”è¯¥å…³æ³¨ï¼Ÿ\n",
    "\n",
    "æƒ³è±¡ä¸€ä¸‹ï¼Œä½ æ­£åœ¨ä¸æŸäººäº¤è°ˆï¼Œè€Œä¸æ˜¯è®°ä½ä¹‹å‰æ‰€è¯´çš„å†…å®¹ï¼Œæ¯æ¬¡ä½ å›åº”æ—¶éƒ½å¿…é¡»ä»å¤´å¼€å§‹ã€‚è¿™ä¼šå¾ˆæ…¢ä¸”æ•ˆç‡ä½ä¸‹ï¼Œå¯¹å§ï¼Ÿåœ¨ Transformer æ¨¡å‹çš„ä¸–ç•Œä¸­ï¼Œæœ‰ä¸€ä¸ªç±»ä¼¼çš„æ¦‚å¿µï¼Œé‚£å°±æ˜¯é”®å€¼ç¼“å­˜ï¼ˆKV Cacheï¼‰çš„åº”ç”¨ã€‚\n",
    "\n",
    "KV ç¼“å­˜å¯¹äºä¼˜åŒ–è‡ªå›å½’æ¨¡å‹ä¸­çš„ç”Ÿæˆæ˜¯å¿…è¦çš„ï¼Œåœ¨è¿™ç§æ¨¡å‹ä¸­ï¼Œæ¨¡å‹é€šè¿‡é€ä¸ªé¢„æµ‹æ–‡æœ¬æ¥ç”Ÿæˆã€‚è¿™ä¸ªè¿‡ç¨‹å¯èƒ½å¾ˆæ…¢ï¼Œå› ä¸ºæ¨¡å‹ä¸€æ¬¡åªèƒ½ç”Ÿæˆä¸€ä¸ªæ ‡è®°ï¼Œå¹¶ä¸”æ¯ä¸ªæ–°çš„é¢„æµ‹éƒ½ä¾èµ–äºå…ˆå‰çš„ä¸Šä¸‹æ–‡ã€‚è¿™æ„å‘³ç€ï¼Œè¦åœ¨ç”Ÿæˆä¸­é¢„æµ‹ç¬¬ 1000 ä¸ªæ ‡è®°ï¼Œä½ éœ€è¦æ¥è‡ªå‰ 999 ä¸ªæ ‡è®°çš„ä¿¡æ¯ï¼Œè¿™äº›ä¿¡æ¯ä»¥å¯¹è¿™äº›æ ‡è®°çš„è¡¨ç¤ºè¿›è¡Œä¸€äº›çŸ©é˜µä¹˜æ³•çš„å½¢å¼å‡ºç°ã€‚ä½†æ˜¯ï¼Œè¦é¢„æµ‹ç¬¬ 1001 ä¸ªæ ‡è®°ï¼Œä½ è¿˜éœ€è¦æ¥è‡ªå‰ 999 ä¸ªæ ‡è®°çš„ç›¸åŒä¿¡æ¯ï¼Œä»¥åŠæ¥è‡ªç¬¬ 1000 ä¸ªæ ‡è®°çš„é¢å¤–ä¿¡æ¯ã€‚è¿™å°±æ˜¯é”®å€¼ç¼“å­˜ç”¨äºä¼˜åŒ–é¡ºåºç”Ÿæˆè¿‡ç¨‹çš„åœ°æ–¹ï¼Œé€šè¿‡å­˜å‚¨å…ˆå‰çš„è®¡ç®—ç»“æœä»¥åœ¨åç»­æ ‡è®°ä¸­é‡ç”¨ï¼Œè¿™æ ·å®ƒä»¬å°±ä¸éœ€è¦å†æ¬¡è®¡ç®—ã€‚\n",
    "\n",
    "æ›´å…·ä½“åœ°è¯´ï¼Œé”®å€¼ç¼“å­˜å……å½“è¿™äº›ç”Ÿæˆæ¨¡å‹çš„å†…å­˜åº“ï¼Œæ¨¡å‹å­˜å‚¨æ¥è‡ªå…ˆå‰å¤„ç†æ ‡è®°çš„è‡ªæ³¨æ„åŠ›å±‚çš„é”®å€¼å¯¹ã€‚é€šè¿‡å­˜å‚¨è¿™äº›ä¿¡æ¯ï¼Œæ¨¡å‹å¯ä»¥é¿å…å†—ä½™è®¡ç®—ï¼Œè€Œæ˜¯ä»ç¼“å­˜ä¸­æ£€ç´¢å…ˆå‰æ ‡è®°çš„é”®å’Œå€¼ã€‚è¯·æ³¨æ„ï¼Œç¼“å­˜åªèƒ½åœ¨æ¨ç†ä¸­ä½¿ç”¨ï¼Œåœ¨è®­ç»ƒæ—¶åº”ç¦ç”¨ï¼Œå¦åˆ™å¯èƒ½ä¼šå¯¼è‡´æ„å¤–çš„é”™è¯¯ã€‚\n",
    "\n",
    "_å¯¹äºå–œæ¬¢æ·±å…¥ç ”ç©¶çš„è¯»è€…_\n",
    "\n",
    "### ç¼“å­˜å¯¹è±¡å¦‚ä½•åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­å·¥ä½œ\n",
    "\n",
    "å½“åœ¨è¾“å…¥ä¸­ä½¿ç”¨ç¼“å­˜å¯¹è±¡æ—¶ï¼Œæ³¨æ„åŠ›æ¨¡å—æ‰§è¡Œå‡ ä¸ªå…³é”®æ­¥éª¤ï¼Œä»¥æ— ç¼åœ°æ•´åˆè¿‡å»å’Œç°åœ¨çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "æ³¨æ„åŠ›æ¨¡å—å°†å½“å‰é”®å€¼ä¸å­˜å‚¨åœ¨ç¼“å­˜ä¸­çš„è¿‡å»é”®å€¼è¿æ¥èµ·æ¥ã€‚è¿™å¯¼è‡´æ³¨æ„åŠ›æƒé‡å½¢çŠ¶ä¸º `(new_tokens_length, past_kv_length + new_tokens_length)`ã€‚æœ¬è´¨ä¸Šï¼Œè¿‡å»å’Œå½“å‰çš„é”®å€¼è¢«ç»„åˆèµ·æ¥è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œç¡®ä¿æ¨¡å‹è€ƒè™‘å…ˆå‰çš„ä¸Šä¸‹æ–‡å’Œæ–°çš„è¾“å…¥ã€‚è¿æ¥çš„é”®å€¼ç”¨äºè®¡ç®—æ³¨æ„åŠ›åˆ†æ•°ï¼Œä»è€Œäº§ç”Ÿå½¢çŠ¶ä¸º `(new_tokens_length, past_kv_length + new_tokens_length)` çš„æ³¨æ„åŠ›æƒé‡ã€‚\n",
    "\n",
    "å› æ­¤ï¼Œå½“è¿­ä»£è°ƒç”¨ `forward()` è€Œä¸æ˜¯ `generate()` æ–¹æ³•æ—¶ï¼Œç¡®ä¿æ³¨æ„åŠ›æ©ç çš„å½¢çŠ¶ä¸è¿‡å»å’Œå½“å‰é”®å€¼çš„ç»„åˆé•¿åº¦ç›¸åŒ¹é…è‡³å…³é‡è¦ã€‚æ³¨æ„åŠ›æ©ç çš„å½¢çŠ¶åº”è¯¥æ˜¯ `(batch_size, past_kv_length + new_tokens_length)`ã€‚å½“ä½ è°ƒç”¨ `generate()` æ–¹æ³•æ—¶ï¼Œè¿™é€šå¸¸ç”±å†…éƒ¨å¤„ç†ã€‚å¦‚æœä½ æƒ³è¦ä½¿ç”¨ Cache ç±»å®ç°è‡ªå·±çš„ç”Ÿæˆå¾ªç¯ï¼Œè¯·è€ƒè™‘è¿™ä¸€ç‚¹ï¼Œå¹¶ä¸ºå½“å‰å’Œè¿‡å»æ ‡è®°å‡†å¤‡æ³¨æ„åŠ›æ©ç ã€‚\n",
    "\n",
    "ç¼–å†™è‡ªå·±çš„ç”Ÿæˆå¾ªç¯æ—¶ï¼Œä½ éœ€è¦äº†è§£çš„ä¸€ä¸ªé‡è¦æ¦‚å¿µæ˜¯ `cache_position`ã€‚å¦‚æœä½ æƒ³è¦é€šè¿‡è°ƒç”¨ `forward()` æ¥é‡ç”¨å·²ç»å¡«å……çš„ Cache å¯¹è±¡ï¼Œä½ å¿…é¡»ä¼ å…¥ä¸€ä¸ªæœ‰æ•ˆçš„ `cache_position`ï¼Œè¿™å°†æŒ‡ç¤ºåºåˆ—ä¸­è¾“å…¥çš„ä½ç½®ã€‚è¯·æ³¨æ„ï¼Œ`cache_position` ä¸å—å¡«å……çš„å½±å“ï¼Œå¹¶ä¸”æ€»æ˜¯ä¸ºæ¯ä¸ªæ ‡è®°æ·»åŠ ä¸€ä¸ªä½ç½®ã€‚ä¾‹å¦‚ï¼Œå¦‚æœé”®/å€¼ç¼“å­˜åŒ…å« 10 ä¸ªæ ‡è®°ï¼ˆæ— è®ºå…¶ä¸­æœ‰å¤šå°‘æ˜¯å¡«å……æ ‡è®°ï¼‰ï¼Œä¸‹ä¸€ä¸ªæ ‡è®°çš„ç¼“å­˜ä½ç½®åº”è¯¥æ˜¯ `torch.tensor([10])`ã€‚\n",
    "\n",
    "ä¸‹é¢æ˜¯å¦‚ä½•å®ç°è‡ªå·±çš„ç”Ÿæˆå¾ªç¯çš„ç¤ºä¾‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee30696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "past_key_values = DynamicCache()\n",
    "messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n",
    "inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n",
    "\n",
    "generated_ids = inputs.input_ids\n",
    "cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\n",
    "max_new_tokens = 10\n",
    "\n",
    "for _ in range(max_new_tokens):\n",
    "    outputs = model(**inputs, cache_position=cache_position, past_key_values=past_key_values, use_cache=True)\n",
    "    # Greedily sample one next token\n",
    "    next_token_ids = outputs.logits[:, -1:].argmax(-1)\n",
    "    generated_ids = torch.cat([generated_ids, next_token_ids], dim=-1)\n",
    "\n",
    "    # Prepare inputs for the next generation step by leaving unprocessed tokens, in our case we have only one new token\n",
    "    # and expanding attn mask for the new token, as explained above\n",
    "    attention_mask = inputs[\"attention_mask\"]\n",
    "    attention_mask = torch.cat([attention_mask, attention_mask.new_ones((attention_mask.shape[0], 1))], dim=-1)\n",
    "    inputs = {\"input_ids\": next_token_ids, \"attention_mask\": attention_mask}\n",
    "    cache_position = cache_position[-1:] + 1 # add one more position for the next token\n",
    "\n",
    "print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa3c33e",
   "metadata": {},
   "source": [
    "\n",
    "## ä½¿ç”¨ç¼“å­˜ç”Ÿæˆ\n",
    "\n",
    "åœ¨ ğŸ¤— Transformers ä¸­ï¼Œæˆ‘ä»¬æ”¯æŒå„ç§ç¼“å­˜ç±»å‹æ¥ä¼˜åŒ–ä¸åŒæ¨¡å‹å’Œä»»åŠ¡ä¸­çš„æ€§èƒ½ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰æ¨¡å‹éƒ½ä½¿ç”¨ç¼“å­˜ç”Ÿæˆï¼Œå…¶ä¸­ [~DynamicCache](/docs/transformers/main/en/internal/generation_utils#transformers.DynamicCache) ç±»æ˜¯å¤§å¤šæ•°æ¨¡å‹çš„é»˜è®¤ç¼“å­˜ã€‚å®ƒå…è®¸æˆ‘ä»¬åŠ¨æ€åœ°å¢é•¿ç¼“å­˜å¤§å°ï¼Œé€šè¿‡åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­ä¿å­˜è¶Šæ¥è¶Šå¤šçš„é”®å’Œå€¼ã€‚å¦‚æœä½ å‡ºäºæŸç§åŸå› ä¸æƒ³ä½¿ç”¨ç¼“å­˜ï¼Œä½ å¯ä»¥åœ¨ `generate()` æ–¹æ³•ä¸­ä¼ å…¥ `use_cache=False`ã€‚\n",
    "\n",
    "è¯·å‚è€ƒä¸‹è¡¨ä»¥äº†è§£ç¼“å­˜ç±»å‹ä¹‹é—´çš„åŒºåˆ«ï¼Œå¹¶é€‰æ‹©æœ€é€‚åˆä½ ç”¨ä¾‹çš„ç¼“å­˜ã€‚å»ºè®®åœ¨è°ƒç”¨æ¨¡å‹ä¹‹å‰åˆå§‹åŒ–çš„æ¨¡å‹åº”è¯¥ä½œä¸º kwarg ä¼ é€’ç»™æ¨¡å‹ã€‚åœ¨æ‰€æœ‰å…¶ä»–æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥ç®€å•åœ°å®šä¹‰æ‰€éœ€çš„ `cache_implementation`ï¼Œæˆ‘ä»¬è´Ÿè´£å¤„ç†å…¶ä½™çš„äº‹æƒ…ã€‚\n",
    "\n",
    "| ç¼“å­˜ç±»å‹     | å†…å­˜æ•ˆç‡ | æ”¯æŒ torch.compile() | å»ºè®®åˆå§‹åŒ– | å»¶è¿Ÿ | é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆ |\n",
    "| ------------ | -------- | -------------------- | ---------- | ---- | ------------ |\n",
    "| åŠ¨æ€ç¼“å­˜     | å¦       | å¦                   | å¦         | ä¸­ç­‰ | å¦           |\n",
    "| é™æ€ç¼“å­˜     | å¦       | æ˜¯                   | æ˜¯         | é«˜   | å¦           |\n",
    "| å¸è½½ç¼“å­˜     | æ˜¯       | å¦                   | å¦         | ä½   | æ˜¯           |\n",
    "| å¸è½½é™æ€ç¼“å­˜ | å¦       | æ˜¯                   | æ˜¯         | é«˜   | æ˜¯           |\n",
    "| é‡åŒ–ç¼“å­˜     | æ˜¯       | å¦                   | å¦         | ä½   | æ˜¯           |\n",
    "| æ»‘åŠ¨çª—å£ç¼“å­˜ | å¦       | æ˜¯                   | æ˜¯         | é«˜   | å¦           |\n",
    "| æ±‡ç¼“å­˜       | æ˜¯       | å¦                   | æ˜¯         | ä¸­ç­‰ | æ˜¯           |\n",
    "\n",
    "è¿™äº›ç¼“å­˜ç±»å¯ä»¥åœ¨ç”Ÿæˆæ—¶é€šè¿‡ `cache_implementation` å‚æ•°è®¾ç½®ã€‚è¦äº†è§£ `cache_implementation` æ ‡å¿—çš„å¯ç”¨é€‰é¡¹ï¼Œè¯·å‚é˜… [API æ–‡æ¡£](./main_classes/text_generation#transformers.GenerationConfig)ã€‚ç°åœ¨ï¼Œè®©æˆ‘ä»¬è¯¦ç»†æ¢è®¨æ¯ç§ç¼“å­˜ç±»å‹ï¼Œå¹¶äº†è§£å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚è¯·æ³¨æ„ï¼Œä¸‹é¢çš„ç¤ºä¾‹é€‚ç”¨äºä»…è§£ç å™¨ Transformer æ¨¡å‹ã€‚æˆ‘ä»¬ä¹Ÿæ”¯æŒç‰¹å®šäºæ¨¡å‹çš„ç¼“å­˜ç±»ï¼Œä¾‹å¦‚ Mamba æˆ– Jambaï¼Œç»§ç»­é˜…è¯»ä»¥äº†è§£æ›´å¤šç»†èŠ‚ã€‚\n",
    "\n",
    "### é‡åŒ–ç¼“å­˜\n",
    "\n",
    "é”®å’Œå€¼ç¼“å­˜å¯èƒ½å ç”¨å¤§é‡å†…å­˜ï¼Œæˆä¸º [é•¿ä¸Šä¸‹æ–‡ç”Ÿæˆçš„ç“¶é¢ˆ](https://huggingface.co/blog/llama31#inference-memory-requirements)ï¼Œç‰¹åˆ«æ˜¯å¯¹äºå¤§å‹è¯­è¨€æ¨¡å‹ã€‚åœ¨ä½¿ç”¨ `generate()` æ—¶é‡åŒ–ç¼“å­˜å¯ä»¥æ˜¾è‘—é™ä½å†…å­˜éœ€æ±‚ï¼Œä½†ä»£ä»·æ˜¯é€Ÿåº¦ã€‚\n",
    "\n",
    "`transformers` ä¸­çš„ KV ç¼“å­˜é‡åŒ–å¾ˆå¤§ç¨‹åº¦ä¸Šå—åˆ°è®ºæ–‡ [â€œKIVI: A Tuning-Free Asymmetric 2bit Quantization for KV Cacheâ€](https://arxiv.org/abs/2402.02750) çš„å¯å‘ï¼Œç›®å‰æ”¯æŒ [~QuantoQuantizedCache](/docs/transformers/main/en/internal/generation_utils#transformers.QuantoQuantizedCache) å’Œ [~HQQQuantizedCache](/docs/transformers/main/en/internal/generation_utils#transformers.HQQQuantizedCache) ç±»ã€‚æœ‰å…³å†…éƒ¨å·¥ä½œåŸç†çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…è¯¥è®ºæ–‡ã€‚\n",
    "\n",
    "è¦å¯ç”¨é”®å€¼ç¼“å­˜çš„é‡åŒ–ï¼Œéœ€è¦åœ¨ `generation_config` ä¸­æŒ‡ç¤º `cache_implementation=\"quantized\"`ã€‚é‡åŒ–ç›¸å…³çš„å‚æ•°åº”è¯¥ä½œä¸º `dict` æˆ– [~QuantizedCacheConfig](/docs/transformers/main/en/internal/generation_utils#transformers.QuantizedCacheConfig) ç±»çš„å®ä¾‹ä¼ é€’ç»™ `generation_config`ã€‚ä½ å¿…é¡»æŒ‡ç¤ºåœ¨ [~QuantizedCacheConfig](/docs/transformers/main/en/internal/generation_utils#transformers.QuantizedCacheConfig) ä¸­ä½¿ç”¨å“ªä¸ªé‡åŒ–åç«¯ï¼Œé»˜è®¤æ˜¯ `quanto`ã€‚\n",
    "\n",
    "å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ `quanto` åç«¯ï¼Œå»ºè®®å°† `axis-key/axis-value` å‚æ•°è®¾ç½®ä¸º `0`ï¼›å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ `HQQ` åç«¯ï¼Œåˆ™è®¾ç½®ä¸º `1`ã€‚å¯¹äºå…¶ä»–é…ç½®å€¼ï¼Œè¯·ä½¿ç”¨é»˜è®¤å€¼ï¼Œé™¤éä½ å†…å­˜ä¸è¶³ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯èƒ½éœ€è¦è€ƒè™‘å‡å°‘æ®‹å·®é•¿åº¦ã€‚\n",
    "\n",
    "ç¼“å­˜é‡åŒ–å¯èƒ½åœ¨å»¶è¿Ÿæ–¹é¢æœ‰å®³ï¼Œå¦‚æœä¸Šä¸‹æ–‡é•¿åº¦çŸ­ä¸” GPU VRAM è¶³å¤Ÿè¿è¡Œè€Œä¸éœ€è¦ç¼“å­˜é‡åŒ–ã€‚å»ºè®®åœ¨å†…å­˜æ•ˆç‡å’Œå»¶è¿Ÿä¹‹é—´å¯»æ±‚å¹³è¡¡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af05219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n",
    "inputs = tokenizer(\"I like rock music because\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"quantized\", cache_config={\"nbits\": 4, \"backend\": \"quanto\"})\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n",
    "\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e48a046",
   "metadata": {},
   "source": [
    "\n",
    "### å¸è½½ç¼“å­˜\n",
    "\n",
    "ä¸ KV ç¼“å­˜é‡åŒ–ç±»ä¼¼ï¼Œ[~OffloadedCache](/docs/transformers/main/en/internal/generation_utils#transformers.OffloadedCache) ç­–ç•¥æ—¨åœ¨å‡å°‘ GPU VRAM ä½¿ç”¨ã€‚å®ƒé€šè¿‡å°†å¤§å¤šæ•°å±‚çš„ KV ç¼“å­˜ç§»åŠ¨åˆ° CPU æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å½“æ¨¡å‹çš„ `forward()` æ–¹æ³•è¿­ä»£å±‚æ—¶ï¼Œæ­¤ç­–ç•¥åœ¨ GPU ä¸Šç»´æŠ¤å½“å‰å±‚ç¼“å­˜ã€‚åŒæ—¶ï¼Œå®ƒå¼‚æ­¥åœ°é¢„å–ä¸‹ä¸€å±‚ç¼“å­˜ï¼Œå¹¶å°†å‰ä¸€å±‚ç¼“å­˜å‘é€å› CPUã€‚ä¸ KV ç¼“å­˜é‡åŒ–ä¸åŒï¼Œæ­¤ç­–ç•¥æ€»æ˜¯äº§ç”Ÿä¸é»˜è®¤ KV ç¼“å­˜å®ç°ç›¸åŒçš„ç»“æœã€‚å› æ­¤ï¼Œå®ƒå¯ä»¥ä½œä¸ºæ›¿æ¢æˆ–å¤‡ç”¨æ–¹æ¡ˆã€‚\n",
    "\n",
    "æ ¹æ®ä½ çš„æ¨¡å‹å’Œç”Ÿæˆä»»åŠ¡çš„ç‰¹å¾ï¼ˆä¸Šä¸‹æ–‡å¤§å°ã€ç”Ÿæˆçš„æ ‡è®°æ•°ã€å…‰æŸæ•°ç­‰ï¼‰ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°ä¸é»˜è®¤ KV ç¼“å­˜å®ç°ç›¸æ¯”ï¼Œç”Ÿæˆååé‡ç•¥æœ‰ä¸‹é™ã€‚\n",
    "\n",
    "è¦å¯ç”¨ KV ç¼“å­˜å¸è½½ï¼Œè¯·åœ¨ `generation_config` ä¸­ä¼ é€’ `cache_implementation=\"offloaded\"`ï¼Œæˆ–è€…ç›´æ¥ä¼ é€’ç»™ `generate()` è°ƒç”¨ã€‚ä½¿ç”¨ `cache_implementation=\"offloaded_static\"` ç”¨äºå¸è½½é™æ€ç¼“å­˜ï¼ˆä¹Ÿè¯·å‚é˜…ä¸‹é¢çš„ [å¸è½½é™æ€ç¼“å­˜](#offloaded-static-cache)ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d04274",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n",
    "inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])\n",
    "\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=23)\n",
    "print(tokenizer.batch_decode(out, skip_special_tokens=True)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4ab96e",
   "metadata": {},
   "source": [
    "\n",
    "ç¼“å­˜å¸è½½éœ€è¦ä¸€ä¸ª GPUï¼Œå¹¶ä¸”å¯èƒ½æ¯”åŠ¨æ€ KV ç¼“å­˜æ…¢ã€‚å¦‚æœä½ é‡åˆ° CUDA å†…å­˜ä¸è¶³é”™è¯¯ï¼Œè¯·ä½¿ç”¨å®ƒã€‚\n",
    "\n",
    "ä¸‹é¢çš„ç¤ºä¾‹æ˜¾ç¤ºäº†å¦‚ä½•å°† KV ç¼“å­˜å¸è½½ç”¨ä½œå¤‡ç”¨ç­–ç•¥ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932338af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "def resilient_generate(model, *args, **kwargs):\n",
    "    oom = False\n",
    "    try:\n",
    "        return model.generate(*args, **kwargs)\n",
    "    except torch.cuda.OutOfMemoryError as e:\n",
    "        print(e)\n",
    "        print(\"retrying with cache_implementation='offloaded'\")\n",
    "        oom = True\n",
    "    if oom:\n",
    "        torch.cuda.empty_cache()\n",
    "        kwargs[\"cache_implementation\"] = \"offloaded\"\n",
    "        return model.generate(*args, **kwargs)\n",
    "\n",
    "ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(ckpt)\n",
    "model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n",
    "prompt = [\"okay \"*1000 + \"Fun fact: The most\"]\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "beams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }\n",
    "out = resilient_generate(model, **inputs, **beams)\n",
    "responses = tokenizer.batch_decode(out[:,:-28:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cf8c0f",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨æ‹¥æœ‰ 50 GB RAM çš„ GPU ä¸Šè¿è¡Œæ­¤ä»£ç å°†æ‰“å°:\n",
    "\n",
    "\"\"\"\n",
    "CUDA out of memory. Tried to allocate 4.83 GiB. GPU\n",
    "retrying with cache_implementation='offloaded'\n",
    "\"\"\"\n",
    "\n",
    "ç„¶åæˆåŠŸç”Ÿæˆ 40 ä¸ªå…‰æŸã€‚\n",
    "\n",
    "### é™æ€ç¼“å­˜\n",
    "\n",
    "ç”±äºâ€œDynamicCacheâ€ä¼šéšç€æ¯ä¸ªç”Ÿæˆæ­¥éª¤åŠ¨æ€å¢é•¿ï¼Œå®ƒé˜»æ­¢ä½ åˆ©ç”¨ JIT ä¼˜åŒ–ã€‚[~StaticCache](/docs/transformers/main/en/internal/generation_utils#transformers.StaticCache) ä¸ºé”®å’Œå€¼é¢„å…ˆåˆ†é…ä¸€ä¸ªç‰¹å®šçš„æœ€å¤§å¤§å°ï¼Œå…è®¸ä½ ç”Ÿæˆåˆ°æœ€å¤§é•¿åº¦ï¼Œè€Œæ— éœ€ä¿®æ”¹ç¼“å­˜å¤§å°ã€‚æ£€æŸ¥ä¸‹é¢çš„ä½¿ç”¨ç¤ºä¾‹ã€‚\n",
    "\n",
    "æœ‰å…³å¸¦æœ‰é™æ€ç¼“å­˜å’Œ JIT ç¼–è¯‘çš„æ›´å¤šç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ [StaticCache & torchcompile](./llm_optims#static-kv-cache-and-torchcompile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ded6cd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# simply pass the cache implementation=\"static\"\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"static\")\n",
    "tokenizer.batch_decode(out, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f798ba",
   "metadata": {},
   "source": [
    "\n",
    "## å¸è½½é™æ€ç¼“å­˜\n",
    "\n",
    "ä¸ [~OffloadedCache](/docs/transformers/main/en/internal/generation_utils#transformers.OffloadedCache) å­˜åœ¨ç”¨äºå¸è½½â€œDynamicCacheâ€ä¸€æ ·ï¼Œä¹Ÿå­˜åœ¨å¸è½½é™æ€ç¼“å­˜ã€‚å®ƒå®Œå…¨æ”¯æŒ JIT ä¼˜åŒ–ã€‚åªéœ€åœ¨ `generation_config` æˆ–ç›´æ¥ä¼ é€’ç»™ `generate()` è°ƒç”¨æ—¶ä¼ é€’ `cache_implementation=\"offloaded_static\"`ã€‚è¿™å°†ä½¿ç”¨ [~OffloadedStaticCache](/docs/transformers/main/en/internal/generation_utils#transformers.OffloadedStaticCache) å®ç°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d536a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# simply pass the cache implementation=\"offloaded_static\"\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=20, cache_implementation=\"offloaded_static\")\n",
    "tokenizer.batch_decode(out, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d049fde9",
   "metadata": {},
   "source": [
    "\n",
    "### æ»‘åŠ¨çª—å£ç¼“å­˜\n",
    "\n",
    "é¡¾åæ€ä¹‰ï¼Œè¿™ç§ç¼“å­˜ç±»å‹åœ¨å…ˆå‰çš„é”®å’Œå€¼ä¸Šå®ç°äº†ä¸€ä¸ªæ»‘åŠ¨çª—å£ï¼Œåªä¿ç•™æœ€å `sliding_window` ä¸ªæ ‡è®°ã€‚å®ƒåº”è¯¥ä¸æ”¯æŒæ»‘åŠ¨çª—å£æ³¨æ„åŠ›çš„æ¨¡å‹ï¼ˆå¦‚ Mistralï¼‰ä¸€èµ·ä½¿ç”¨ã€‚æ­¤å¤–ï¼Œä¸é™æ€ç¼“å­˜ç±»ä¼¼ï¼Œè¿™ä¸ªç¼“å­˜ä¹Ÿæ˜¯ JIT å‹å¥½çš„ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨ä¸é™æ€ç¼“å­˜ç›¸åŒçš„ç¼–è¯‘æŠ€æœ¯ã€‚\n",
    "\n",
    "æ³¨æ„ï¼Œä½ åªèƒ½å¯¹æ”¯æŒæ»‘åŠ¨çª—å£çš„æ¨¡å‹ä½¿ç”¨æ­¤ç¼“å­˜ï¼Œä¾‹å¦‚ Mistral æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72592861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.float16).to(\"cuda:0\")\n",
    "inputs = tokenizer(\"Yesterday I was on a rock concert and.\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# can be used by passing in cache implementation\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=30, cache_implementation=\"sliding_window\")\n",
    "tokenizer.batch_decode(out, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a001d",
   "metadata": {},
   "source": [
    "\n",
    "### æ±‡ç¼“å­˜\n",
    "\n",
    "æ±‡ç¼“å­˜æ˜¯åœ¨ [â€œEfficient Streaming Language Models with Attention Sinksâ€](https://arxiv.org/abs/2309.17453) ä¸­å¼•å…¥çš„ã€‚å®ƒå…è®¸ä½ ç”Ÿæˆé•¿æ–‡æœ¬åºåˆ—ï¼ˆæ ¹æ®è®ºæ–‡æ˜¯â€œæ— é™é•¿åº¦â€ï¼‰ï¼Œè€Œæ— éœ€ä»»ä½•å¾®è°ƒã€‚è¿™æ˜¯é€šè¿‡æ™ºèƒ½å¤„ç†å…ˆå‰çš„é”®å’Œå€¼æ¥å®ç°çš„ï¼Œå…·ä½“æ¥è¯´ï¼Œå®ƒä¿ç•™åºåˆ—ä¸­çš„ä¸€äº›åˆå§‹æ ‡è®°ï¼Œç§°ä¸ºâ€œæ±‡æ ‡è®°â€ã€‚è¿™æ˜¯åŸºäºè§‚å¯Ÿï¼Œè¿™äº›åˆå§‹æ ‡è®°åœ¨ç”Ÿæˆè¿‡ç¨‹ä¸­å¸å¼•äº†å¤§éƒ¨åˆ†æ³¨æ„åŠ›åˆ†æ•°ã€‚åœ¨â€œæ±‡æ ‡è®°â€ä¹‹åå‡ºç°çš„æ ‡è®°ä»¥æ»‘åŠ¨çª—å£çš„æ–¹å¼è¢«ä¸¢å¼ƒï¼Œåªä¿ç•™æœ€æ–°çš„ `window_size` ä¸ªæ ‡è®°ã€‚é€šè¿‡å°†è¿™äº›åˆå§‹æ ‡è®°ä½œä¸ºâ€œæ³¨æ„åŠ›æ±‡â€ï¼Œæ¨¡å‹å³ä½¿åœ¨å¤„ç†éå¸¸é•¿çš„æ–‡æœ¬æ—¶ä¹Ÿèƒ½ä¿æŒç¨³å®šçš„æ€§èƒ½ï¼Œä»è€Œä¸¢å¼ƒäº†å¤§éƒ¨åˆ†å…ˆå‰çš„çŸ¥è¯†ã€‚\n",
    "\n",
    "ä¸å…¶ä»–ç¼“å­˜ç±»ä¸åŒï¼Œè¿™ä¸ªç±»ä¸èƒ½é€šè¿‡æŒ‡ç¤º `cache_implementation` ç›´æ¥ä½¿ç”¨ã€‚ä½ å¿…é¡»åœ¨è°ƒç”¨ `generate()` ä¹‹å‰åˆå§‹åŒ– Cacheï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bf6e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, SinkCache\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16).to(\"cuda:0\")\n",
    "inputs = tokenizer(\"This is a long story about unicorns, fairies and magic.\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# get our cache, specify number of sink tokens and window size\n",
    "# Note that window size already includes sink tokens, so has to be larger\n",
    "past_key_values = SinkCache(window_length=256, num_sink_tokens=4)\n",
    "out = model.generate(**inputs, do_sample=False, max_new_tokens=30, past_key_values=past_key_values)\n",
    "tokenizer.batch_decode(out, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c76289",
   "metadata": {},
   "source": [
    "\n",
    "### ç¼–ç å™¨-è§£ç å™¨ç¼“å­˜\n",
    "\n",
    "[~EncoderDecoderCache](/docs/transformers/main/en/internal/generation_utils#transformers.EncoderDecoderCache) æ˜¯ä¸€ä¸ªåŒ…è£…å™¨ï¼Œæ—¨åœ¨å¤„ç†ç¼–ç å™¨-è§£ç å™¨æ¨¡å‹çš„ç¼“å­˜éœ€æ±‚ã€‚è¿™ç§ç¼“å­˜ç±»å‹ä¸“é—¨æ„å»ºæ¥ç®¡ç†è‡ªæ³¨æ„åŠ›å’Œäº¤å‰æ³¨æ„åŠ›ç¼“å­˜ï¼Œç¡®ä¿å­˜å‚¨å’Œæ£€ç´¢è¿™äº›å¤æ‚æ¨¡å‹æ‰€éœ€çš„è¿‡å»é”®/å€¼ã€‚ç¼–ç å™¨-è§£ç å™¨ç¼“å­˜çš„ä¸€ä¸ªå¾ˆé…·çš„ç‰¹æ€§æ˜¯ï¼Œä½ å¯ä»¥ä¸ºç¼–ç å™¨å’Œè§£ç å™¨è®¾ç½®ä¸åŒçš„ç¼“å­˜ç±»å‹ï¼Œå…·ä½“å–å†³äºä½ çš„ç”¨ä¾‹ã€‚ç›®å‰è¿™ç§ç¼“å­˜ä»…åœ¨ [Whisper](./model_doc/whisper) æ¨¡å‹ä¸­æ”¯æŒï¼Œä½†æˆ‘ä»¬å¾ˆå¿«ä¼šæ·»åŠ æ›´å¤šæ¨¡å‹ã€‚\n",
    "\n",
    "åœ¨ç”¨æ³•æ–¹é¢ï¼Œæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«çš„æ“ä½œï¼Œè°ƒç”¨ `generate()` æˆ– `forward()` å°†å¤„ç†æ‰€æœ‰äº‹æƒ…ã€‚\n",
    "\n",
    "### æ¨¡å‹ç‰¹å®šç¼“å­˜ç±»\n",
    "\n",
    "æŸäº›æ¨¡å‹éœ€è¦ä»¥ç‰¹å®šæ–¹å¼å­˜å‚¨å…ˆå‰çš„é”®ã€å€¼æˆ–çŠ¶æ€ï¼Œä¸Šè¿°ç¼“å­˜ç±»æ— æ³•ä½¿ç”¨ã€‚å¯¹äºæ­¤ç±»æƒ…å†µï¼Œæˆ‘ä»¬æœ‰å‡ ä¸ªä¸“é—¨ä¸ºç‰¹å®šæ¨¡å‹è®¾è®¡çš„ç¼“å­˜ç±»ã€‚è¿™äº›æ¨¡å‹åªæ¥å—å®ƒä»¬è‡ªå·±çš„ä¸“ç”¨ç¼“å­˜ç±»ï¼Œå¹¶ä¸”ä¸æ”¯æŒä½¿ç”¨ä»»ä½•å…¶ä»–ç¼“å­˜ç±»å‹ã€‚ä¸€äº›ä¾‹å­åŒ…æ‹¬ [~HybridCache](/docs/transformers/main/en/internal/generation_utils#transformers.HybridCache) ç”¨äº [Gemma2](./model_doc/gemma2) ç³»åˆ—æ¨¡å‹æˆ– [~MambaCache](/docs/transformers/main/en/internal/generation_utils#transformers.MambaCache) ç”¨äº [Mamba](./model_doc/mamba) æ¶æ„æ¨¡å‹ã€‚\n",
    "\n",
    "## ä½¿ç”¨ç¼“å­˜è¿›è¡Œè¿­ä»£ç”Ÿæˆ\n",
    "\n",
    "æˆ‘ä»¬å·²ç»çœ‹åˆ°äº†å¦‚ä½•åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨æ¯ç§ç¼“å­˜ç±»å‹ã€‚å¦‚æœä½ æƒ³åœ¨è¿­ä»£ç”Ÿæˆè®¾ç½®ä¸­ä½¿ç”¨ç¼“å­˜ï¼Œä¾‹å¦‚åœ¨èŠå¤©æœºå™¨äººç­‰åº”ç”¨ç¨‹åºä¸­ï¼Œå…¶ä¸­äº¤äº’æ¶‰åŠå¤šä¸ªå›åˆå’ŒæŒç»­çš„æ¥å›äº¤æµï¼Œè¯¥æ€ä¹ˆåŠï¼Ÿä½¿ç”¨ç¼“å­˜è¿›è¡Œè¿­ä»£ç”Ÿæˆå…è®¸è¿™äº›ç³»ç»Ÿæœ‰æ•ˆåœ°å¤„ç†æŒç»­çš„å¯¹è¯ï¼Œè€Œæ— éœ€åœ¨æ¯ä¸€æ­¥é‡æ–°å¤„ç†æ•´ä¸ªä¸Šä¸‹æ–‡ã€‚ä½†åœ¨ä½ å¼€å§‹å®ç°ä¹‹å‰ï¼Œæœ‰ä¸€äº›æç¤ºä½ åº”è¯¥çŸ¥é“ï¼š\n",
    "\n",
    "è¿›è¡Œè¿­ä»£ç”Ÿæˆçš„ä¸€èˆ¬æ ¼å¼å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œä½ å¿…é¡»åˆå§‹åŒ–ä¸€ä¸ªä½ æƒ³è¦çš„ç±»å‹çš„ç©ºç¼“å­˜ï¼Œç„¶åä½ å¯ä»¥å¼€å§‹è¿­ä»£åœ°è¾“å…¥æ–°çš„æç¤ºã€‚å¯¹è¯å†å²è®°å½•å’Œæ ¼å¼çš„è·Ÿè¸ªå¯ä»¥é€šè¿‡èŠå¤©æ¨¡æ¿æ¥å®Œæˆï¼Œæ›´å¤šå†…å®¹è¯·é˜…è¯» [chat_templating](./chat_templating)ã€‚\n",
    "\n",
    "å¦‚æœä½ ä½¿ç”¨çš„æ˜¯æ±‡ç¼“å­˜ï¼Œä½ å¿…é¡»å°†è¾“å…¥è£å‰ªåˆ°æœ€å¤§é•¿åº¦ï¼Œå› ä¸ºæ±‡ç¼“å­˜å¯ä»¥ç”Ÿæˆæ¯”å…¶æœ€å¤§çª—å£é•¿åº¦æ›´é•¿çš„æ–‡æœ¬ï¼Œä½†å®ƒæœŸæœ›ç¬¬ä¸€ä¸ªè¾“å…¥ä¸è¶…è¿‡æœ€å¤§ç¼“å­˜é•¿åº¦ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9ce15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers.cache_utils import (\n",
    "    DynamicCache,\n",
    "    SinkCache,\n",
    "    StaticCache,\n",
    "    SlidingWindowCache,\n",
    "    QuantoQuantizedCache,\n",
    "    QuantizedCacheConfig,\n",
    ")\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "user_prompts = [\"Hello, what's your name?\", \"Btw, yesterday I was on a rock concert.\"]\n",
    "\n",
    "past_key_values = DynamicCache()\n",
    "max_cache_length = past_key_values.get_max_length()\n",
    "\n",
    "messages = []\n",
    "for prompt in user_prompts:\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n",
    "    if isinstance(past_key_values, SinkCache):\n",
    "        inputs = {k: v[:, -max_cache_length:] for k, v in inputs.items()}\n",
    "\n",
    "    input_length = inputs[\"input_ids\"].shape[1]\n",
    "\n",
    "    outputs = model.generate(**inputs, do_sample=False, max_new_tokens=256, past_key_values=past_key_values)\n",
    "    completion = tokenizer.decode(outputs[0, input_length:], skip_special_tokens=True)\n",
    "    messages.append({\"role\": \"assistant\", \"content\": completion})\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db710a12",
   "metadata": {},
   "source": [
    "\n",
    "## é‡æ–°ä½¿ç”¨ç¼“å­˜ä»¥ç»§ç»­ç”Ÿæˆ\n",
    "\n",
    "æœ‰æ—¶ä½ å¯èƒ½æƒ³è¦é¦–å…ˆç”¨é”®/å€¼å¡«å……ç¼“å­˜å¯¹è±¡ä»¥ç”¨äºæŸäº›å‰ç¼€æç¤ºï¼Œå¹¶å¤šæ¬¡é‡æ–°ä½¿ç”¨å®ƒä»¥ç”Ÿæˆä¸åŒçš„åºåˆ—ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥æ„å»ºä¸€ä¸ª `Cache` å¯¹è±¡æ¥ä¿å­˜æŒ‡ä»¤æç¤ºï¼Œå¹¶å¤šæ¬¡ä½¿ç”¨ä¸åŒçš„æ–‡æœ¬åºåˆ—é‡æ–°ä½¿ç”¨å®ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb053a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, DynamicCache, StaticCache\n",
    "\n",
    "model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# åˆå§‹åŒ–å…·æœ‰è¶³å¤Ÿå¤§æœ€å¤§é•¿åº¦çš„ StaticCacheï¼ˆä¸‹é¢çš„ç¤ºä¾‹ä¸­ä¸º 1024 ä¸ªæ ‡è®°ï¼‰\n",
    "# å¦‚æœæ›´é€‚åˆä½ çš„éœ€æ±‚ï¼Œä½ ä¹Ÿå¯ä»¥åˆå§‹åŒ–ä¸€ä¸ª DynamicCache\n",
    "prompt_cache = StaticCache(config=model.config, max_batch_size=1, max_cache_len=1024, device=\"cuda\", dtype=torch.bfloat16)\n",
    "\n",
    "INITIAL_PROMPT = \"You are a helpful assistant. \"\n",
    "inputs_initial_prompt = tokenizer(INITIAL_PROMPT, return_tensors=\"pt\").to(\"cuda\")\n",
    "# è¿™æ˜¯å…¬å…±æç¤ºç¼“å­˜ï¼Œæˆ‘ä»¬éœ€è¦åœ¨æ²¡æœ‰æ¢¯åº¦çš„æƒ…å†µä¸‹è¿è¡Œ forward æ¥å¤åˆ¶\n",
    "with torch.no_grad():\n",
    "    prompt_cache = model(**inputs_initial_prompt, past_key_values = prompt_cache).past_key_values\n",
    "\n",
    "prompts = [\"Help me to write a blogpost about travelling.\", \"What is the capital of France?\"]\n",
    "responses = []\n",
    "for prompt in prompts:\n",
    "    new_inputs = tokenizer(INITIAL_PROMPT + prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    past_key_values = copy.deepcopy(prompt_cache)\n",
    "    outputs = model.generate(**new_inputs, past_key_values=past_key_values, max_new_tokens=20)\n",
    "    response = tokenizer.batch_decode(outputs)[0]\n",
    "    responses.append(response)\n",
    "\n",
    "print(responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109bf4df",
   "metadata": {},
   "source": [
    "\n",
    "## é—ç•™ç¼“å­˜æ ¼å¼\n",
    "\n",
    "åœ¨å¼•å…¥ `Cache` å¯¹è±¡ä¹‹å‰ï¼ŒLLM çš„ç¼“å­˜æ›¾ç»æ˜¯ä¸€ä¸ªå…ƒç»„çš„å…ƒç»„ï¼Œå…¶ä¸­åŒ…å«å¼ é‡ã€‚é—ç•™æ ¼å¼å…·æœ‰åŠ¨æ€å¤§å°ï¼Œéšç€æˆ‘ä»¬ç”Ÿæˆæ–‡æœ¬è€Œå¢é•¿â€”â€”ä¸ `DynamicCache` éå¸¸ç›¸ä¼¼ã€‚å¦‚æœä½ çš„é¡¹ç›®ä¾èµ–äºè¿™ç§é—ç•™æ ¼å¼ï¼Œä½ å¯ä»¥æ— ç¼åœ°å°†å…¶è½¬æ¢ä¸º `DynamicCache`ï¼Œåä¹‹äº¦ç„¶ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f728ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n",
    "inputs = tokenizer(\"Hello, my name is\", return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# `return_dict_in_generate=True` æ˜¯è¿”å›ç¼“å­˜æ‰€å¿…éœ€çš„ã€‚`return_legacy_cache` å¼ºåˆ¶è¿”å›çš„ç¼“å­˜\n",
    "# ä¸ºé—ç•™ç±»å‹\n",
    "generation_outputs = model.generate(**inputs, return_dict_in_generate=True, return_legacy_cache=True, max_new_tokens=5)\n",
    "\n",
    "# æˆ‘ä»¬å¯ä»¥å°†é—ç•™ç¼“å­˜è½¬æ¢ä¸º DynamicCache â€”â€” åä¹‹äº¦ç„¶ã€‚å¦‚æœä½ æœ‰ç‰¹å®šçš„æ ¼å¼æ¥æ“ä½œç¼“å­˜çš„è‡ªå®šä¹‰é€»è¾‘ï¼Œè¿™å¾ˆæœ‰å¸®åŠ©ã€‚\n",
    "cache = DynamicCache.from_legacy_cache(generation_outputs.past_key_values)\n",
    "legacy_format_cache = cache.to_legacy_cache()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
