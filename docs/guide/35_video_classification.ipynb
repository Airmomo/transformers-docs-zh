{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0a3f3b",
   "metadata": {},
   "source": [
    "# 视频分类\n",
    "\n",
    "视频分类是指为整个视频分配一个标签或类别。每个视频通常只属于一个类别。视频分类模型接收视频作为输入，并返回该视频所属类别的预测结果。这些模型可以用于对视频内容进行分类。视频分类的一个实际应用是动作/活动识别，这在健身应用程序中非常有用。对于视障人士，尤其是在出行时，视频分类也是很有帮助的。\n",
    "\n",
    "本指南将向你展示如何：\n",
    "\n",
    "1. 在 [UCF101 数据集](https://www.crcv.ucf.edu/data/UCF101.php)的子集上微调 [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae)。\n",
    "2. 使用微调后的模型进行推理。\n",
    "\n",
    "要查看与本任务兼容的所有架构和检查点，我们推荐查看 [任务页面](https://huggingface.co/tasks/video-classification)。\n",
    "\n",
    "在开始之前，请确保你已经安装了所有必要的库：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2e424",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q pytorchvideo transformers evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d036ba",
   "metadata": {},
   "source": [
    "\n",
    "你将使用 [PyTorchVideo](https://pytorchvideo.org/)（简称 `pytorchvideo`）来处理和准备视频。\n",
    "\n",
    "我们建议你登录到你的 Hugging Face 账户，以便可以上传和分享你的模型。当提示输入令牌时，请输入你的令牌以登录：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87893d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918abcd7",
   "metadata": {},
   "source": [
    "\n",
    "## 加载 UCF101 数据集\n",
    "\n",
    "首先加载 [UCF-101 数据集](https://www.crcv.ucf.edu/data/UCF101.php)的子集。这将让你有机会进行实验并确保一切正常运行，然后再花费更多时间在完整数据集上训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35da8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab40af4",
   "metadata": {},
   "source": [
    "\n",
    "下载子集后，需要解压压缩包：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a30922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(file_path) as t:\n",
    "    t.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e71346",
   "metadata": {},
   "source": [
    "\n",
    "数据集的组织结构如下：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3425d7d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "UCF101_subset/\n",
    "    train/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    val/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    test/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26decc",
   "metadata": {},
   "source": [
    "\n",
    "然后你可以统计总视频数量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = \"UCF101_subset\"\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/ *.avi\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/ *.avi\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/ *.avi\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"总视频数: {video_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a512419",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86172b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/ *.avi\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/ *.avi\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/ *.avi\"))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657b555",
   "metadata": {},
   "source": [
    "\n",
    "（排序后的）视频路径如下所示：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "447be6b8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "...\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eeb2e",
   "metadata": {},
   "source": [
    "\n",
    "你会注意到，有些视频片段属于同一个组/场景，组由视频文件路径中的 `g` 表示。例如，`v_ApplyEyeMakeup_g07_c04.avi` 和 `v_ApplyEyeMakeup_g07_c06.avi` 属于同一组。\n",
    "\n",
    "对于验证和评估拆分，你不希望有来自同一组/场景的视频片段，以防止数据泄露。本教程中使用的子集考虑了这一点。\n",
    "\n",
    "接下来，你将从数据集中提取标签集。同时，创建两个字典，有助于初始化模型：\n",
    "\n",
    "- `label2id`：将类别名称映射到整数。\n",
    "- `id2label`：将整数映射到类别名称。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"唯一类别: {list(label2id.keys())}.\")\n",
    "# 唯一类别: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805e6c0",
   "metadata": {},
   "source": [
    "\n",
    "共有 10 个唯一类别。每个类别在训练集中有 30 个视频。\n",
    "\n",
    "## 加载模型进行微调\n",
    "\n",
    "从预训练检查点实例化视频分类模型及其关联的图像处理器。模型的编码器带有预训练参数，而分类头则是随机初始化的。图像处理器将在编写数据集的预处理管道时派上用场。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # 如果你计划微调一个已经微调过的检查点，提供此参数\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0a09f",
   "metadata": {},
   "source": [
    "\n",
    "在加载模型时，你可能会注意到以下警告：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39968956",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "某些预训练检查点中的权重在初始化 VideoMAEForVideoClassification 时未被使用：\n",
    "- 这是预期的行为，如果你是从另一个任务或架构的预训练模型初始化 VideoMAEForVideoClassification（例如，从 BertForPreTraining 初始化 BertForSequenceClassification 模型）。\n",
    "- 如果你期望初始化的 VideoMAEForVideoClassification 与预训练模型完全相同（例如，从 BertForSequenceClassification 初始化 BertForSequenceClassification 模型），则这不是预期的行为。\n",
    "VideoMAEForVideoClassification 的一些权重未从预训练检查点中初始化，而是新初始化的：\n",
    "- classifier.bias, classifier.weight\n",
    "你应该在此模型上训练下游任务，以便能够使用它进行预测和推理。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcf4ac",
   "metadata": {},
   "source": [
    "\n",
    "警告告诉我们，我们在丢弃一些权重（例如 `classifier` 层的权重和偏置），并随机初始化一些其他权重（新的 `classifier` 层的权重和偏置）。这是预期的行为，因为我们在添加一个新的头部，而没有预训练的权重，因此库提醒我们需要在使用模型进行推理之前对其进行微调，而这正是我们将要做的。\n",
    "\n",
    "**注意**：[此检查点](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)在类似下游任务上进行了微调，具有相当大的领域重叠，因此在本任务上的表现更好。你可以查看 [此检查点](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)，它是通过对 `MCG-NJU/videomae-base-finetuned-kinetics` 进行微调获得的。\n",
    "\n",
    "## 准备数据集进行训练\n",
    "\n",
    "为了预处理视频，你将利用 [PyTorchVideo 库](https://pytorchvideo.org/)。首先导入所需的依赖项。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767ec93",
   "metadata": {},
   "source": [
    "\n",
    "对于训练数据集的变换，使用均匀时间采样、像素归一化、随机裁剪和随机水平翻转的组合。对于验证和评估数据集的变换，使用相同的变换链，但不包括随机裁剪和水平翻转。有关这些变换的详细信息，请参阅 [PyTorchVideo 官方文档](https://pytorchvideo.org)。\n",
    "\n",
    "使用与预训练模型关联的 `image_processor` 获取以下信息：\n",
    "\n",
    "- 用于归一化视频帧像素的图像均值和标准差。\n",
    "- 视频帧将调整大小的空间分辨率。\n",
    "\n",
    "首先定义一些常量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ef281",
   "metadata": {},
   "source": [
    "\n",
    "现在，分别定义数据集特定的变换和数据集。从训练集开始：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa292c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            RandomShortSideScale(min_size=256, max_size=320),\n",
    "            RandomCrop(resize_to),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7ab67",
   "metadata": {},
   "source": [
    "\n",
    "同样的工作流程可以应用于验证和评估集：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d68d3",
   "metadata": {},
   "source": [
    "\n",
    "**注意**：上述数据集管道取自 [PyTorchVideo 官方示例](https://pytorchvideo.org/docs/tutorial_classification#dataset)。我们使用的是 `[pytorchvideo.data.Ucf101()](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101)` 函数，因为它专为 UCF-101 数据集定制。在底层，它返回一个 `[pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset)` 对象。`LabeledVideoDataset` 类是 PyTorchVideo 数据集中所有视频的基础类。因此，如果你想使用不支持的自定义数据集，可以扩展 `LabeledVideoDataset` 类。更多详情请参考 `data` API [文档](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)。如果你的数据集结构与此类似（如上所示），那么使用 `pytorchvideo.data.Ucf101()` 应该没问题。\n",
    "\n",
    "你可以通过访问 `num_videos` 参数来了解数据集中的视频数量。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\n",
    "# (300, 30, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17289a66",
   "metadata": {},
   "source": [
    "\n",
    "## 可视化预处理后的视频以便更好地调试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"反归一化图像像素。\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"从视频张量中生成 GIF。\n",
    "    \n",
    "    期望视频张量的形状为：\n",
    "    (num_frames, num_channels, height, width)。\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"从视频张量中生成并显示 GIF。\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dcde3",
   "metadata": {},
   "source": [
    "\n",
    "![打篮球的人](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif)\n",
    "\n",
    "## 训练模型\n",
    "\n",
    "使用 🤗 Transformers 的 [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) 来训练模型。要实例化 `Trainer`，你需要定义训练配置和评估指标。最重要的是 `TrainingArguments`，这是一个包含所有属性以配置训练的类。它需要一个输出文件夹名称，该名称将用于保存模型的检查点。它还有助于在 🤗 Hub 上同步模型仓库中的所有信息。\n",
    "\n",
    "大多数训练参数都是自解释的，但其中一个相当重要的是 `remove_unused_columns=False`。这个参数会删除模型调用函数未使用的所有特征。默认情况下，它为 `True`，因为在大多数情况下，删除未使用的特征列是理想的，使其更容易将输入解包到模型的调用函数中。但在这种情况下，你需要未使用的特征（特别是 `video`）来创建 `pixel_values`（这是模型输入中必需的键）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dd85c",
   "metadata": {},
   "source": [
    "\n",
    "`pytorchvideo.data.Ucf101()` 返回的数据集没有实现 `__len__` 方法。因此，我们在实例化 `TrainingArguments` 时必须定义 `max_steps`。\n",
    "\n",
    "接下来，你需要定义一个函数来计算预测的指标，这将使用你现在加载的 `metric`。唯一的预处理是你需要对预测的 logits 进行 argmax：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35526748",
   "metadata": {},
   "source": [
    "\n",
    "**关于评估的说明**：\n",
    "\n",
    "在 [VideoMAE 论文](https://arxiv.org/abs/2203.12602)中，作者使用了以下评估策略。他们在测试视频的多个片段上评估模型，并对这些片段应用不同的裁剪，报告聚合分数。然而，为了简单和简洁，本教程中不考虑这一点。\n",
    "\n",
    "你还需定义一个 `collate_fn`，用于将样本批处理在一起。每个批次包含两个键，即 `pixel_values` 和 `labels`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # 调整为 (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46362349",
   "metadata": {},
   "source": [
    "\n",
    "然后，你只需将所有这些内容以及数据集传递给 `Trainer`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bd496",
   "metadata": {},
   "source": [
    "\n",
    "你可能会想知道为什么在预处理数据时已经传入了 `image_processor`。这只是为了确保图像处理器的配置文件（存储为 JSON）也会上传到 Hub 仓库中。\n",
    "\n",
    "现在通过调用 `train` 方法微调模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcad2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8874d09",
   "metadata": {},
   "source": [
    "\n",
    "训练完成后，使用 `push_to_hub()` 方法将模型分享到 Hub，这样每个人都可以使用你的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8aa108",
   "metadata": {},
   "source": [
    "\n",
    "## 推理\n",
    "\n",
    "太好了，现在你已经微调了一个模型，可以使用它进行推理！\n",
    "\n",
    "加载一个视频用于推理：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e807677",
   "metadata": {},
   "source": [
    "\n",
    "![队伍进行篮球比赛](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif)\n",
    "\n",
    "最简单的方法是使用模型进行推理，就是将其放在一个 `pipeline` 中。实例化一个用于视频分类的 `pipeline` 并传递你的视频：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n",
    "video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87679e3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e39c58f5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[\n",
    "    {'score': 0.9272987842559814, 'label': 'BasketballDunk'},\n",
    "    {'score': 0.017777055501937866, 'label': 'BabyCrawling'},\n",
    "    {'score': 0.01663011871278286, 'label': 'BalanceBeam'},\n",
    "    {'score': 0.009560945443809032, 'label': 'BandMarching'},\n",
    "    {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464889c3",
   "metadata": {},
   "source": [
    "\n",
    "你也可以手动复制 `pipeline` 的结果，如果你愿意的话：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0244689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [sample_test_video[\"label\"]]\n",
    "        ),  # 如果没有标签，可以跳过\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # 前向传播\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits\n",
    "\n",
    "logits = run_inference(trained_model, sample_test_video[\"video\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b9f88",
   "metadata": {},
   "source": [
    "\n",
    "解码 `logits`，我们得到：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d97d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"预测类别:\", model.config.id2label[predicted_class_idx])\n",
    "# 预测类别: BasketballDunk"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
