{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0a3f3b",
   "metadata": {},
   "source": [
    "# è§†é¢‘åˆ†ç±»\n",
    "\n",
    "è§†é¢‘åˆ†ç±»æ˜¯æŒ‡ä¸ºæ•´ä¸ªè§†é¢‘åˆ†é…ä¸€ä¸ªæ ‡ç­¾æˆ–ç±»åˆ«ã€‚æ¯ä¸ªè§†é¢‘é€šå¸¸åªå±äºä¸€ä¸ªç±»åˆ«ã€‚è§†é¢‘åˆ†ç±»æ¨¡å‹æ¥æ”¶è§†é¢‘ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›è¯¥è§†é¢‘æ‰€å±ç±»åˆ«çš„é¢„æµ‹ç»“æœã€‚è¿™äº›æ¨¡å‹å¯ä»¥ç”¨äºå¯¹è§†é¢‘å†…å®¹è¿›è¡Œåˆ†ç±»ã€‚è§†é¢‘åˆ†ç±»çš„ä¸€ä¸ªå®é™…åº”ç”¨æ˜¯åŠ¨ä½œ/æ´»åŠ¨è¯†åˆ«ï¼Œè¿™åœ¨å¥èº«åº”ç”¨ç¨‹åºä¸­éå¸¸æœ‰ç”¨ã€‚å¯¹äºè§†éšœäººå£«ï¼Œå°¤å…¶æ˜¯åœ¨å‡ºè¡Œæ—¶ï¼Œè§†é¢‘åˆ†ç±»ä¹Ÿæ˜¯å¾ˆæœ‰å¸®åŠ©çš„ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—å°†å‘ä½ å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "1. åœ¨ [UCF101 æ•°æ®é›†](https://www.crcv.ucf.edu/data/UCF101.php)çš„å­é›†ä¸Šå¾®è°ƒ [VideoMAE](https://huggingface.co/docs/transformers/main/en/model_doc/videomae)ã€‚\n",
    "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "è¦æŸ¥çœ‹ä¸æœ¬ä»»åŠ¡å…¼å®¹çš„æ‰€æœ‰æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬æ¨èæŸ¥çœ‹ [ä»»åŠ¡é¡µé¢](https://huggingface.co/tasks/video-classification)ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿ä½ å·²ç»å®‰è£…äº†æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b2e424",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q pytorchvideo transformers evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d036ba",
   "metadata": {},
   "source": [
    "\n",
    "ä½ å°†ä½¿ç”¨ [PyTorchVideo](https://pytorchvideo.org/)ï¼ˆç®€ç§° `pytorchvideo`ï¼‰æ¥å¤„ç†å’Œå‡†å¤‡è§†é¢‘ã€‚\n",
    "\n",
    "æˆ‘ä»¬å»ºè®®ä½ ç™»å½•åˆ°ä½ çš„ Hugging Face è´¦æˆ·ï¼Œä»¥ä¾¿å¯ä»¥ä¸Šä¼ å’Œåˆ†äº«ä½ çš„æ¨¡å‹ã€‚å½“æç¤ºè¾“å…¥ä»¤ç‰Œæ—¶ï¼Œè¯·è¾“å…¥ä½ çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87893d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918abcd7",
   "metadata": {},
   "source": [
    "\n",
    "## åŠ è½½ UCF101 æ•°æ®é›†\n",
    "\n",
    "é¦–å…ˆåŠ è½½ [UCF-101 æ•°æ®é›†](https://www.crcv.ucf.edu/data/UCF101.php)çš„å­é›†ã€‚è¿™å°†è®©ä½ æœ‰æœºä¼šè¿›è¡Œå®éªŒå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸è¿è¡Œï¼Œç„¶åå†èŠ±è´¹æ›´å¤šæ—¶é—´åœ¨å®Œæ•´æ•°æ®é›†ä¸Šè®­ç»ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35da8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "hf_dataset_identifier = \"sayakpaul/ucf101-subset\"\n",
    "filename = \"UCF101_subset.tar.gz\"\n",
    "file_path = hf_hub_download(repo_id=hf_dataset_identifier, filename=filename, repo_type=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ab40af4",
   "metadata": {},
   "source": [
    "\n",
    "ä¸‹è½½å­é›†åï¼Œéœ€è¦è§£å‹å‹ç¼©åŒ…ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a30922",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(file_path) as t:\n",
    "    t.extractall(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e71346",
   "metadata": {},
   "source": [
    "\n",
    "æ•°æ®é›†çš„ç»„ç»‡ç»“æ„å¦‚ä¸‹ï¼š\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c3425d7d",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "UCF101_subset/\n",
    "    train/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    val/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ...\n",
    "    test/\n",
    "        BandMarching/\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        Archery\n",
    "            video_1.mp4\n",
    "            video_2.mp4\n",
    "            ...\n",
    "        ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb26decc",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åä½ å¯ä»¥ç»Ÿè®¡æ€»è§†é¢‘æ•°é‡ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9a410",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "dataset_root_path = \"UCF101_subset\"\n",
    "dataset_root_path = pathlib.Path(dataset_root_path)\n",
    "\n",
    "video_count_train = len(list(dataset_root_path.glob(\"train/*/ *.avi\")))\n",
    "video_count_val = len(list(dataset_root_path.glob(\"val/*/ *.avi\")))\n",
    "video_count_test = len(list(dataset_root_path.glob(\"test/*/ *.avi\")))\n",
    "video_total = video_count_train + video_count_val + video_count_test\n",
    "print(f\"æ€»è§†é¢‘æ•°: {video_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a512419",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86172b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_video_file_paths = (\n",
    "    list(dataset_root_path.glob(\"train/*/ *.avi\"))\n",
    "    + list(dataset_root_path.glob(\"val/*/ *.avi\"))\n",
    "    + list(dataset_root_path.glob(\"test/*/ *.avi\"))\n",
    ")\n",
    "all_video_file_paths[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4657b555",
   "metadata": {},
   "source": [
    "\n",
    "ï¼ˆæ’åºåçš„ï¼‰è§†é¢‘è·¯å¾„å¦‚ä¸‹æ‰€ç¤ºï¼š\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "447be6b8",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "...\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c04.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g07_c06.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g08_c01.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c02.avi',\n",
    "'UCF101_subset/train/ApplyEyeMakeup/v_ApplyEyeMakeup_g09_c06.avi'\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157eeb2e",
   "metadata": {},
   "source": [
    "\n",
    "ä½ ä¼šæ³¨æ„åˆ°ï¼Œæœ‰äº›è§†é¢‘ç‰‡æ®µå±äºåŒä¸€ä¸ªç»„/åœºæ™¯ï¼Œç»„ç”±è§†é¢‘æ–‡ä»¶è·¯å¾„ä¸­çš„ `g` è¡¨ç¤ºã€‚ä¾‹å¦‚ï¼Œ`v_ApplyEyeMakeup_g07_c04.avi` å’Œ `v_ApplyEyeMakeup_g07_c06.avi` å±äºåŒä¸€ç»„ã€‚\n",
    "\n",
    "å¯¹äºéªŒè¯å’Œè¯„ä¼°æ‹†åˆ†ï¼Œä½ ä¸å¸Œæœ›æœ‰æ¥è‡ªåŒä¸€ç»„/åœºæ™¯çš„è§†é¢‘ç‰‡æ®µï¼Œä»¥é˜²æ­¢æ•°æ®æ³„éœ²ã€‚æœ¬æ•™ç¨‹ä¸­ä½¿ç”¨çš„å­é›†è€ƒè™‘äº†è¿™ä¸€ç‚¹ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œä½ å°†ä»æ•°æ®é›†ä¸­æå–æ ‡ç­¾é›†ã€‚åŒæ—¶ï¼Œåˆ›å»ºä¸¤ä¸ªå­—å…¸ï¼Œæœ‰åŠ©äºåˆå§‹åŒ–æ¨¡å‹ï¼š\n",
    "\n",
    "- `label2id`ï¼šå°†ç±»åˆ«åç§°æ˜ å°„åˆ°æ•´æ•°ã€‚\n",
    "- `id2label`ï¼šå°†æ•´æ•°æ˜ å°„åˆ°ç±»åˆ«åç§°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3b7dde",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_labels = sorted({str(path).split(\"/\")[2] for path in all_video_file_paths})\n",
    "label2id = {label: i for i, label in enumerate(class_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"å”¯ä¸€ç±»åˆ«: {list(label2id.keys())}.\")\n",
    "# å”¯ä¸€ç±»åˆ«: ['ApplyEyeMakeup', 'ApplyLipstick', 'Archery', 'BabyCrawling', 'BalanceBeam', 'BandMarching', 'BaseballPitch', 'Basketball', 'BasketballDunk', 'BenchPress']."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0805e6c0",
   "metadata": {},
   "source": [
    "\n",
    "å…±æœ‰ 10 ä¸ªå”¯ä¸€ç±»åˆ«ã€‚æ¯ä¸ªç±»åˆ«åœ¨è®­ç»ƒé›†ä¸­æœ‰ 30 ä¸ªè§†é¢‘ã€‚\n",
    "\n",
    "## åŠ è½½æ¨¡å‹è¿›è¡Œå¾®è°ƒ\n",
    "\n",
    "ä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹å®ä¾‹åŒ–è§†é¢‘åˆ†ç±»æ¨¡å‹åŠå…¶å…³è”çš„å›¾åƒå¤„ç†å™¨ã€‚æ¨¡å‹çš„ç¼–ç å™¨å¸¦æœ‰é¢„è®­ç»ƒå‚æ•°ï¼Œè€Œåˆ†ç±»å¤´åˆ™æ˜¯éšæœºåˆå§‹åŒ–çš„ã€‚å›¾åƒå¤„ç†å™¨å°†åœ¨ç¼–å†™æ•°æ®é›†çš„é¢„å¤„ç†ç®¡é“æ—¶æ´¾ä¸Šç”¨åœºã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c7639c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import VideoMAEImageProcessor, VideoMAEForVideoClassification\n",
    "\n",
    "model_ckpt = \"MCG-NJU/videomae-base\"\n",
    "image_processor = VideoMAEImageProcessor.from_pretrained(model_ckpt)\n",
    "model = VideoMAEForVideoClassification.from_pretrained(\n",
    "    model_ckpt,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    "    ignore_mismatched_sizes=True,  # å¦‚æœä½ è®¡åˆ’å¾®è°ƒä¸€ä¸ªå·²ç»å¾®è°ƒè¿‡çš„æ£€æŸ¥ç‚¹ï¼Œæä¾›æ­¤å‚æ•°\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec0a09f",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨åŠ è½½æ¨¡å‹æ—¶ï¼Œä½ å¯èƒ½ä¼šæ³¨æ„åˆ°ä»¥ä¸‹è­¦å‘Šï¼š\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "39968956",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "æŸäº›é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸­çš„æƒé‡åœ¨åˆå§‹åŒ– VideoMAEForVideoClassification æ—¶æœªè¢«ä½¿ç”¨ï¼š\n",
    "- è¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºï¼Œå¦‚æœä½ æ˜¯ä»å¦ä¸€ä¸ªä»»åŠ¡æˆ–æ¶æ„çš„é¢„è®­ç»ƒæ¨¡å‹åˆå§‹åŒ– VideoMAEForVideoClassificationï¼ˆä¾‹å¦‚ï¼Œä» BertForPreTraining åˆå§‹åŒ– BertForSequenceClassification æ¨¡å‹ï¼‰ã€‚\n",
    "- å¦‚æœä½ æœŸæœ›åˆå§‹åŒ–çš„ VideoMAEForVideoClassification ä¸é¢„è®­ç»ƒæ¨¡å‹å®Œå…¨ç›¸åŒï¼ˆä¾‹å¦‚ï¼Œä» BertForSequenceClassification åˆå§‹åŒ– BertForSequenceClassification æ¨¡å‹ï¼‰ï¼Œåˆ™è¿™ä¸æ˜¯é¢„æœŸçš„è¡Œä¸ºã€‚\n",
    "VideoMAEForVideoClassification çš„ä¸€äº›æƒé‡æœªä»é¢„è®­ç»ƒæ£€æŸ¥ç‚¹ä¸­åˆå§‹åŒ–ï¼Œè€Œæ˜¯æ–°åˆå§‹åŒ–çš„ï¼š\n",
    "- classifier.bias, classifier.weight\n",
    "ä½ åº”è¯¥åœ¨æ­¤æ¨¡å‹ä¸Šè®­ç»ƒä¸‹æ¸¸ä»»åŠ¡ï¼Œä»¥ä¾¿èƒ½å¤Ÿä½¿ç”¨å®ƒè¿›è¡Œé¢„æµ‹å’Œæ¨ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bcf4ac",
   "metadata": {},
   "source": [
    "\n",
    "è­¦å‘Šå‘Šè¯‰æˆ‘ä»¬ï¼Œæˆ‘ä»¬åœ¨ä¸¢å¼ƒä¸€äº›æƒé‡ï¼ˆä¾‹å¦‚ `classifier` å±‚çš„æƒé‡å’Œåç½®ï¼‰ï¼Œå¹¶éšæœºåˆå§‹åŒ–ä¸€äº›å…¶ä»–æƒé‡ï¼ˆæ–°çš„ `classifier` å±‚çš„æƒé‡å’Œåç½®ï¼‰ã€‚è¿™æ˜¯é¢„æœŸçš„è¡Œä¸ºï¼Œå› ä¸ºæˆ‘ä»¬åœ¨æ·»åŠ ä¸€ä¸ªæ–°çš„å¤´éƒ¨ï¼Œè€Œæ²¡æœ‰é¢„è®­ç»ƒçš„æƒé‡ï¼Œå› æ­¤åº“æé†’æˆ‘ä»¬éœ€è¦åœ¨ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ä¹‹å‰å¯¹å…¶è¿›è¡Œå¾®è°ƒï¼Œè€Œè¿™æ­£æ˜¯æˆ‘ä»¬å°†è¦åšçš„ã€‚\n",
    "\n",
    "**æ³¨æ„**ï¼š[æ­¤æ£€æŸ¥ç‚¹](https://huggingface.co/MCG-NJU/videomae-base-finetuned-kinetics)åœ¨ç±»ä¼¼ä¸‹æ¸¸ä»»åŠ¡ä¸Šè¿›è¡Œäº†å¾®è°ƒï¼Œå…·æœ‰ç›¸å½“å¤§çš„é¢†åŸŸé‡å ï¼Œå› æ­¤åœ¨æœ¬ä»»åŠ¡ä¸Šçš„è¡¨ç°æ›´å¥½ã€‚ä½ å¯ä»¥æŸ¥çœ‹ [æ­¤æ£€æŸ¥ç‚¹](https://huggingface.co/sayakpaul/videomae-base-finetuned-kinetics-finetuned-ucf101-subset)ï¼Œå®ƒæ˜¯é€šè¿‡å¯¹ `MCG-NJU/videomae-base-finetuned-kinetics` è¿›è¡Œå¾®è°ƒè·å¾—çš„ã€‚\n",
    "\n",
    "## å‡†å¤‡æ•°æ®é›†è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "ä¸ºäº†é¢„å¤„ç†è§†é¢‘ï¼Œä½ å°†åˆ©ç”¨ [PyTorchVideo åº“](https://pytorchvideo.org/)ã€‚é¦–å…ˆå¯¼å…¥æ‰€éœ€çš„ä¾èµ–é¡¹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d425f0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorchvideo.data\n",
    "\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2767ec93",
   "metadata": {},
   "source": [
    "\n",
    "å¯¹äºè®­ç»ƒæ•°æ®é›†çš„å˜æ¢ï¼Œä½¿ç”¨å‡åŒ€æ—¶é—´é‡‡æ ·ã€åƒç´ å½’ä¸€åŒ–ã€éšæœºè£å‰ªå’Œéšæœºæ°´å¹³ç¿»è½¬çš„ç»„åˆã€‚å¯¹äºéªŒè¯å’Œè¯„ä¼°æ•°æ®é›†çš„å˜æ¢ï¼Œä½¿ç”¨ç›¸åŒçš„å˜æ¢é“¾ï¼Œä½†ä¸åŒ…æ‹¬éšæœºè£å‰ªå’Œæ°´å¹³ç¿»è½¬ã€‚æœ‰å…³è¿™äº›å˜æ¢çš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚é˜… [PyTorchVideo å®˜æ–¹æ–‡æ¡£](https://pytorchvideo.org)ã€‚\n",
    "\n",
    "ä½¿ç”¨ä¸é¢„è®­ç»ƒæ¨¡å‹å…³è”çš„ `image_processor` è·å–ä»¥ä¸‹ä¿¡æ¯ï¼š\n",
    "\n",
    "- ç”¨äºå½’ä¸€åŒ–è§†é¢‘å¸§åƒç´ çš„å›¾åƒå‡å€¼å’Œæ ‡å‡†å·®ã€‚\n",
    "- è§†é¢‘å¸§å°†è°ƒæ•´å¤§å°çš„ç©ºé—´åˆ†è¾¨ç‡ã€‚\n",
    "\n",
    "é¦–å…ˆå®šä¹‰ä¸€äº›å¸¸é‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea3a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = image_processor.image_mean\n",
    "std = image_processor.image_std\n",
    "if \"shortest_edge\" in image_processor.size:\n",
    "    height = width = image_processor.size[\"shortest_edge\"]\n",
    "else:\n",
    "    height = image_processor.size[\"height\"]\n",
    "    width = image_processor.size[\"width\"]\n",
    "resize_to = (height, width)\n",
    "\n",
    "num_frames_to_sample = model.config.num_frames\n",
    "sample_rate = 4\n",
    "fps = 30\n",
    "clip_duration = num_frames_to_sample * sample_rate / fps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948ef281",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ï¼Œåˆ†åˆ«å®šä¹‰æ•°æ®é›†ç‰¹å®šçš„å˜æ¢å’Œæ•°æ®é›†ã€‚ä»è®­ç»ƒé›†å¼€å§‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa292c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            RandomShortSideScale(min_size=256, max_size=320),\n",
    "            RandomCrop(resize_to),\n",
    "            RandomHorizontalFlip(p=0.5),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "train_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"train\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"random\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=train_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf7ab67",
   "metadata": {},
   "source": [
    "\n",
    "åŒæ ·çš„å·¥ä½œæµç¨‹å¯ä»¥åº”ç”¨äºéªŒè¯å’Œè¯„ä¼°é›†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c73a693",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_transform = Compose([\n",
    "    ApplyTransformToKey(\n",
    "        key=\"video\",\n",
    "        transform=Compose([\n",
    "            UniformTemporalSubsample(num_frames_to_sample),\n",
    "            Lambda(lambda x: x / 255.0),\n",
    "            Normalize(mean, std),\n",
    "            Resize(resize_to),\n",
    "        ]),\n",
    "    ),\n",
    "])\n",
    "\n",
    "val_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"val\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")\n",
    "\n",
    "test_dataset = pytorchvideo.data.Ucf101(\n",
    "    data_path=os.path.join(dataset_root_path, \"test\"),\n",
    "    clip_sampler=pytorchvideo.data.make_clip_sampler(\"uniform\", clip_duration),\n",
    "    decode_audio=False,\n",
    "    transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e0d68d3",
   "metadata": {},
   "source": [
    "\n",
    "**æ³¨æ„**ï¼šä¸Šè¿°æ•°æ®é›†ç®¡é“å–è‡ª [PyTorchVideo å®˜æ–¹ç¤ºä¾‹](https://pytorchvideo.org/docs/tutorial_classification#dataset)ã€‚æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ `[pytorchvideo.data.Ucf101()](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.Ucf101)` å‡½æ•°ï¼Œå› ä¸ºå®ƒä¸“ä¸º UCF-101 æ•°æ®é›†å®šåˆ¶ã€‚åœ¨åº•å±‚ï¼Œå®ƒè¿”å›ä¸€ä¸ª `[pytorchvideo.data.labeled_video_dataset.LabeledVideoDataset](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html#pytorchvideo.data.LabeledVideoDataset)` å¯¹è±¡ã€‚`LabeledVideoDataset` ç±»æ˜¯ PyTorchVideo æ•°æ®é›†ä¸­æ‰€æœ‰è§†é¢‘çš„åŸºç¡€ç±»ã€‚å› æ­¤ï¼Œå¦‚æœä½ æƒ³ä½¿ç”¨ä¸æ”¯æŒçš„è‡ªå®šä¹‰æ•°æ®é›†ï¼Œå¯ä»¥æ‰©å±• `LabeledVideoDataset` ç±»ã€‚æ›´å¤šè¯¦æƒ…è¯·å‚è€ƒ `data` API [æ–‡æ¡£](https://pytorchvideo.readthedocs.io/en/latest/api/data/data.html)ã€‚å¦‚æœä½ çš„æ•°æ®é›†ç»“æ„ä¸æ­¤ç±»ä¼¼ï¼ˆå¦‚ä¸Šæ‰€ç¤ºï¼‰ï¼Œé‚£ä¹ˆä½¿ç”¨ `pytorchvideo.data.Ucf101()` åº”è¯¥æ²¡é—®é¢˜ã€‚\n",
    "\n",
    "ä½ å¯ä»¥é€šè¿‡è®¿é—® `num_videos` å‚æ•°æ¥äº†è§£æ•°æ®é›†ä¸­çš„è§†é¢‘æ•°é‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307fbbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_dataset.num_videos, val_dataset.num_videos, test_dataset.num_videos)\n",
    "# (300, 30, 75)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17289a66",
   "metadata": {},
   "source": [
    "\n",
    "## å¯è§†åŒ–é¢„å¤„ç†åçš„è§†é¢‘ä»¥ä¾¿æ›´å¥½åœ°è°ƒè¯•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7543953d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import numpy as np\n",
    "from IPython.display import Image\n",
    "\n",
    "def unnormalize_img(img):\n",
    "    \"\"\"åå½’ä¸€åŒ–å›¾åƒåƒç´ ã€‚\"\"\"\n",
    "    img = (img * std) + mean\n",
    "    img = (img * 255).astype(\"uint8\")\n",
    "    return img.clip(0, 255)\n",
    "\n",
    "def create_gif(video_tensor, filename=\"sample.gif\"):\n",
    "    \"\"\"ä»è§†é¢‘å¼ é‡ä¸­ç”Ÿæˆ GIFã€‚\n",
    "    \n",
    "    æœŸæœ›è§†é¢‘å¼ é‡çš„å½¢çŠ¶ä¸ºï¼š\n",
    "    (num_frames, num_channels, height, width)ã€‚\n",
    "    \"\"\"\n",
    "    frames = []\n",
    "    for video_frame in video_tensor:\n",
    "        frame_unnormalized = unnormalize_img(video_frame.permute(1, 2, 0).numpy())\n",
    "        frames.append(frame_unnormalized)\n",
    "    kargs = {\"duration\": 0.25}\n",
    "    imageio.mimsave(filename, frames, \"GIF\", **kargs)\n",
    "    return filename\n",
    "\n",
    "def display_gif(video_tensor, gif_name=\"sample.gif\"):\n",
    "    \"\"\"ä»è§†é¢‘å¼ é‡ä¸­ç”Ÿæˆå¹¶æ˜¾ç¤º GIFã€‚\"\"\"\n",
    "    video_tensor = video_tensor.permute(1, 0, 2, 3)\n",
    "    gif_filename = create_gif(video_tensor, gif_name)\n",
    "    return Image(filename=gif_filename)\n",
    "\n",
    "sample_video = next(iter(train_dataset))\n",
    "video_tensor = sample_video[\"video\"]\n",
    "display_gif(video_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0dcde3",
   "metadata": {},
   "source": [
    "\n",
    "![æ‰“ç¯®çƒçš„äºº](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif.gif)\n",
    "\n",
    "## è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "ä½¿ç”¨ ğŸ¤— Transformers çš„ [`Trainer`](https://huggingface.co/docs/transformers/main_classes/trainer) æ¥è®­ç»ƒæ¨¡å‹ã€‚è¦å®ä¾‹åŒ– `Trainer`ï¼Œä½ éœ€è¦å®šä¹‰è®­ç»ƒé…ç½®å’Œè¯„ä¼°æŒ‡æ ‡ã€‚æœ€é‡è¦çš„æ˜¯ `TrainingArguments`ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ‰€æœ‰å±æ€§ä»¥é…ç½®è®­ç»ƒçš„ç±»ã€‚å®ƒéœ€è¦ä¸€ä¸ªè¾“å‡ºæ–‡ä»¶å¤¹åç§°ï¼Œè¯¥åç§°å°†ç”¨äºä¿å­˜æ¨¡å‹çš„æ£€æŸ¥ç‚¹ã€‚å®ƒè¿˜æœ‰åŠ©äºåœ¨ ğŸ¤— Hub ä¸ŠåŒæ­¥æ¨¡å‹ä»“åº“ä¸­çš„æ‰€æœ‰ä¿¡æ¯ã€‚\n",
    "\n",
    "å¤§å¤šæ•°è®­ç»ƒå‚æ•°éƒ½æ˜¯è‡ªè§£é‡Šçš„ï¼Œä½†å…¶ä¸­ä¸€ä¸ªç›¸å½“é‡è¦çš„æ˜¯ `remove_unused_columns=False`ã€‚è¿™ä¸ªå‚æ•°ä¼šåˆ é™¤æ¨¡å‹è°ƒç”¨å‡½æ•°æœªä½¿ç”¨çš„æ‰€æœ‰ç‰¹å¾ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œå®ƒä¸º `True`ï¼Œå› ä¸ºåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œåˆ é™¤æœªä½¿ç”¨çš„ç‰¹å¾åˆ—æ˜¯ç†æƒ³çš„ï¼Œä½¿å…¶æ›´å®¹æ˜“å°†è¾“å…¥è§£åŒ…åˆ°æ¨¡å‹çš„è°ƒç”¨å‡½æ•°ä¸­ã€‚ä½†åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä½ éœ€è¦æœªä½¿ç”¨çš„ç‰¹å¾ï¼ˆç‰¹åˆ«æ˜¯ `video`ï¼‰æ¥åˆ›å»º `pixel_values`ï¼ˆè¿™æ˜¯æ¨¡å‹è¾“å…¥ä¸­å¿…éœ€çš„é”®ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976c7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "model_name = model_ckpt.split(\"/\")[-1]\n",
    "new_model_name = f\"{model_name}-finetuned-ucf101-subset\"\n",
    "num_epochs = 4\n",
    "\n",
    "args = TrainingArguments(\n",
    "    new_model_name,\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    "    max_steps=(train_dataset.num_videos // batch_size) * num_epochs,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141dd85c",
   "metadata": {},
   "source": [
    "\n",
    "`pytorchvideo.data.Ucf101()` è¿”å›çš„æ•°æ®é›†æ²¡æœ‰å®ç° `__len__` æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åœ¨å®ä¾‹åŒ– `TrainingArguments` æ—¶å¿…é¡»å®šä¹‰ `max_steps`ã€‚\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦å®šä¹‰ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—é¢„æµ‹çš„æŒ‡æ ‡ï¼Œè¿™å°†ä½¿ç”¨ä½ ç°åœ¨åŠ è½½çš„ `metric`ã€‚å”¯ä¸€çš„é¢„å¤„ç†æ˜¯ä½ éœ€è¦å¯¹é¢„æµ‹çš„ logits è¿›è¡Œ argmaxï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3d44aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35526748",
   "metadata": {},
   "source": [
    "\n",
    "**å…³äºè¯„ä¼°çš„è¯´æ˜**ï¼š\n",
    "\n",
    "åœ¨ [VideoMAE è®ºæ–‡](https://arxiv.org/abs/2203.12602)ä¸­ï¼Œä½œè€…ä½¿ç”¨äº†ä»¥ä¸‹è¯„ä¼°ç­–ç•¥ã€‚ä»–ä»¬åœ¨æµ‹è¯•è§†é¢‘çš„å¤šä¸ªç‰‡æ®µä¸Šè¯„ä¼°æ¨¡å‹ï¼Œå¹¶å¯¹è¿™äº›ç‰‡æ®µåº”ç”¨ä¸åŒçš„è£å‰ªï¼ŒæŠ¥å‘Šèšåˆåˆ†æ•°ã€‚ç„¶è€Œï¼Œä¸ºäº†ç®€å•å’Œç®€æ´ï¼Œæœ¬æ•™ç¨‹ä¸­ä¸è€ƒè™‘è¿™ä¸€ç‚¹ã€‚\n",
    "\n",
    "ä½ è¿˜éœ€å®šä¹‰ä¸€ä¸ª `collate_fn`ï¼Œç”¨äºå°†æ ·æœ¬æ‰¹å¤„ç†åœ¨ä¸€èµ·ã€‚æ¯ä¸ªæ‰¹æ¬¡åŒ…å«ä¸¤ä¸ªé”®ï¼Œå³ `pixel_values` å’Œ `labels`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014e84f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    # è°ƒæ•´ä¸º (num_frames, num_channels, height, width)\n",
    "    pixel_values = torch.stack(\n",
    "        [example[\"video\"].permute(1, 0, 2, 3) for example in examples]\n",
    "    )\n",
    "    labels = torch.tensor([example[\"label\"] for example in examples])\n",
    "    return {\"pixel_values\": pixel_values, \"labels\": labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46362349",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œä½ åªéœ€å°†æ‰€æœ‰è¿™äº›å†…å®¹ä»¥åŠæ•°æ®é›†ä¼ é€’ç»™ `Trainer`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f576fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857bd496",
   "metadata": {},
   "source": [
    "\n",
    "ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ä¸ºä»€ä¹ˆåœ¨é¢„å¤„ç†æ•°æ®æ—¶å·²ç»ä¼ å…¥äº† `image_processor`ã€‚è¿™åªæ˜¯ä¸ºäº†ç¡®ä¿å›¾åƒå¤„ç†å™¨çš„é…ç½®æ–‡ä»¶ï¼ˆå­˜å‚¨ä¸º JSONï¼‰ä¹Ÿä¼šä¸Šä¼ åˆ° Hub ä»“åº“ä¸­ã€‚\n",
    "\n",
    "ç°åœ¨é€šè¿‡è°ƒç”¨ `train` æ–¹æ³•å¾®è°ƒæ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebcad2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_results = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8874d09",
   "metadata": {},
   "source": [
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ `push_to_hub()` æ–¹æ³•å°†æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œè¿™æ ·æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨ä½ çš„æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccf980b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8aa108",
   "metadata": {},
   "source": [
    "\n",
    "## æ¨ç†\n",
    "\n",
    "å¤ªå¥½äº†ï¼Œç°åœ¨ä½ å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼\n",
    "\n",
    "åŠ è½½ä¸€ä¸ªè§†é¢‘ç”¨äºæ¨ç†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d202fa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test_video = next(iter(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e807677",
   "metadata": {},
   "source": [
    "\n",
    "![é˜Ÿä¼è¿›è¡Œç¯®çƒæ¯”èµ›](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/sample_gif_two.gif)\n",
    "\n",
    "æœ€ç®€å•çš„æ–¹æ³•æ˜¯ä½¿ç”¨æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œå°±æ˜¯å°†å…¶æ”¾åœ¨ä¸€ä¸ª `pipeline` ä¸­ã€‚å®ä¾‹åŒ–ä¸€ä¸ªç”¨äºè§†é¢‘åˆ†ç±»çš„ `pipeline` å¹¶ä¼ é€’ä½ çš„è§†é¢‘ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb3784a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "video_cls = pipeline(model=\"my_awesome_video_cls_model\")\n",
    "video_cls(\"https://huggingface.co/datasets/sayakpaul/ucf101-subset/resolve/main/v_BasketballDunk_g14_c06.avi\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87679e3f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "e39c58f5",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "[\n",
    "    {'score': 0.9272987842559814, 'label': 'BasketballDunk'},\n",
    "    {'score': 0.017777055501937866, 'label': 'BabyCrawling'},\n",
    "    {'score': 0.01663011871278286, 'label': 'BalanceBeam'},\n",
    "    {'score': 0.009560945443809032, 'label': 'BandMarching'},\n",
    "    {'score': 0.0068979403004050255, 'label': 'BaseballPitch'}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464889c3",
   "metadata": {},
   "source": [
    "\n",
    "ä½ ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼Œå¦‚æœä½ æ„¿æ„çš„è¯ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0244689",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, video):\n",
    "    # (num_frames, num_channels, height, width)\n",
    "    perumuted_sample_test_video = video.permute(1, 0, 2, 3)\n",
    "    inputs = {\n",
    "        \"pixel_values\": perumuted_sample_test_video.unsqueeze(0),\n",
    "        \"labels\": torch.tensor(\n",
    "            [sample_test_video[\"label\"]]\n",
    "        ),  # å¦‚æœæ²¡æœ‰æ ‡ç­¾ï¼Œå¯ä»¥è·³è¿‡\n",
    "    }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    model = model.to(device)\n",
    "\n",
    "    # å‰å‘ä¼ æ’­\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    return logits\n",
    "\n",
    "logits = run_inference(trained_model, sample_test_video[\"video\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362b9f88",
   "metadata": {},
   "source": [
    "\n",
    "è§£ç  `logits`ï¼Œæˆ‘ä»¬å¾—åˆ°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8d97d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_idx = logits.argmax(-1).item()\n",
    "print(\"é¢„æµ‹ç±»åˆ«:\", model.config.id2label[predicted_class_idx])\n",
    "# é¢„æµ‹ç±»åˆ«: BasketballDunk"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
