{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1c658a3",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬ç”Ÿæˆç­–ç•¥\n",
    "\n",
    "æ–‡æœ¬ç”Ÿæˆæ˜¯è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„æ ¸å¿ƒï¼Œä¾‹å¦‚å¼€æ”¾å¼æ–‡æœ¬ç”Ÿæˆã€æ‘˜è¦ã€ç¿»è¯‘ç­‰ã€‚å®ƒè¿˜åœ¨å„ç§æ··åˆæ¨¡å¼åº”ç”¨ä¸­å‘æŒ¥ä½œç”¨ï¼Œè¿™äº›åº”ç”¨ä»¥æ–‡æœ¬ä½œä¸ºè¾“å‡ºï¼Œä¾‹å¦‚è¯­éŸ³è½¬æ–‡æœ¬å’Œè§†è§‰è½¬æ–‡æœ¬ã€‚ä¸€äº›å¯ä»¥ç”Ÿæˆæ–‡æœ¬çš„æ¨¡å‹åŒ…æ‹¬ GPT2ã€XLNetã€OpenAI GPTã€CTRLã€TransformerXLã€XLMã€Bartã€T5ã€GIT å’Œ Whisperã€‚\n",
    "\n",
    "æŸ¥çœ‹ä¸€äº›ä½¿ç”¨ `generate()` æ–¹æ³•ä¸ºä¸åŒä»»åŠ¡ç”Ÿæˆæ–‡æœ¬è¾“å‡ºçš„ç¤ºä¾‹ï¼š\n",
    "\n",
    "- [æ–‡æœ¬æ‘˜è¦](./tasks/summarization#inference)\n",
    "- [å›¾åƒæè¿°](./model_doc/git#transformers.GitForCausalLM.forward.example)\n",
    "- [éŸ³é¢‘è½¬å½•](./model_doc/whisper#transformers.WhisperForConditionalGeneration.forward.example)\n",
    "\n",
    "è¯·æ³¨æ„ï¼Œ`generate` æ–¹æ³•çš„è¾“å…¥å–å†³äºæ¨¡å‹çš„æ¨¡æ€ã€‚å®ƒä»¬ç”±æ¨¡å‹çš„é¢„å¤„ç†å™¨ç±»è¿”å›ï¼Œä¾‹å¦‚ AutoTokenizer æˆ– AutoProcessorã€‚å¦‚æœä¸€ä¸ªæ¨¡å‹çš„é¢„å¤„ç†å™¨åˆ›å»ºå¤šç§è¾“å…¥ï¼Œè¯·å°†æ‰€æœ‰è¾“å…¥ä¼ é€’ç»™ `generate()`ã€‚ä½ å¯ä»¥åœ¨ç›¸åº”çš„æ¨¡å‹æ–‡æ¡£ä¸­äº†è§£æ›´å¤šå…³äºå•ä¸ªæ¨¡å‹é¢„å¤„ç†å™¨çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "é€‰æ‹©è¾“å‡ºæ ‡è®°ä»¥ç”Ÿæˆæ–‡æœ¬çš„è¿‡ç¨‹ç§°ä¸ºè§£ç ï¼Œä½ å¯ä»¥è‡ªå®šä¹‰ `generate()` æ–¹æ³•å°†ä½¿ç”¨çš„è§£ç ç­–ç•¥ã€‚ä¿®æ”¹è§£ç ç­–ç•¥ä¸ä¼šæ”¹å˜ä»»ä½•å¯è®­ç»ƒå‚æ•°çš„å€¼ã€‚ç„¶è€Œï¼Œå®ƒå¯ä»¥å¯¹ç”Ÿæˆè¾“å‡ºçš„è´¨é‡äº§ç”Ÿæ˜¾è‘—å½±å“ã€‚å®ƒå¯ä»¥å¸®åŠ©å‡å°‘æ–‡æœ¬ä¸­çš„é‡å¤ï¼Œå¹¶ä½¿å…¶æ›´åŠ è¿è´¯ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—æè¿°äº†ï¼š\n",
    "\n",
    "- é»˜è®¤ç”Ÿæˆé…ç½®\n",
    "- å¸¸è§çš„è§£ç ç­–ç•¥åŠå…¶ä¸»è¦å‚æ•°\n",
    "- å°†è‡ªå®šä¹‰ç”Ÿæˆé…ç½®ä¸ä½ åœ¨ ğŸ¤— Hub ä¸Šå¾®è°ƒçš„æ¨¡å‹ä¸€èµ·ä¿å­˜å’Œå…±äº«\n",
    "\n",
    "## é»˜è®¤æ–‡æœ¬ç”Ÿæˆé…ç½®\n",
    "\n",
    "æ¨¡å‹çš„è§£ç ç­–ç•¥åœ¨å…¶ç”Ÿæˆé…ç½®ä¸­å®šä¹‰ã€‚å½“åœ¨ [pipeline()](/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) å†…éƒ¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œæ¨ç†æ—¶ï¼Œæ¨¡å‹è°ƒç”¨ `PreTrainedModel.generate()` æ–¹æ³•ï¼Œè¯¥æ–¹æ³•åœ¨å¹•ååº”ç”¨é»˜è®¤ç”Ÿæˆé…ç½®ã€‚å½“æ²¡æœ‰ä¸æ¨¡å‹ä¸€èµ·ä¿å­˜è‡ªå®šä¹‰é…ç½®æ—¶ï¼Œä¹Ÿä½¿ç”¨é»˜è®¤é…ç½®ã€‚\n",
    "\n",
    "å½“ä½ æ˜¾å¼åŠ è½½æ¨¡å‹æ—¶ï¼Œå¯ä»¥é€šè¿‡ `model.generation_config` æ£€æŸ¥å…¶é™„å¸¦çš„ç”Ÿæˆé…ç½®ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dbae609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n",
    "model.generation_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c536d36",
   "metadata": {},
   "source": [
    "\n",
    "æ‰“å° `model.generation_config` åªä¼šæ˜¾ç¤ºä¸é»˜è®¤ç”Ÿæˆé…ç½®ä¸åŒçš„å€¼ï¼Œè€Œä¸ä¼šåˆ—å‡ºä»»ä½•é»˜è®¤å€¼ã€‚\n",
    "\n",
    "é»˜è®¤ç”Ÿæˆé…ç½®å°†è¾“å‡ºä¸è¾“å…¥æç¤ºçš„å¤§å°é™åˆ¶ä¸ºæœ€å¤š 20 ä¸ªæ ‡è®°ï¼Œä»¥é¿å…é‡åˆ°èµ„æºé™åˆ¶ã€‚é»˜è®¤è§£ç ç­–ç•¥æ˜¯è´ªå©ªæœç´¢ï¼Œè¿™æ˜¯æœ€ç®€å•çš„è§£ç ç­–ç•¥ï¼Œå®ƒé€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°ã€‚å¯¹äºè®¸å¤šä»»åŠ¡å’Œå°è¾“å‡ºå¤§å°ï¼Œè¿™å·¥ä½œå¾—å¾ˆå¥½ã€‚ç„¶è€Œï¼Œå½“ç”¨äºç”Ÿæˆæ›´é•¿çš„è¾“å‡ºæ—¶ï¼Œè´ªå©ªæœç´¢å¯èƒ½ä¼šå¼€å§‹äº§ç”Ÿé«˜åº¦é‡å¤çš„ç»“æœã€‚\n",
    "\n",
    "## è‡ªå®šä¹‰æ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "ä½ å¯ä»¥é€šè¿‡å°†å‚æ•°åŠå…¶å€¼ç›´æ¥ä¼ é€’ç»™ `generate` æ–¹æ³•æ¥è¦†ç›–ä»»ä½• `generation_config`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ae3df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_model.generate(**inputs, num_beams=4, do_sample=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281ccb5f",
   "metadata": {},
   "source": [
    "\n",
    "å³ä½¿é»˜è®¤è§£ç ç­–ç•¥ä¸»è¦é€‚ç”¨äºä½ çš„ä»»åŠ¡ï¼Œä½ ä»ç„¶å¯ä»¥è°ƒæ•´ä¸€äº›ä¸œè¥¿ã€‚ä¸€äº›å¸¸è§çš„è°ƒæ•´å‚æ•°åŒ…æ‹¬ï¼š\n",
    "\n",
    "- `max_new_tokens`ï¼šè¦ç”Ÿæˆçš„æœ€å¤§æ ‡è®°æ•°ã€‚æ¢å¥è¯è¯´ï¼Œè¾“å‡ºåºåˆ—çš„å¤§å°ï¼Œä¸åŒ…æ‹¬æç¤ºä¸­çš„æ ‡è®°ã€‚ä½œä¸ºä½¿ç”¨è¾“å‡ºé•¿åº¦ä½œä¸ºåœæ­¢æ ‡å‡†çš„æ›¿ä»£æ–¹æ¡ˆï¼Œä½ å¯ä»¥é€‰æ‹©åœ¨å®Œæ•´ç”Ÿæˆè¶…è¿‡ä¸€å®šæ—¶é—´æ—¶åœæ­¢ç”Ÿæˆã€‚è¦äº†è§£æ›´å¤šä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [StoppingCriteria](/docs/transformers/main/en/internal/generation_utils#transformers.StoppingCriteria)ã€‚\n",
    "- `num_beams`ï¼šé€šè¿‡æŒ‡å®šå¤§äº 1 çš„å…‰æŸæ•°ï¼Œä½ å®é™…ä¸Šæ˜¯ä»è´ªå©ªæœç´¢åˆ‡æ¢åˆ°å…‰æŸæœç´¢ã€‚è¿™ç§ç­–ç•¥åœ¨æ¯ä¸ªæ—¶é—´æ­¥è¯„ä¼°å¤šä¸ªå‡è®¾ï¼Œå¹¶æœ€ç»ˆé€‰æ‹©æ•´ä¸ªåºåˆ—å…·æœ‰æœ€é«˜æ¦‚ç‡çš„å‡è®¾ã€‚è¿™æœ‰åŠ©äºè¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹æ ‡è®°å¼€å§‹çš„å…·æœ‰è¾ƒé«˜æ¦‚ç‡çš„åºåˆ—ï¼Œè¿™äº›åºåˆ—ä¼šè¢«è´ªå©ªæœç´¢å¿½ç•¥ã€‚å¯è§†åŒ–å®ƒå¦‚ä½•å·¥ä½œ [è¿™é‡Œ](https://huggingface.co/spaces/m-ric/beam_search_visualizer)ã€‚\n",
    "- `do_sample`ï¼šå¦‚æœè®¾ç½®ä¸º `True`ï¼Œæ­¤å‚æ•°å°†å¯ç”¨è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚å¤šé¡¹å¼é‡‡æ ·ã€å…‰æŸæœç´¢å¤šé¡¹å¼é‡‡æ ·ã€Top-K é‡‡æ ·å’Œ Top-p é‡‡æ ·ã€‚æ‰€æœ‰è¿™äº›ç­–ç•¥éƒ½ä»æ•´ä¸ªè¯æ±‡çš„æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ï¼Œå¹¶å…·æœ‰å„ç§ç­–ç•¥ç‰¹å®šçš„è°ƒæ•´ã€‚\n",
    "- `num_return_sequences`ï¼šæ¯ä¸ªè¾“å…¥è¦è¿”å›çš„åºåˆ—å€™é€‰æ•°ã€‚æ­¤é€‰é¡¹ä»…é€‚ç”¨äºæ”¯æŒå¤šä¸ªåºåˆ—å€™é€‰çš„è§£ç ç­–ç•¥ï¼Œä¾‹å¦‚å…‰æŸæœç´¢å’Œé‡‡æ ·çš„å˜ä½“ã€‚åƒè´ªå©ªæœç´¢å’Œå¯¹æ¯”æœç´¢è¿™æ ·çš„è§£ç ç­–ç•¥è¿”å›å•ä¸ªè¾“å‡ºåºåˆ—ã€‚\n",
    "\n",
    "## ä¿å­˜è‡ªå®šä¹‰è§£ç ç­–ç•¥ä¸ä½ çš„æ¨¡å‹\n",
    "\n",
    "å¦‚æœä½ æƒ³è¦åˆ†äº«ä½ çš„å¾®è°ƒæ¨¡å‹ä»¥åŠç‰¹å®šçš„ç”Ÿæˆé…ç½®ï¼Œä½ å¯ä»¥ï¼š\n",
    "\n",
    "- åˆ›å»ºä¸€ä¸ª [GenerationConfig](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) ç±»å®ä¾‹\n",
    "- æŒ‡å®šè§£ç ç­–ç•¥å‚æ•°\n",
    "- ä½¿ç”¨ [GenerationConfig.save_pretrained()](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained) ä¿å­˜ä½ çš„ç”Ÿæˆé…ç½®ï¼Œç¡®ä¿å…¶ `config_file_name` å‚æ•°ä¸ºç©º\n",
    "- å°† `push_to_hub` è®¾ç½®ä¸º `True` ä»¥å°†ä½ çš„é…ç½®ä¸Šä¼ åˆ°æ¨¡å‹çš„ä»“åº“\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b43089a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"my_account/my_model\")\n",
    "generation_config = GenerationConfig(\n",
    "    max_new_tokens=50, do_sample=True, top_k=50, eos_token_id=model.config.eos_token_id\n",
    ")\n",
    "generation_config.save_pretrained(\"my_account/my_model\", push_to_hub=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af3c49d",
   "metadata": {},
   "source": [
    "\n",
    "ä½ è¿˜å¯ä»¥åœ¨å•ä¸ªç›®å½•ä¸­å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼Œåˆ©ç”¨ [GenerationConfig.save_pretrained()](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.save_pretrained) ä¸­çš„ `config_file_name` å‚æ•°ã€‚ç¨åä½ å¯ä»¥ä½¿ç”¨ [GenerationConfig.from_pretrained()](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.from_pretrained) å®ä¾‹åŒ–å®ƒä»¬ã€‚å¦‚æœä½ æƒ³è¦ä¸ºå•ä¸ªæ¨¡å‹å­˜å‚¨å¤šä¸ªç”Ÿæˆé…ç½®ï¼ˆä¾‹å¦‚ï¼Œä¸€ä¸ªç”¨äºåˆ›æ„æ–‡æœ¬ç”Ÿæˆçš„é‡‡æ ·ï¼Œä¸€ä¸ªç”¨äºä½¿ç”¨å…‰æŸæœç´¢çš„æ‘˜è¦ï¼‰ï¼Œè¿™å°†éå¸¸æœ‰ç”¨ã€‚ä½ å¿…é¡»æ‹¥æœ‰åœ¨æ¨¡å‹ä¸Šæ·»åŠ é…ç½®æ–‡ä»¶çš„æ­£ç¡® Hub æƒé™ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdbbade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-small\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n",
    "\n",
    "translation_generation_config = GenerationConfig(\n",
    "    num_beams=4,\n",
    "    early_stopping=True,\n",
    "    decoder_start_token_id=0,\n",
    "    eos_token_id=model.config.eos_token_id,\n",
    "    pad_token=model.config.pad_token_id,\n",
    ")\n",
    "\n",
    "# æç¤ºï¼šæ·»åŠ  `push_to_hub=True` ä»¥æ¨é€åˆ° Hub\n",
    "translation_generation_config.save_pretrained(\"/tmp\", \"translation_generation_config.json\")\n",
    "\n",
    "# ä½ å¯ä»¥ä½¿ç”¨å‘½åç”Ÿæˆé…ç½®æ–‡ä»¶æ¥å‚æ•°åŒ–ç”Ÿæˆ\n",
    "generation_config = GenerationConfig.from_pretrained(\"/tmp\", \"translation_generation_config.json\")\n",
    "inputs = tokenizer(\"translate English to French: Configuration files are easy to use!\", return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, generation_config=generation_config)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480a8b3",
   "metadata": {},
   "source": [
    "\n",
    "## æµå¼ä¼ è¾“\n",
    "\n",
    "`generate()` æ”¯æŒæµå¼ä¼ è¾“ï¼Œé€šè¿‡å…¶ `streamer` è¾“å…¥ã€‚`streamer` è¾“å…¥ä¸ä»»ä½•å…·æœ‰ä»¥ä¸‹æ–¹æ³•çš„ç±»å®ä¾‹å…¼å®¹ï¼š`put()` å’Œ `end()`ã€‚åœ¨å†…éƒ¨ï¼Œ`put()` ç”¨äºæ¨é€æ–°æ ‡è®°ï¼Œ`end()` ç”¨äºæ ‡è®°æ–‡æœ¬ç”Ÿæˆçš„ç»“æŸã€‚\n",
    "\n",
    "æµå¼ä¼ è¾“ç±»çš„ API ä»åœ¨å¼€å‘ä¸­ï¼Œå°†æ¥å¯èƒ½ä¼šå‘ç”Ÿå˜åŒ–ã€‚\n",
    "\n",
    "å®é™…ä¸Šï¼Œä½ å¯ä»¥ä¸ºæ‰€æœ‰ç›®çš„åˆ¶ä½œè‡ªå·±çš„æµå¼ä¼ è¾“ç±»ï¼æˆ‘ä»¬ä¹Ÿæœ‰åŸºæœ¬æµå¼ä¼ è¾“ç±»å¯ä¾›ä½ ä½¿ç”¨ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨ [TextStreamer](/docs/transformers/main/en/internal/generation_utils#transformers.TextStreamer) ç±»å°† `generate()` çš„è¾“å‡ºæµå¼ä¼ è¾“åˆ°ä½ çš„å±å¹•ï¼Œä¸€æ¬¡ä¸€ä¸ªå•è¯ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117b11fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TextStreamer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n",
    "streamer = TextStreamer(tok)\n",
    "\n",
    "# å°½ç®¡è¿”å›é€šå¸¸çš„è¾“å‡ºï¼Œä½†æµå¼ä¼ è¾“å™¨è¿˜ä¼šå°†ç”Ÿæˆçš„æ–‡æœ¬æ‰“å°åˆ° stdoutã€‚\n",
    "_ = model.generate(**inputs, streamer=streamer, max_new_tokens=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfbb1eb",
   "metadata": {},
   "source": [
    "\n",
    "## æ°´å°\n",
    "\n",
    "`generate()` æ”¯æŒé€šè¿‡éšæœºæ ‡è®°ä¸€éƒ¨åˆ†æ ‡è®°ä¸ºâ€œç»¿è‰²â€æ¥å¯¹ç”Ÿæˆçš„æ–‡æœ¬è¿›è¡Œæ°´å°å¤„ç†ã€‚åœ¨ç”Ÿæˆæ—¶ï¼Œâ€œç»¿è‰²â€æ ‡è®°çš„ logits å°†æœ‰ä¸€ä¸ªå°çš„â€œåå·®â€å€¼æ·»åŠ ï¼Œä»è€Œæœ‰æ›´é«˜çš„ç”Ÿæˆæ¦‚ç‡ã€‚å¯ä»¥é€šè¿‡è®¡ç®—æ–‡æœ¬ä¸­â€œç»¿è‰²â€æ ‡è®°çš„æ¯”ä¾‹å¹¶ä¼°è®¡äººç±»ç”Ÿæˆçš„æ–‡æœ¬ä¸­è·å¾—è¯¥æ•°é‡â€œç»¿è‰²â€æ ‡è®°çš„ç»Ÿè®¡å¯èƒ½æ€§æ¥æ£€æµ‹æ°´å°æ–‡æœ¬ã€‚è¿™ç§æ°´å°ç­–ç•¥åœ¨è®ºæ–‡ [â€œOn the Reliability of Watermarks for Large Language Modelsâ€](https://arxiv.org/abs/2306.04634) ä¸­æå‡ºã€‚æœ‰å…³æ°´å°å†…éƒ¨å·¥ä½œåŸç†çš„æ›´å¤šä¿¡æ¯ï¼Œå»ºè®®å‚è€ƒè¯¥è®ºæ–‡ã€‚\n",
    "\n",
    "æ°´å°å¯ç”¨äº `transformers` ä¸­çš„ä»»ä½•ç”Ÿæˆæ¨¡å‹ï¼Œå¹¶ä¸”ä¸éœ€è¦é¢å¤–çš„åˆ†ç±»æ¨¡å‹æ¥æ£€æµ‹æ°´å°æ–‡æœ¬ã€‚è¦è§¦å‘æ°´å°ï¼Œå¯ä»¥ç›´æ¥å°†å¸¦æœ‰å¿…è¦å‚æ•°çš„ [WatermarkingConfig](/docs/transformers/main/en/main_classes/text_generation#transformers.WatermarkingConfig) ä¼ é€’ç»™ `.generate()` æ–¹æ³•ï¼Œæˆ–è€…å°†å…¶æ·»åŠ åˆ° [GenerationConfig](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig)ã€‚ç¨åå¯ä»¥ä½¿ç”¨ [WatermarkDetector](/docs/transformers/main/en/internal/generation_utils#transformers.WatermarkDetector) æ£€æµ‹æ°´å°æ–‡æœ¬ã€‚\n",
    "\n",
    "WatermarkDetector åœ¨å†…éƒ¨ä¾èµ–äºâ€œç»¿è‰²â€æ ‡è®°çš„æ¯”ä¾‹ï¼Œä»¥åŠç”Ÿæˆçš„æ–‡æœ¬æ˜¯å¦éµå¾ªç€è‰²æ¨¡å¼ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå»ºè®®åœ¨æç¤ºæ–‡æœ¬æ¯”ç”Ÿæˆæ–‡æœ¬é•¿å¾—å¤šæ—¶å»æ‰æç¤ºæ–‡æœ¬ã€‚å½“ä¸€æ‰¹ä¸­çš„æŸä¸ªåºåˆ—æ¯”å…¶ä»–è¡Œé•¿å¾—å¤šå¯¼è‡´å…¶ä»–è¡Œè¢«å¡«å……æ—¶ï¼Œè¿™ä¹Ÿå¯èƒ½äº§ç”Ÿå½±å“ã€‚æ­¤å¤–ï¼Œæ£€æµ‹å™¨**å¿…é¡»**ä½¿ç”¨ä¸ç”Ÿæˆæ—¶ç›¸åŒçš„ watermark é…ç½®å‚æ•°è¿›è¡Œåˆå§‹åŒ–ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬ç”Ÿæˆä¸€äº›å¸¦æ°´å°çš„æ–‡æœ¬ã€‚åœ¨ä¸‹é¢çš„ä»£ç ç‰‡æ®µä¸­ï¼Œæˆ‘ä»¬å°†åå·®è®¾ç½®ä¸º 2.5ï¼Œè¿™æ˜¯å°†æ·»åŠ åˆ°â€œç»¿è‰²â€æ ‡è®°çš„ logits çš„å€¼ã€‚ç”Ÿæˆæ°´å°æ–‡æœ¬åï¼Œæˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶ä¼ é€’ç»™ `WatermarkDetector` ä»¥æ£€æŸ¥æ–‡æœ¬æ˜¯å¦ç”±æœºå™¨ç”Ÿæˆï¼ˆå¯¹äºæœºå™¨ç”Ÿæˆçš„è¾“å‡º `True`ï¼Œå¦åˆ™è¾“å‡º `False`ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c32b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, WatermarkDetector, WatermarkingConfig\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "tok.pad_token_id = tok.eos_token_id\n",
    "tok.padding_side = \"left\"\n",
    "\n",
    "inputs = tok([\"This is the beginning of a long story\", \"Alice and Bob are\"], padding=True, return_tensors=\"pt\")\n",
    "input_len = inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "watermarking_config = WatermarkingConfig(bias=2.5, seeding_scheme=\"selfhash\")\n",
    "out = model.generate(**inputs, watermarking_config=watermarking_config, do_sample=False, max_length=20)\n",
    "\n",
    "detector = WatermarkDetector(model_config=model.config, device=\"cpu\", watermarking_config=watermarking_config)\n",
    "detection_out = detector(out, return_dict=True)\n",
    "detection_out.prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3c5e3d",
   "metadata": {},
   "source": [
    "\n",
    "## è§£ç ç­–ç•¥\n",
    "\n",
    "`generate()` å‚æ•°çš„æŸäº›ç»„åˆï¼Œä»¥åŠæœ€ç»ˆçš„ `generation_config`ï¼Œå¯ç”¨äºå¯ç”¨ç‰¹å®šçš„è§£ç ç­–ç•¥ã€‚å¦‚æœä½ æ˜¯è¿™æ–¹é¢çš„æ–°æ‰‹ï¼Œæˆ‘ä»¬å»ºè®®é˜…è¯» [è¿™ç¯‡åšå®¢æ–‡ç« ï¼Œå…¶ä¸­è¯´æ˜äº†å¸¸è§è§£ç ç­–ç•¥çš„å·¥ä½œåŸç†](https://huggingface.co/blog/how-to-generate)ã€‚\n",
    "\n",
    "åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å±•ç¤ºæ§åˆ¶è§£ç ç­–ç•¥çš„å‚æ•°ï¼Œå¹¶è¯´æ˜å¦‚ä½•ä½¿ç”¨å®ƒä»¬ã€‚\n",
    "\n",
    "é€‰æ‹©ç»™å®šçš„è§£ç ç­–ç•¥ä¸æ˜¯ä½ å½±å“ `generate()` ç»“æœçš„å”¯ä¸€æ–¹å¼ã€‚è§£ç ç­–ç•¥ä¸»è¦åŸºäº logitsï¼Œå³ä¸‹ä¸€ä¸ªæ ‡è®°çš„æ¦‚ç‡åˆ†å¸ƒï¼Œå› æ­¤é€‰æ‹©ä¸€ä¸ªå¥½çš„ logits æ“ä½œç­–ç•¥å¯ä»¥å¤§æœ‰å¸®åŠ©ï¼æ¢å¥è¯è¯´ï¼Œé™¤äº†é€‰æ‹©è§£ç ç­–ç•¥ä¹‹å¤–ï¼Œæ“çºµ logits æ˜¯ä½ å¯ä»¥é‡‡å–çš„å¦ä¸€ä¸ªç»´åº¦ã€‚æµè¡Œçš„ logits æ“ä½œç­–ç•¥åŒ…æ‹¬ `top_p`ã€`min_p` å’Œ `repetition_penalty` â€” ä½ å¯ä»¥åœ¨ [GenerationConfig](/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig) ç±»ä¸­æŸ¥çœ‹å®Œæ•´åˆ—è¡¨ã€‚\n",
    "\n",
    "### è´ªå©ªæœç´¢\n",
    "\n",
    "`generate` é»˜è®¤ä½¿ç”¨è´ªå©ªæœç´¢è§£ç ï¼Œå› æ­¤ä½ ä¸éœ€è¦ä¼ é€’ä»»ä½•å‚æ•°æ¥å¯ç”¨å®ƒã€‚è¿™æ„å‘³ç€å‚æ•° `num_beams` è®¾ç½®ä¸º 1 å¹¶ä¸” `do_sample=False`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcbd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "prompt = \"I look forward to\"\n",
    "checkpoint = \"distilbert/distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "outputs = model.generate(**inputs)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2c88cf",
   "metadata": {},
   "source": [
    "\n",
    "### å¯¹æ¯”æœç´¢\n",
    "\n",
    "å¯¹æ¯”æœç´¢è§£ç ç­–ç•¥åœ¨ 2022 å¹´çš„è®ºæ–‡ [A Contrastive Framework for Neural Text Generation](https://arxiv.org/abs/2202.06417) ä¸­æå‡ºã€‚å®ƒå±•ç¤ºäº†åœ¨ç”Ÿæˆéé‡å¤ä¸”è¿è´¯çš„é•¿è¾“å‡ºæ–¹é¢çš„ä¼˜è¶Šç»“æœã€‚è¦äº†è§£å¯¹æ¯”æœç´¢å¦‚ä½•å·¥ä½œï¼Œè¯·æŸ¥çœ‹ [è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/introducing-csearch)ã€‚å¯ç”¨å’Œæ§åˆ¶å¯¹æ¯”æœç´¢è¡Œä¸ºçš„ä¸»è¦å‚æ•°æ˜¯ `penalty_alpha` å’Œ `top_k`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfa66f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "checkpoint = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "prompt = \"Hugging Face Company is\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, penalty_alpha=0.6, top_k=4, max_new_tokens=100)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d859542",
   "metadata": {},
   "source": [
    "\n",
    "### å¤šé¡¹å¼é‡‡æ ·\n",
    "\n",
    "ä¸æ€»æ˜¯é€‰æ‹©å…·æœ‰æœ€é«˜æ¦‚ç‡çš„æ ‡è®°ä½œä¸ºä¸‹ä¸€ä¸ªæ ‡è®°çš„è´ªå©ªæœç´¢ä¸åŒï¼Œå¤šé¡¹å¼é‡‡æ ·ï¼ˆä¹Ÿç§°ä¸ºç¥–å…ˆé‡‡æ ·ï¼‰æ ¹æ®æ¨¡å‹ç»™å‡ºçš„æ•´ä¸ªè¯æ±‡çš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªæ ‡è®°ã€‚æ¯ä¸ªå…·æœ‰éé›¶æ¦‚ç‡çš„æ ‡è®°éƒ½æœ‰è¢«é€‰ä¸­çš„æœºä¼šï¼Œä»è€Œå‡å°‘äº†é‡å¤çš„é£é™©ã€‚\n",
    "\n",
    "è¦å¯ç”¨å¤šé¡¹å¼é‡‡æ ·ï¼Œè¯·è®¾ç½® `do_sample=True` å’Œ `num_beams=1`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfb7dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "set_seed(0)  # ä¸ºäº†å¯é‡å¤æ€§\n",
    "\n",
    "checkpoint = \"openai-community/gpt2-large\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "prompt = \"Today was an amazing day because\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(**inputs, do_sample=True, num_beams=1, max_new_tokens=100)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007faca9",
   "metadata": {},
   "source": [
    "\n",
    "### å…‰æŸæœç´¢è§£ç \n",
    "\n",
    "ä¸è´ªå©ªæœç´¢ä¸åŒï¼Œå…‰æŸæœç´¢è§£ç åœ¨æ¯ä¸ªæ—¶é—´æ­¥ä¿æŒå¤šä¸ªå‡è®¾ï¼Œå¹¶æœ€ç»ˆé€‰æ‹©æ•´ä¸ªåºåˆ—å…·æœ‰æœ€é«˜æ¦‚ç‡çš„å‡è®¾ã€‚è¿™æœ‰åŠ©äºè¯†åˆ«ä»¥è¾ƒä½æ¦‚ç‡åˆå§‹æ ‡è®°å¼€å§‹çš„å…·æœ‰è¾ƒé«˜æ¦‚ç‡çš„åºåˆ—ï¼Œè¿™äº›åºåˆ—ä¼šè¢«è´ªå©ªæœç´¢å¿½ç•¥ã€‚\n",
    "\n",
    "[![å…‰æŸæœç´¢è§£ç ç¤ºæ„å›¾](./resources/images/beam_search.png)](https://huggingface.co/spaces/m-ric/beam_search_visualizer)\n",
    "\n",
    "ä½ å¯ä»¥åœ¨ [è¿™ä¸ªäº¤äº’å¼æ¼”ç¤º](https://huggingface.co/spaces/m-ric/beam_search_visualizer) ä¸­äº†è§£å…‰æŸæœç´¢è§£ç å¦‚ä½•å·¥ä½œï¼šè¾“å…¥ä½ çš„å¥å­ï¼Œå¹¶è°ƒæ•´å‚æ•°ä»¥æŸ¥çœ‹è§£ç å…‰æŸå¦‚ä½•å˜åŒ–ã€‚\n",
    "\n",
    "è¦å¯ç”¨æ­¤è§£ç ç­–ç•¥ï¼Œè¯·æŒ‡å®šå¤§äº 1 çš„ `num_beams`ï¼ˆå³è¦è·Ÿè¸ªçš„å‡è®¾æ•°ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030ab1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "prompt = \"It is astonishing how one can\"\n",
    "checkpoint = \"openai-community/gpt2-medium\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model.generate(**inputs, num_beams=5, max_new_tokens=50)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1abf7c2",
   "metadata": {},
   "source": [
    "\n",
    "### å…‰æŸæœç´¢å¤šé¡¹å¼é‡‡æ ·\n",
    "\n",
    "é¡¾åæ€ä¹‰ï¼Œè¿™ç§è§£ç ç­–ç•¥ç»“åˆäº†å…‰æŸæœç´¢å’Œå¤šé¡¹å¼é‡‡æ ·ã€‚ä½ éœ€è¦æŒ‡å®šå¤§äº 1 çš„ `num_beams` å¹¶è®¾ç½® `do_sample=True` ä»¥ä½¿ç”¨æ­¤è§£ç ç­–ç•¥ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a747228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, set_seed\n",
    "set_seed(0)  # ä¸ºäº†å¯é‡å¤æ€§\n",
    "\n",
    "prompt = \"translate English to German: The house is wonderful.\"\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model.generate(**inputs, num_beams=5, do_sample=True)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4392f4",
   "metadata": {},
   "source": [
    "\n",
    "### å¤šæ ·åŒ–å…‰æŸæœç´¢è§£ç \n",
    "\n",
    "å¤šæ ·åŒ–å…‰æŸæœç´¢è§£ç ç­–ç•¥æ˜¯å…‰æŸæœç´¢ç­–ç•¥çš„æ‰©å±•ï¼Œå®ƒå…è®¸ç”Ÿæˆæ›´å¤šæ ·åŒ–çš„å…‰æŸåºåˆ—é›†ä»¥ä¾›é€‰æ‹©ã€‚è¦äº†è§£å…¶å·¥ä½œåŸç†ï¼Œè¯·å‚é˜… [Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models](https://arxiv.org/pdf/1610.02424.pdf)ã€‚è¿™ç§æ–¹æ³•æœ‰ä¸‰ä¸ªä¸»è¦å‚æ•°ï¼š`num_beams`ã€`num_beam_groups` å’Œ `diversity_penalty`ã€‚å¤šæ ·æ€§æƒ©ç½šç¡®ä¿è¾“å‡ºåœ¨ç»„ä¹‹é—´æ˜¯ä¸åŒçš„ï¼Œå¹¶ä¸”åœ¨æ¯ä¸ªç»„å†…ä½¿ç”¨å…‰æŸæœç´¢ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cff8983",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint = \"google/pegasus-xsum\"\n",
    "prompt = (\n",
    "    \"The Permaculture Design Principles are a set of universal design principles \"\n",
    "    \"that can be applied to any location, climate and culture, and they allow us to design \"\n",
    "    \"the most efficient and sustainable human habitation and food production systems. \"\n",
    "    \"Permaculture is a design system that encompasses a wide variety of disciplines, such \"\n",
    "    \"as ecology, landscape design, environmental science and energy conservation, and the \"\n",
    "    \"Permaculture design principles are drawn from these various disciplines. Each individual \"\n",
    "    \"design principle itself embodies a complete conceptual framework based on sound \"\n",
    "    \"scientific principles. When we bring all these separate  principles together, we can \"\n",
    "    \"create a design system that both looks at whole systems, the parts that these systems \"\n",
    "    \"consist of, and how those parts interact with each other to create a complex, dynamic, \"\n",
    "    \"living system. Each design principle serves as a tool that allows us to integrate all \"\n",
    "    \"the separate parts of a design, referred to as elements, into a functional, synergistic, \"\n",
    "    \"whole system, where the elements harmoniously interact and work together in the most \"\n",
    "    \"efficient way possible.\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "\n",
    "outputs = model.generate(**inputs, num_beams=5, num_beam_groups=5, max_new_tokens=30, diversity_penalty=1.0)\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdecffe3",
   "metadata": {},
   "source": [
    "\n",
    "æœ¬æŒ‡å—è¯´æ˜äº†å¯ç”¨å„ç§è§£ç ç­–ç•¥çš„ä¸»è¦å‚æ•°ã€‚`generate` æ–¹æ³•è¿˜æœ‰æ›´é«˜çº§çš„å‚æ•°ï¼Œå¯ä»¥è®©ä½ å¯¹ `generate` æ–¹æ³•çš„è¡Œä¸ºæœ‰æ›´è¿›ä¸€æ­¥çš„æŒæ§ã€‚æœ‰å…³å¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ï¼Œè¯·å‚é˜… [API æ–‡æ¡£](./main_classes/text_generation.md)ã€‚\n",
    "\n",
    "### æŠ•æœºè§£ç \n",
    "\n",
    "æŠ•æœºè§£ç ï¼ˆä¹Ÿç§°ä¸ºè¾…åŠ©è§£ç ï¼‰æ˜¯å¯¹ä¸Šè¿°è§£ç ç­–ç•¥çš„ä¿®æ”¹ï¼Œå®ƒä½¿ç”¨è¾…åŠ©æ¨¡å‹ï¼ˆç†æƒ³æƒ…å†µä¸‹æ˜¯ä¸€ä¸ªæ›´å°çš„æ¨¡å‹ï¼‰æ¥ç”Ÿæˆå‡ ä¸ªå€™é€‰æ ‡è®°ã€‚ä¸»æ¨¡å‹ç„¶ååœ¨å•ä¸ªå‰å‘ä¼ é€’ä¸­éªŒè¯å€™é€‰æ ‡è®°ï¼Œä»è€ŒåŠ å¿«è§£ç è¿‡ç¨‹ã€‚å¦‚æœ `do_sample=True`ï¼Œåˆ™ä½¿ç”¨ [æŠ•æœºè§£ç è®ºæ–‡](https://arxiv.org/pdf/2211.17192.pdf) ä¸­å¼•å…¥çš„å¸¦æœ‰é‡æ–°é‡‡æ ·çš„æ ‡è®°éªŒè¯ã€‚è¾…åŠ©è§£ç å‡è®¾ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ç›¸åŒçš„åˆ†è¯å™¨ï¼Œå¦åˆ™ï¼Œè¯·å‚é˜…é€šç”¨è¾…åŠ©è§£ç ã€‚\n",
    "\n",
    "ç›®å‰ï¼Œåªæœ‰è´ªå©ªæœç´¢å’Œé‡‡æ ·æ”¯æŒè¾…åŠ©è§£ç ï¼Œå¹¶ä¸”è¾…åŠ©è§£ç ä¸æ”¯æŒæ‰¹é‡è¾“å…¥ã€‚è¦äº†è§£æ›´å¤šå…³äºè¾…åŠ©è§£ç çš„ä¿¡æ¯ï¼Œè¯·æŸ¥çœ‹ [è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/assisted-generation)ã€‚\n",
    "\n",
    "#### é€šç”¨è¾…åŠ©è§£ç \n",
    "\n",
    "é€šç”¨è¾…åŠ©è§£ç ï¼ˆUADï¼‰æ·»åŠ äº†å¯¹ä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ä¸åŒåˆ†è¯å™¨çš„æ”¯æŒã€‚è¦ä½¿ç”¨å®ƒï¼Œåªéœ€ä½¿ç”¨ `tokenizer` å’Œ `assistant_tokenizer` å‚æ•°ä¼ é€’åˆ†è¯å™¨ï¼ˆè§ä¸‹æ–‡ï¼‰ã€‚åœ¨å†…éƒ¨ï¼Œä¸»æ¨¡å‹è¾“å…¥æ ‡è®°è¢«é‡æ–°ç¼–ç ä¸ºè¾…åŠ©æ¨¡å‹æ ‡è®°ï¼Œç„¶ååœ¨è¾…åŠ©ç¼–ç ä¸­ç”Ÿæˆå€™é€‰æ ‡è®°ï¼Œè¿™äº›å€™é€‰æ ‡è®°åˆè¢«é‡æ–°ç¼–ç ä¸ºä¸»æ¨¡å‹å€™é€‰æ ‡è®°ã€‚ç„¶åæŒ‰ç…§ä¸Šè¿°æ–¹å¼ç»§ç»­éªŒè¯ã€‚é‡æ–°ç¼–ç æ­¥éª¤æ¶‰åŠå°†æ ‡è®° id è§£ç ä¸ºæ–‡æœ¬ï¼Œç„¶åä½¿ç”¨ä¸åŒçš„åˆ†è¯å™¨å¯¹æ–‡æœ¬è¿›è¡Œç¼–ç ã€‚ç”±äºé‡æ–°ç¼–ç æ ‡è®°å¯èƒ½å¯¼è‡´åˆ†è¯å·®å¼‚ï¼ŒUAD æ‰¾åˆ°æºç¼–ç å’Œç›®æ ‡ç¼–ç ä¹‹é—´çš„æœ€é•¿å…¬å…±å­åºåˆ—ï¼Œä»¥ç¡®ä¿æ–°æ ‡è®°åŒ…å«æ­£ç¡®çš„æç¤ºåç¼€ã€‚\n",
    "\n",
    "è¦å¯ç”¨è¾…åŠ©è§£ç ï¼Œè¯·è®¾ç½® `assistant_model` å‚æ•°ä¸ºæ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbaceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7aa7a0",
   "metadata": {},
   "source": [
    "\n",
    "å¦‚æœä¸»æ¨¡å‹å’Œè¾…åŠ©æ¨¡å‹å…·æœ‰ä¸åŒçš„åˆ†è¯å™¨ï¼Œè¯·ä½¿ç”¨é€šç”¨è¾…åŠ©è§£ç ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"google/gemma-2-9b\"\n",
    "assistant_checkpoint = \"double7/vicuna-68m\"\n",
    "\n",
    "assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97cdb02b",
   "metadata": {},
   "source": [
    "\n",
    "å½“ä½¿ç”¨è¾…åŠ©è§£ç ä¸é‡‡æ ·æ–¹æ³•æ—¶ï¼Œä½ å¯ä»¥ä½¿ç”¨ `temperature` å‚æ•°æ¥æ§åˆ¶éšæœºæ€§ï¼Œå°±åƒåœ¨å¤šé¡¹å¼é‡‡æ ·ä¸­ä¸€æ ·ã€‚ç„¶è€Œï¼Œåœ¨è¾…åŠ©è§£ç ä¸­ï¼Œé™ä½æ¸©åº¦å¯èƒ½æœ‰åŠ©äºæé«˜å»¶è¿Ÿã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035bc74e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, set_seed\n",
    "set_seed(42)  # ä¸ºäº†å¯é‡å¤æ€§\n",
    "\n",
    "prompt = \"Alice and Bob\"\n",
    "checkpoint = \"EleutherAI/pythia-1.4b-deduped\"\n",
    "assistant_checkpoint = \"EleutherAI/pythia-160m-deduped\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model, do_sample=True, temperature=0.5)\n",
    "tokenizer.batch_decode(outputs, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880a2e5a",
   "metadata": {},
   "source": [
    "\n",
    "æˆ–è€…ï¼Œä½ ä¹Ÿå¯ä»¥è®¾ç½® `prompt_lookup_num_tokens` ä»¥è§¦å‘åŸºäº n-gram çš„è¾…åŠ©è§£ç ï¼Œè€Œä¸æ˜¯åŸºäºæ¨¡å‹çš„è¾…åŠ©è§£ç ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œé˜…è¯»æ›´å¤šå…³äºå®ƒçš„ä¿¡æ¯ [è¿™é‡Œ](https://twitter.com/joao_gante/status/1747322413006643259)ã€‚\n",
    "\n",
    "### DoLa è§£ç \n",
    "\n",
    "**D**ecoding by C**o**ntrasting **La**yers (DoLa) æ˜¯ä¸€ç§å¯¹æ¯”è§£ç ç­–ç•¥ï¼Œç”¨äºæé«˜ LLM çš„äº‹å®æ€§å’Œå‡å°‘å¹»è§‰ï¼Œå¦‚ ICLR 2024 å¹´çš„è®ºæ–‡ [DoLa: Decoding by Contrasting Layers Improves Factuality in Large Language Models](https://arxiv.org/abs/2309.03883) æ‰€è¿°ã€‚\n",
    "\n",
    "DoLa é€šè¿‡å¯¹æ¯”ä»æœ€ç»ˆå±‚ä¸æ—©æœŸå±‚è·å¾—çš„ logits ä¹‹é—´çš„å·®å¼‚æ¥å®ç°ï¼Œä»è€Œæ”¾å¤§ç‰¹å®šäºå˜å‹å™¨å±‚çš„äº‹å®çŸ¥è¯†ã€‚\n",
    "\n",
    "è¦åœ¨è°ƒç”¨ `model.generate` å‡½æ•°æ—¶æ¿€æ´» DoLa è§£ç ï¼Œè¯·æ‰§è¡Œä»¥ä¸‹ä¸¤ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "1. è®¾ç½® `dola_layers` å‚æ•°ï¼Œå®ƒå¯ä»¥æ˜¯å­—ç¬¦ä¸²æˆ–æ•´æ•°åˆ—è¡¨ã€‚\n",
    "   - å¦‚æœè®¾ç½®ä¸ºå­—ç¬¦ä¸²ï¼Œåˆ™å¯ä»¥æ˜¯ `low` æˆ– `high` ä¹‹ä¸€ã€‚\n",
    "   - å¦‚æœè®¾ç½®ä¸ºæ•´æ•°åˆ—è¡¨ï¼Œåˆ™åº”è¯¥æ˜¯ä»‹äº 0 åˆ°æ¨¡å‹æ€»å±‚æ•°ä¹‹é—´çš„å±‚ç´¢å¼•åˆ—è¡¨ã€‚ç¬¬ 0 å±‚æ˜¯è¯åµŒå…¥ï¼Œç¬¬ 1 å±‚æ˜¯ç¬¬ä¸€ä¸ªå˜å‹å™¨å±‚ï¼Œä¾æ­¤ç±»æ¨ã€‚\n",
    "2. å»ºè®®è®¾ç½® `repetition_penalty = 1.2` ä»¥å‡å°‘ DoLa è§£ç ä¸­çš„é‡å¤ã€‚\n",
    "\n",
    "ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†ä½¿ç”¨ 32 å±‚ LLaMA-7B æ¨¡å‹çš„ DoLa è§£ç ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208e3653",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"huggyllama/llama-7b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"huggyllama/llama-7b\", torch_dtype=torch.float16)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "set_seed(42)\n",
    "\n",
    "text = \"On what date was the Declaration of Independence officially signed?\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
    "\n",
    "# Vanilla greddy decoding\n",
    "vanilla_output = model.generate(**inputs, do_sample=False, max_new_tokens=50)\n",
    "tokenizer.batch_decode(vanilla_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "# DoLa decoding with contrasting higher part of layers (layers 16,18,...,30)\n",
    "dola_high_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers='high')\n",
    "tokenizer.batch_decode(dola_high_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
    "\n",
    "# DoLa decoding with contrasting specific layers (layers 28 and 30)\n",
    "dola_custom_output = model.generate(**inputs, do_sample=False, max_new_tokens=50, dola_layers=[28,30], repetition_penalty=1.2)\n",
    "tokenizer.batch_decode(dola_custom_output[:, inputs.input_ids.shape[-1]:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18bd4f89",
   "metadata": {},
   "source": [
    "\n",
    "#### ç†è§£ `dola_layers` å‚æ•°\n",
    "\n",
    "`dola_layers` ä»£è¡¨æ—©æœŸå±‚é€‰æ‹©ä¸­çš„å€™é€‰å±‚ï¼Œå¦‚ DoLa è®ºæ–‡ä¸­æ‰€è¿°ã€‚é€‰å®šçš„æ—©æœŸå±‚å°†ä¸æœ€ç»ˆå±‚è¿›è¡Œå¯¹æ¯”ã€‚\n",
    "\n",
    "å°† `dola_layers` è®¾ç½®ä¸º `'low'` æˆ– `'high'` å°†åˆ†åˆ«é€‰æ‹©è¾ƒä½æˆ–è¾ƒé«˜çš„å±‚è¿›è¡Œå¯¹æ¯”ã€‚\n",
    "\n",
    "- å¯¹äº `N` å±‚æ¨¡å‹ï¼Œå…¶ä¸­ `N <= 40` å±‚ï¼Œ`range(0, N // 2, 2)` å’Œ `range(N // 2, N, 2)` çš„å±‚åˆ†åˆ«ç”¨äº `'low'` å’Œ `'high'` å±‚ã€‚\n",
    "- å¯¹äº `N > 40` å±‚çš„æ¨¡å‹ï¼Œ`range(0, 20, 2)` å’Œ `range(N - 20, N, 2)` çš„å±‚åˆ†åˆ«ç”¨äº `'low'` å’Œ `'high'` å±‚ã€‚\n",
    "- å¦‚æœæ¨¡å‹æœ‰ç»‘å®šçš„è¯åµŒå…¥ï¼Œæˆ‘ä»¬å°†è·³è¿‡è¯åµŒå…¥ï¼ˆç¬¬ 0 å±‚ï¼‰å¹¶ä»ç¬¬ 2 å±‚å¼€å§‹ï¼Œå› ä¸ºä»è¯åµŒå…¥ä¸­çš„æ—©æœŸé€€å‡ºå°†æˆä¸ºæ’ç­‰å‡½æ•°ã€‚\n",
    "- å°† `dola_layers` è®¾ç½®ä¸ºæ•´æ•°åˆ—è¡¨ï¼Œç”¨äºå±‚ç´¢å¼•ï¼Œä»¥æ‰‹åŠ¨æŒ‡å®šè¦å¯¹æ¯”çš„å±‚ã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `dola_layers=[28,30]` å°†å¯¹æ¯”æœ€ç»ˆå±‚ï¼ˆç¬¬ 32 å±‚ï¼‰ä¸ç¬¬ 28 å±‚å’Œç¬¬ 30 å±‚ã€‚\n",
    "\n",
    "è®ºæ–‡å»ºè®®ï¼Œå¯¹äºåƒ TruthfulQA è¿™æ ·çš„ç®€ç­”é¢˜ä»»åŠ¡ï¼Œå¯¹æ¯” `'high'` å±‚ï¼›è€Œå¯¹äºæ‰€æœ‰å…¶ä»–é•¿ç­”æ¡ˆæ¨ç†ä»»åŠ¡ï¼Œä¾‹å¦‚ GSM8Kã€StrategyQAã€FACTOR å’Œ VicunaQAï¼Œå¯¹æ¯” `'low'` å±‚ã€‚ä¸å»ºè®®å°† DoLa åº”ç”¨äºåƒ GPT-2 è¿™æ ·çš„å°æ¨¡å‹ï¼Œå› ä¸ºè®ºæ–‡é™„å½• N ä¸­çš„ç»“æœæ˜¾ç¤ºæ•ˆæœä¸ä½³ã€‚\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
