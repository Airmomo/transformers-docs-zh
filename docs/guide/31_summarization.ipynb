{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84d41fe2",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬æ‘˜è¦\n",
    "\n",
    "æ–‡æœ¬æ‘˜è¦æ˜¯å°†æ–‡æ¡£æˆ–æ–‡ç« çš„é‡è¦ä¿¡æ¯å‹ç¼©æˆæ›´çŸ­ç‰ˆæœ¬çš„ä»»åŠ¡ã€‚ä¸ç¿»è¯‘ä¸€æ ·ï¼Œå®ƒä¹Ÿæ˜¯å¯ä»¥è¡¨è¿°ä¸ºåºåˆ—åˆ°åºåˆ—ä»»åŠ¡çš„å¦ä¸€ä¸ªä¾‹å­ã€‚æ‘˜è¦å¯ä»¥æ˜¯ï¼š\n",
    "\n",
    "- **æå–å¼**ï¼šä»æ–‡æ¡£ä¸­æå–æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚\n",
    "- **æŠ½è±¡å¼**ï¼šç”Ÿæˆæ–°çš„æ–‡æœ¬ï¼Œæ•æ‰æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—å°†å‘æ‚¨å±•ç¤ºå¦‚ä½•ï¼š\n",
    "\n",
    "1. åœ¨ BillSum æ•°æ®é›†çš„åŠ åˆ©ç¦å°¼äºšå·æ³•æ¡ˆå­é›†ä¸Šå¾®è°ƒ T5 æ¨¡å‹ï¼Œç”¨äºæŠ½è±¡å¼æ‘˜è¦ã€‚\n",
    "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "è¦æŸ¥çœ‹æ‰€æœ‰ä¸è¯¥ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬å»ºè®®æ‚¨æŸ¥çœ‹ä»»åŠ¡é¡µé¢ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b802b842",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb34eac",
   "metadata": {},
   "source": [
    "\n",
    "æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ï¼Œè¿™æ ·æ‚¨å°±å¯ä»¥ä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa09fb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6c75728",
   "metadata": {},
   "source": [
    "\n",
    "## åŠ è½½ BillSum æ•°æ®é›†\n",
    "\n",
    "é¦–å…ˆï¼Œä» ğŸ¤— Datasets åº“ä¸­åŠ è½½ BillSum æ•°æ®é›†çš„è¾ƒå°çš„åŠ åˆ©ç¦å°¼äºšå·æ³•æ¡ˆå­é›†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd35520c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4ce294",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ `train_test_split` æ–¹æ³•å°†æ•°æ®é›†åˆ†æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfc75c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum = billsum.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc4f54d",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df50d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "billsum[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a7811",
   "metadata": {},
   "source": [
    "\n",
    "æ‚¨éœ€è¦ä½¿ç”¨ä¸¤ä¸ªå­—æ®µï¼š\n",
    "\n",
    "- `text`ï¼šæ³•æ¡ˆçš„æ–‡æœ¬ï¼Œå°†ä½œä¸ºæ¨¡å‹çš„è¾“å…¥ã€‚\n",
    "- `summary`ï¼š`text` çš„æµ“ç¼©ç‰ˆæœ¬ï¼Œå°†ä½œä¸ºæ¨¡å‹çš„ç›®æ ‡ã€‚\n",
    "\n",
    "## é¢„å¤„ç†\n",
    "\n",
    "ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ T5 åˆ†è¯å™¨æ¥å¤„ç† `text` å’Œ `summary`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c494385",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff5a999",
   "metadata": {},
   "source": [
    "\n",
    "æ‚¨éœ€è¦åˆ›å»ºçš„é¢„å¤„ç†å‡½æ•°éœ€è¦æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š\n",
    "\n",
    "1. åœ¨è¾“å…¥å‰åŠ ä¸Šæç¤ºï¼Œä»¥ä¾¿ T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚ä¸€äº›èƒ½å¤Ÿå¤„ç†å¤šç§ NLP ä»»åŠ¡çš„æ¨¡å‹éœ€è¦é’ˆå¯¹ç‰¹å®šä»»åŠ¡è¿›è¡Œæç¤ºã€‚\n",
    "2. åœ¨å¯¹æ ‡ç­¾è¿›è¡Œåˆ†è¯æ—¶ä½¿ç”¨ `text_target` å‚æ•°ã€‚\n",
    "3. æˆªæ–­åºåˆ—ï¼Œä½¿å…¶é•¿åº¦ä¸è¶…è¿‡ `max_length` å‚æ•°è®¾ç½®çš„æœ€å¤§é•¿åº¦ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c85aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"summarize: \"\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844392c1",
   "metadata": {},
   "source": [
    "\n",
    "è¦ä½¿ç”¨ ğŸ¤— Datasets çš„ `map` æ–¹æ³•åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ï¼Œè¯·è®¾ç½® `batched=True` ä»¥åŒæ—¶å¤„ç†æ•°æ®é›†ä¸­çš„å¤šä¸ªå…ƒç´ ï¼Œä»è€ŒåŠ å¿« `map` å‡½æ•°çš„é€Ÿåº¦ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ffd0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_billsum = billsum.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d11505",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ä½¿ç”¨ `DataCollatorForSeq2Seq` åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚åœ¨æ•´ç†è¿‡ç¨‹ä¸­ï¼ŒåŠ¨æ€åœ°å°†å¥å­å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€é•¿çš„é•¿åº¦ï¼Œè€Œä¸æ˜¯å°†æ•´ä¸ªæ•°æ®é›†å¡«å……åˆ°æœ€å¤§é•¿åº¦ï¼Œè¿™æ ·åšæ›´æœ‰æ•ˆç‡ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc239db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c002aa37",
   "metadata": {},
   "source": [
    "\n",
    "## è¯„ä¼°\n",
    "\n",
    "åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªæŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— Evaluate åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ ROUGE æŒ‡æ ‡ï¼ˆè¯·å‚é˜… ğŸ¤— Evaluate å¿«é€Ÿå…¥é—¨ï¼Œä»¥äº†è§£æ›´å¤šå…³äºå¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f670870a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "rouge = evaluate.load(\"rouge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a748f2",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ `compute` ä»¥è®¡ç®— ROUGE æŒ‡æ ‡ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6c8630",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0ca821",
   "metadata": {},
   "source": [
    "\n",
    "æ‚¨çš„ `compute_metrics` å‡½æ•°ç°åœ¨å‡†å¤‡å¥½äº†ï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ï¼Œæ‚¨å°†è¿”å›åˆ°å®ƒã€‚\n",
    "\n",
    "## è®­ç»ƒ\n",
    "\n",
    "å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ Trainer è¿›è¡Œæ¨¡å‹å¾®è°ƒï¼Œè¯·æŸ¥çœ‹è¿™é‡Œçš„åŸºæœ¬æ•™ç¨‹ï¼\n",
    "\n",
    "æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ `AutoModelForSeq2SeqLM` åŠ è½½ T5ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69d5b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9e50c8",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "1. åœ¨ `Seq2SeqTrainingArguments` ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚å”¯ä¸€éœ€è¦çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚æ‚¨å¯ä»¥é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼ŒTrainer å°†è¯„ä¼° ROUGE æŒ‡æ ‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚\n",
    "2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ `Seq2SeqTrainer`ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€åˆ†è¯å™¨ã€æ•°æ®æ•´ç†å™¨ã€`compute_metrics` å‡½æ•°ã€‚\n",
    "3. è°ƒç”¨ `train()` ä»¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817acfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"my_awesome_billsum_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=4,\n",
    "    predict_with_generate=True,\n",
    "    fp16=True, # å¯¹äº XPUï¼Œæ›´æ”¹ä¸º bf16=True\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_billsum[\"train\"],\n",
    "    eval_dataset=tokenized_billsum[\"test\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35618465",
   "metadata": {},
   "source": [
    "\n",
    "ä¸€æ—¦è®­ç»ƒå®Œæˆï¼Œä½¿ç”¨ `push_to_hub()` æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c3938f",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a05bce6",
   "metadata": {},
   "source": [
    "\n",
    "## æ¨ç†\n",
    "\n",
    "å¤ªå¥½äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ï¼\n",
    "\n",
    "æƒ³å‡ºä¸€äº›æ‚¨æƒ³è¦æ‘˜è¦çš„æ–‡æœ¬ã€‚å¯¹äº T5ï¼Œæ‚¨éœ€è¦æ ¹æ®æ‚¨æ­£åœ¨å¤„ç†çš„ä»»åŠ¡æ¥å‰ç¼€æ‚¨çš„è¾“å…¥ã€‚å¯¹äºæ‘˜è¦ï¼Œæ‚¨åº”è¯¥åƒä¸‹é¢è¿™æ ·å‰ç¼€æ‚¨çš„è¾“å…¥ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38e9971",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"summarize: The Inflation Reduction Act lowers prescription drug costs, health care costs, and energy costs. It's the most aggressive action on tackling the climate crisis in American history, which will lift up American workers and create good-paying, union jobs across the country. It'll lower the deficit and ask the ultra-wealthy and corporations to pay their fair share. And no one making under $400,000 per year will pay a penny more in taxes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db533b24",
   "metadata": {},
   "source": [
    "\n",
    "å°è¯•ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†å…¶ç”¨äº `pipeline()`ã€‚ä¸ºæ‘˜è¦å®ä¾‹åŒ–ä¸€ä¸ª `pipeline`ï¼Œå¹¶å°†æ‚¨çš„æ–‡æœ¬ä¼ é€’ç»™å®ƒï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93615501",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "summarizer = pipeline(\"summarization\", model=\"username/my_awesome_billsum_model\")\n",
    "summarizer(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136731fd",
   "metadata": {},
   "source": [
    "\n",
    "å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f0470",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n",
    "inputs = tokenizer(text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n",
    "outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n",
    "\n",
    "tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25c9d67",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
