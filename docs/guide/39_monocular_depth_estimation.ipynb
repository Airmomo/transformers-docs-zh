{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba1f7402",
   "metadata": {},
   "source": [
    "# 单目与多目深度估计的区别\n",
    "\n",
    "“单目”指的是使用单张图像（即只有一个视角或镜头）来获取深度信息。与之相对的是双目或多目深度估计，后者使用两个或多个摄像头来获取深度信息，这个表格能帮助你更好地理解和对比单目深度估计与双目或多目深度估计的特点和应用场景。\n",
    "\n",
    "| 特征         | 单目深度估计                                                                                                                   | 双目或多目深度估计                                                           |\n",
    "| ------------ | ------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------- |\n",
    "| **输入**     | 单张图像                                                                                                                       | 多张图像（通常从不同视角拍摄）                                               |\n",
    "| **挑战**     | - 尺度模糊性：无法直接提供绝对深度信息<br>- 光照影响：光照变化影响估计准确性<br>- 遮挡和反射：遮挡物和反射表面干扰深度信息提取 | - 设备复杂性：需要多个摄像头<br>- 计算复杂性：处理多张图像需要更多的计算资源 |\n",
    "| **优势**     | - 设备简单：只需一个摄像头<br>- 适用范围广：适合资源受限的设备，如智能手机、无人机等                                           | - 直接计算深度：通过视差直接计算深度，准确性高                               |\n",
    "| **工作原理** | - 利用纹理、颜色、阴影、几何结构和上下文等视觉线索<br>- 通过学习和模型训练来估计深度                                           | - 通过视差（不同视角下的位置差异）直接计算深度                               |\n",
    "| **应用**     | - 3D 重建<br>- 增强现实<br>- 自动驾驶辅助系统<br>- 机器人导航（资源受限环境）                                                  | - 自动驾驶汽车<br>- 机器人导航（高精度要求）<br>- 工业检测和测量             |\n",
    "| **典型模型** | - Depth Anything V2<br>- ZoeDepth                                                                                              | - Stereo Matching<br>- Structure from Motion (SfM)                           |\n",
    "\n",
    "# 单目深度估计\n",
    "\n",
    "单目深度估计是一项计算机视觉任务，涉及从单个图像中预测场景的深度信息。换句话说，它是从单个摄像头视角估计场景中物体距离的过程。\n",
    "\n",
    "单目深度估计有多种应用，包括 3D 重建、增强现实、自动驾驶和机器人技术。这是一项具有挑战性的任务，因为它需要模型理解场景中物体之间的复杂关系以及相应的深度信息，这些信息可能会受到光照条件、遮挡和纹理等因素的影响。\n",
    "\n",
    "深度估计主要分为两类：\n",
    "\n",
    "- **绝对深度估计**：这种任务变体旨在提供从摄像头到物体的确切深度测量值。这个术语通常与度量深度估计互换使用，其中深度以米或英尺等精确单位表示。绝对深度估计模型输出的深度图包含代表真实距离的数值。\n",
    "- **相对深度估计**：相对深度估计旨在预测场景中物体或点的深度顺序，而不提供精确的测量值。这些模型输出的深度图显示哪些部分更靠近或远离，但不提供实际的距离。\n",
    "\n",
    "在本指南中，我们将介绍如何使用 [Depth Anything V2](https://huggingface.co/depth-anything/Depth-Anything-V2-Large)（一种最先进的零样本相对深度估计模型）和 [ZoeDepth](https://huggingface.co/docs/transformers/main/en/model_doc/zoedepth)（一种绝对深度估计模型）进行推断。\n",
    "\n",
    "查看 [深度估计任务页面](https://huggingface.co/tasks/depth-estimation)，以查看所有兼容的架构和检查点。\n",
    "\n",
    "在开始之前，我们需要安装最新版本的 Transformers：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3224ea4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -U transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53dbbf95",
   "metadata": {},
   "source": [
    "\n",
    "## 深度估计管道\n",
    "\n",
    "使用支持深度估计的模型进行推断的最简单方法是使用相应的 [pipeline()](/docs/transformers/v4.46.2/en/main_classes/pipelines#transformers.pipeline)。从 [Hugging Face Hub 上的检查点](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads)实例化一个管道：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73b613b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n",
    "pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2664dda7",
   "metadata": {},
   "source": [
    "\n",
    "接下来，选择一张图像进行分析：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15bed98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd6bee23",
   "metadata": {},
   "source": [
    "\n",
    "![bee](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg)\n",
    "\n",
    "将图像传递给管道：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32df9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = pipe(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7134f22",
   "metadata": {},
   "source": [
    "\n",
    "管道返回一个包含两个条目的字典。第一个条目称为 `predicted_depth`，是一个张量，其中每个像素的值表示深度（以米为单位）。第二个条目 `depth` 是一个 PIL 图像，可视化了深度估计结果。\n",
    "\n",
    "让我们看一下可视化的结果：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b39b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions[\"depth\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7911029",
   "metadata": {},
   "source": [
    "\n",
    "![depth-visualization](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization.png)\n",
    "\n",
    "## 手动进行深度估计推断\n",
    "\n",
    "现在你已经看到了如何使用深度估计管道，接下来我们来看看如何手动复制相同的结果。\n",
    "\n",
    "首先，从 [Hugging Face Hub 上的检查点](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads)加载模型及其相关处理器。这里我们使用与之前相同的检查点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af041cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForDepthEstimation\n",
    "\n",
    "checkpoint = \"Intel/zoedepth-nyu-kitti\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)\n",
    "model = AutoModelForDepthEstimation.from_pretrained(checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd773344",
   "metadata": {},
   "source": [
    "\n",
    "使用 `image_processor` 准备图像输入，它将处理必要的图像变换，如调整大小和归一化：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f2c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3582f",
   "metadata": {},
   "source": [
    "\n",
    "将准备好的输入传递给模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a1f8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb96f94b",
   "metadata": {},
   "source": [
    "\n",
    "接下来，对结果进行后处理，以去除任何填充并调整深度图的大小以匹配原始图像的尺寸。`post_process_depth_estimation` 函数输出一个包含 `\"predicted_depth\"` 的字典列表。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3d8c4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ZoeDepth 动态填充输入图像。因此，我们在调用 `post_process_depth_estimation` 时传递原始图像的尺寸，以去除填充并调整到原始尺寸。\n",
    "post_processed_output = image_processor.post_process_depth_estimation(\n",
    "    outputs,\n",
    "    source_sizes=[(image.height, image.width)],\n",
    ")\n",
    "\n",
    "predicted_depth = post_processed_output[0][\"predicted_depth\"]\n",
    "depth = (predicted_depth - predicted_depth.min()) / (predicted_depth.max() - predicted_depth.min())\n",
    "depth = depth.detach().cpu().numpy() * 255\n",
    "depth = Image.fromarray(depth.astype(\"uint8\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4383b4c2",
   "metadata": {},
   "source": [
    "\n",
    "在 [原始实现](https://github.com/isl-org/ZoeDepth/blob/edb6daf45458569e24f50250ef1ed08c015f17a7/zoedepth/models/depth_model.py#L131) 中，ZoeDepth 模型会对原始图像和翻转后的图像进行推断，并取平均结果。`post_process_depth_estimation` 函数可以通过传递翻转后的输出到可选参数 `outputs_flipped` 来处理这一点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f8627",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(pixel_values)\n",
    "    outputs_flipped = model(pixel_values=torch.flip(inputs.pixel_values, dims=[3]))\n",
    "\n",
    "post_processed_output = image_processor.post_process_depth_estimation(\n",
    "    outputs,\n",
    "    source_sizes=[(image.height, image.width)],\n",
    "    outputs_flipped=outputs_flipped,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645a121",
   "metadata": {},
   "source": [
    "\n",
    "![depth-visualization-zoe](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/depth-visualization-zoe.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
