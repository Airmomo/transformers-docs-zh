{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "11df696b",
   "metadata": {},
   "source": [
    "# 图像分割\n",
    "\n",
    "图像分割模型用于将图像中对应不同兴趣区域的部分进行分离。这些模型通过为每个像素分配一个标签来实现。分割类型包括语义分割、实例分割和全景分割。\n",
    "\n",
    "在本指南中，我们将：\n",
    "\n",
    "1. 查看不同类型的分割。\n",
    "2. 提供一个端到端的语义分割微调示例。\n",
    "\n",
    "在开始之前，请确保您已安装所有必要的库：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cad4c6a4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q datasets transformers evaluate accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8178621e",
   "metadata": {},
   "source": [
    "\n",
    "我们鼓励您登录您的 Hugging Face 账户，这样您就可以上传并与社区分享您的模型。当提示时，输入您的令牌以登录：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6190a945",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3781f289",
   "metadata": {},
   "source": [
    "\n",
    "## 分割类型\n",
    "\n",
    "语义分割为图像中的每个像素分配一个标签或类别。让我们看一下语义分割模型的输出。它将为图像中遇到的对象的每个实例分配相同的类别，例如，所有猫都将被标记为“猫”，而不是“猫-1”、“猫-2”。我们可以使用 transformers 的图像分割管道快速推断语义分割模型。让我们看一下示例图像。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90345b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/segmentation_input.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb7024b8",
   "metadata": {},
   "source": [
    "\n",
    "我们将使用 [nvidia/segformer-b1-finetuned-cityscapes-1024-1024](https://huggingface.co/nvidia/segformer-b1-finetuned-cityscapes-1024-1024)。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcdd4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_segmentation = pipeline(\"image-segmentation\", \"nvidia/segformer-b1-finetuned-cityscapes-1024-1024\")\n",
    "results = semantic_segmentation(image)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286cd405",
   "metadata": {},
   "source": [
    "\n",
    "分割管道输出包括每个预测类别的掩码。\n",
    "\n",
    "查看汽车类别的掩码，我们可以看到每辆汽车都被分类为相同的掩码。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b8f7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[-1][\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8811a4",
   "metadata": {},
   "source": [
    "\n",
    "![语义分割输出](../../resources/images/semantic_segmentation_output.png)\n",
    "\n",
    "在实例分割中，目标不是对每个像素进行分类，而是预测给定图像中对象的每个实例的掩码。它的工作原理与目标检测非常相似，目标检测中每个实例都有一个边界框，而实例分割中则是一个分割掩码。我们将使用 [facebook/mask2former-swin-large-cityscapes-instance](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-instance) 进行此操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1def34cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "instance_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-instance\")\n",
    "results = instance_segmentation(image)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fef3ca",
   "metadata": {},
   "source": [
    "\n",
    "如您所见，这里有多辆汽车被分类，并且除了属于汽车和人实例的像素外，没有对其他像素进行分类。\n",
    "\n",
    "查看其中一个汽车掩码如下。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edab53c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "results[2][\"mask\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee51cfa",
   "metadata": {},
   "source": [
    "\n",
    "![实例分割输出](../../resources/images/instance_segmentation_output.png)\n",
    "\n",
    "全景分割结合了语义分割和实例分割，每个像素都被分类为一个类别和该类别的实例，并且每个类别的每个实例都有多个掩码。我们可以使用 [facebook/mask2former-swin-large-cityscapes-panoptic](https://huggingface.co/facebook/mask2former-swin-large-cityscapes-panoptic) 进行此操作。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2897696",
   "metadata": {},
   "outputs": [],
   "source": [
    "panoptic_segmentation = pipeline(\"image-segmentation\", \"facebook/mask2former-swin-large-cityscapes-panoptic\")\n",
    "results = panoptic_segmentation(image)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33186907",
   "metadata": {},
   "source": [
    "\n",
    "如您所见，我们有更多的类别。稍后我们将说明每个像素都被分类为其中一个类别。\n",
    "\n",
    "让我们对所有类型的分割进行并排比较。\n",
    "\n",
    "![分割图比较](../../resources/images/segmentation-comparison.png)\n",
    "\n",
    "看到所有类型的分割后，让我们深入了解为语义分割微调模型。\n",
    "\n",
    "语义分割的常见现实应用包括训练自动驾驶汽车以识别行人和重要交通信息、在医学图像中识别细胞和异常情况，以及从卫星图像中监测环境变化。\n",
    "\n",
    "## 为分割微调模型\n",
    "\n",
    "我们现在将：\n",
    "\n",
    "1. 在 [SceneParse150](https://huggingface.co/datasets/scene_parse_150) 数据集上微调 [SegFormer](https://huggingface.co/docs/transformers/main/en/model_doc/segformer#segformer)。\n",
    "2. 使用您微调的模型进行推理。\n",
    "\n",
    "要查看与此任务兼容的所有架构和检查点，我们建议查看 [任务页面](https://huggingface.co/tasks/image-segmentation)。\n",
    "\n",
    "### 加载 SceneParse150 数据集\n",
    "\n",
    "首先从 🤗 Datasets 库中加载 SceneParse150 数据集的一个较小的子集。这将让您有机会进行实验并确保一切正常工作，然后再在完整数据集上花费更多时间进行训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a6991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab5c548",
   "metadata": {},
   "source": [
    "\n",
    "使用 [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) 方法将数据集的 `train` 分割成训练集和测试集：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832bb794",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ds.train_test_split(test_size=0.2)\n",
    "train_ds = ds[\"train\"]\n",
    "test_ds = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5429e11c",
   "metadata": {},
   "source": [
    "\n",
    "然后查看一个示例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0fc3f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds[0]\n",
    "\n",
    "# 查看图像\n",
    "train_ds[0][\"image\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78d533e1",
   "metadata": {},
   "source": [
    "\n",
    "- `image`：场景的 PIL 图像。\n",
    "- `annotation`：分割图的 PIL 图像，这也是模型的目标。\n",
    "- `scene_category`：描述图像场景的类别 ID，如“厨房”或“办公室”。在本指南中，您只需要 `image` 和 `annotation`，它们都是 PIL 图像。\n",
    "\n",
    "您还需要创建一个字典，将标签 ID 映射到标签类，这在稍后设置模型时将非常有用。从 Hub 下载映射并创建 `id2label` 和 `label2id` 字典：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7228601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "repo_id = \"huggingface/label-files\"\n",
    "filename = \"ade20k-id2label.json\"\n",
    "id2label = json.loads(Path(hf_hub_download(repo_id, filename, repo_type=\"dataset\")).read_text())\n",
    "id2label = {int(k): v for k, v in id2label.items()}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "num_labels = len(id2label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd3424b",
   "metadata": {},
   "source": [
    "\n",
    "#### 自定义数据集\n",
    "\n",
    "如果您愿意，也可以创建并使用您自己的数据集。如果您想使用 [run_semantic_segmentation.py](https://github.com/huggingface/transformers/blob/main/examples/pytorch/semantic-segmentation/run_semantic_segmentation.py) 脚本而不是笔记本实例进行训练，可以按照以下步骤操作。该脚本需要：\n",
    "\n",
    "1. 一个包含两个 [Image](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Image) 列的 [DatasetDict](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.DatasetDict)，即“image”和“label”。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cbc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, DatasetDict, Image\n",
    "\n",
    "image_paths_train = [\"path/to/image_1.jpg\", \"path/to/image_2.jpg\", ..., \"path/to/image_n.jpg\"]\n",
    "label_paths_train = [\"path/to/annotation_1.png\", \"path/to/annotation_2.png\", ..., \"path/to/annotation_n.png\"]\n",
    "\n",
    "image_paths_validation = [...]\n",
    "label_paths_validation = [...]\n",
    "\n",
    "def create_dataset(image_paths, label_paths):\n",
    "    dataset = Dataset.from_dict({\"image\": sorted(image_paths),\n",
    "                                \"label\": sorted(label_paths)})\n",
    "    dataset = dataset.cast_column(\"image\", Image())\n",
    "    dataset = dataset.cast_column(\"label\", Image())\n",
    "    return dataset\n",
    "\n",
    "# 步骤 1: 创建 Dataset 对象\n",
    "train_dataset = create_dataset(image_paths_train, label_paths_train)\n",
    "validation_dataset = create_dataset(image_paths_validation, label_paths_validation)\n",
    "\n",
    "# 步骤 2: 创建 DatasetDict\n",
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": validation_dataset,\n",
    "    }\n",
    ")\n",
    "\n",
    "# 步骤 3: 推送到 Hub（假设您已在终端/笔记本中运行了 huggingface-cli login 命令）\n",
    "dataset.push_to_hub(\"your-name/dataset-repo\")\n",
    "\n",
    "# 可选地，您可以将模型推送到 Hub 上的私有仓库\n",
    "# dataset.push_to_hub(\"name of repo on the hub\", private=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013ca44b",
   "metadata": {},
   "source": [
    "\n",
    "2. 一个将类整数映射到类名的 id2label 字典。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6a7d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "# 简单示例\n",
    "id2label = {0: 'cat', 1: 'dog'}\n",
    "with open('id2label.json', 'w') as fp:\n",
    "    json.dump(id2label, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b529254a",
   "metadata": {},
   "source": [
    "\n",
    "例如，查看这个使用上述步骤创建的 [示例数据集](https://huggingface.co/datasets/nielsr/ade20k-demo)，它包含了创建步骤。\n",
    "\n",
    "### 预处理\n",
    "\n",
    "下一步是加载 SegFormer 图像处理器来准备模型所需的图像和注释。一些数据集，如本例中的数据集，使用零索引作为背景类。然而，背景类实际上并不包含在 150 个类中，因此您需要设置 `do_reduce_labels=True` 以从所有标签中减去一。零索引被替换为 `255`，这样 SegFormer 的损失函数就会忽略它：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16721e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"nvidia/mit-b0\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint, do_reduce_labels=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d59bcec",
   "metadata": {},
   "source": [
    "\n",
    "PyTorch\n",
    "\n",
    "通常对图像数据集应用一些数据增强，以使模型更能抵抗过拟合。在本指南中，您将使用 [torchvision](https://pytorch.org/vision/stable/index.html) 的 `ColorJitter` 函数随机更改图像的颜色属性，但您也可以使用您喜欢的任何图像库。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6a44df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import ColorJitter\n",
    "\n",
    "jitter = ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae719fb6",
   "metadata": {},
   "source": [
    "\n",
    "现在创建两个预处理函数来准备模型所需的图像和注释。这些函数将图像转换为 `pixel_values`，并将注释转换为 `labels`。对于训练集，`jitter` 在提供图像给图像处理器之前应用。对于测试集，图像处理器只裁剪和规范化 `images`，并且只裁剪 `labels`，因为在测试期间不应用数据增强。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a542f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transforms(example_batch):\n",
    "    images = [jitter(x) for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs\n",
    "\n",
    "def val_transforms(example_batch):\n",
    "    images = [x for x in example_batch[\"image\"]]\n",
    "    labels = [x for x in example_batch[\"annotation\"]]\n",
    "    inputs = image_processor(images, labels)\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4c682d",
   "metadata": {},
   "source": [
    "\n",
    "要在整个数据集上应用 `jitter`，使用 🤗 Datasets 的 [set_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) 函数。转换是即时应用的，这更快，并且消耗更少的磁盘空间：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996f623f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds.set_transform(train_transforms)\n",
    "test_ds.set_transform(val_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbabb0ec",
   "metadata": {},
   "source": [
    "\n",
    "### 评估\n",
    "\n",
    "在训练过程中包含一个指标通常有助于评估您的模型性能。您可以使用 🤗 [Evaluate](https://huggingface.co/docs/evaluate/index) 库快速加载一个评估方法。对于此任务，加载 [平均交并比](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) 指标（参阅 🤗 Evaluate [快速入门](https://huggingface.co/docs/evaluate/a_quick_tour) 以了解更多关于如何加载和计算指标的信息）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9ce24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"mean_iou\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bcc0463",
   "metadata": {},
   "source": [
    "\n",
    "然后创建一个函数来 [计算](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) 指标。您的预测需要先转换为 logits，然后重塑以匹配标签的大小，然后您才能调用 [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute)：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5991bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    with torch.no_grad():\n",
    "        logits, labels = eval_pred\n",
    "        logits_tensor = torch.from_numpy(logits)\n",
    "        logits_tensor = nn.functional.interpolate(\n",
    "            logits_tensor,\n",
    "            size=labels.shape[-2:],\n",
    "            mode=\"bilinear\",\n",
    "            align_corners=False,\n",
    "        ).argmax(dim=1)\n",
    "\n",
    "        pred_labels = logits_tensor.detach().cpu().numpy()\n",
    "        metrics = metric.compute(\n",
    "            predictions=pred_labels,\n",
    "            references=labels,\n",
    "            num_labels=num_labels,\n",
    "            ignore_index=255,\n",
    "            reduce_labels=False,\n",
    "        )\n",
    "        for key, value in metrics.items():\n",
    "            if isinstance(value, np.ndarray):\n",
    "                metrics[key] = value.tolist()\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e56278b",
   "metadata": {},
   "source": [
    "\n",
    "### 训练\n",
    "\n",
    "PyTorch\n",
    "\n",
    "如果您不熟悉使用 [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 微调模型，请查看 [这里](../training#finetune-with-trainer) 的基本教程！\n",
    "\n",
    "您现在可以开始训练您的模型了！使用 [AutoModelForSemanticSegmentation](/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSemanticSegmentation) 加载 SegFormer，并将模型传递给标签 ID 和标签类之间的映射：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddf253c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSemanticSegmentation, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForSemanticSegmentation.from_pretrained(checkpoint, id2label=id2label, label2id=label2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768ceed9",
   "metadata": {},
   "source": [
    "\n",
    "在这一点上，只剩下三个步骤：\n",
    "\n",
    "1. 在 [TrainingArguments](/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) 中定义您的训练超参数。重要的是您不要删除未使用的列，因为这会删除 `image` 列。如果没有 `image` 列，您就不能创建 `pixel_values`。设置 `remove_unused_columns=False` 以防止这种行为！唯一其他必需的参数是 `output_dir`，它指定保存模型的位置。您将通过设置 `push_to_hub=True` 将模型推送到 Hub（您需要登录 Hugging Face 才能上传您的模型）。在每个 epoch 结束时，[Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 将评估 IoU 指标并保存训练检查点。\n",
    "2. 将训练参数传递给 [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)，以及模型、数据集、tokenizer、数据整理器和 `compute_metrics` 函数。\n",
    "3. 调用 [train()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) 来微调您的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b23ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"segformer-b0-scene-parse-150\",\n",
    "    learning_rate=6e-5,\n",
    "    num_train_epochs=50,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    save_total_limit=3,\n",
    "    eval_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,\n",
    "    eval_steps=20,\n",
    "    logging_steps=1,\n",
    "    eval_accumulation_steps=5,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cb4986",
   "metadata": {},
   "source": [
    "\n",
    "训练完成后，使用 [push_to_hub()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) 方法将您的模型分享到 Hub，以便每个人都可以使用您的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3813dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0db1022",
   "metadata": {},
   "source": [
    "\n",
    "### 推理\n",
    "\n",
    "太好了，现在您已经微调了一个模型，您可以使用它进行推理了！\n",
    "\n",
    "重新加载数据集并加载一个图像进行推理。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b244ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"scene_parse_150\", split=\"train[:50]\")\n",
    "ds = ds.train_test_split(test_size=0.2)\n",
    "test_ds = ds[\"test\"]\n",
    "image = ds[\"test\"][0][\"image\"]\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef36f265",
   "metadata": {},
   "source": [
    "\n",
    "![卧室图像](../../resources/images/semantic-seg-image.png)\n",
    "\n",
    "PyTorch\n",
    "\n",
    "我们现在将看到如何在没有管道的情况下进行推理。使用图像处理器处理图像，并将 `pixel_values` 放在 GPU 上：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3583765",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 如果可用，使用 GPU，否则使用 CPU\n",
    "encoding = image_processor(image, return_tensors=\"pt\")\n",
    "pixel_values = encoding.pixel_values.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d85a7ef",
   "metadata": {},
   "source": [
    "\n",
    "将您的输入传递给模型并返回 `logits`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152ed89",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(pixel_values=pixel_values)\n",
    "logits = outputs.logits.cpu()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57388824",
   "metadata": {},
   "source": [
    "\n",
    "接下来，将 logits 缩放到原始图像大小：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9719a63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampled_logits = nn.functional.interpolate(\n",
    "    logits,\n",
    "    size=image.size[::-1],\n",
    "    mode=\"bilinear\",\n",
    "    align_corners=False,\n",
    ")\n",
    "\n",
    "pred_seg = upsampled_logits.argmax(dim=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b55afe",
   "metadata": {},
   "source": [
    "\n",
    "为了可视化结果，加载 [数据集颜色调色板](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) 作为 `ade_palette()`，它将每个类映射到它们的 RGB 值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fcc3936",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ade_palette():\n",
    "    return np.asarray([\n",
    "        [0, 0, 0],\n",
    "        [120, 120, 120],\n",
    "        [180, 120, 120],\n",
    "        [6, 230, 230],\n",
    "        [80, 50, 50],\n",
    "        [4, 200, 3],\n",
    "        [120, 120, 80],\n",
    "        [140, 140, 140],\n",
    "        [204, 5, 255],\n",
    "        [230, 230, 230],\n",
    "        [4, 250, 7],\n",
    "        [224, 5, 255],\n",
    "        [235, 255, 7],\n",
    "        [150, 5, 61],\n",
    "        [120, 120, 70],\n",
    "        [8, 255, 51],\n",
    "        [255, 6, 82],\n",
    "        [143, 255, 140],\n",
    "        [204, 255, 4],\n",
    "        [255, 51, 7],\n",
    "        [204, 70, 3],\n",
    "        [0, 102, 200],\n",
    "        [61, 230, 250],\n",
    "        [255, 6, 51],\n",
    "        [11, 102, 255],\n",
    "        [255, 7, 71],\n",
    "        [255, 9, 224],\n",
    "        [9, 7, 230],\n",
    "        [220, 220, 220],\n",
    "        [255, 9, 92],\n",
    "        [112, 9, 255],\n",
    "        [8, 255, 214],\n",
    "        [7, 255, 224],\n",
    "        [255, 184, 6],\n",
    "        [10, 255, 71],\n",
    "        [255, 41, 10],\n",
    "        [7, 255, 255],\n",
    "        [224, 255, 8],\n",
    "        [102, 8, 255],\n",
    "        [255, 61, 6],\n",
    "        [255, 194, 7],\n",
    "        [255, 122, 8],\n",
    "        [0, 255, 20],\n",
    "        [255, 8, 41],\n",
    "        [255, 5, 153],\n",
    "        [6, 51, 255],\n",
    "        [235, 12, 255],\n",
    "        [160, 150, 20],\n",
    "        [0, 163, 255],\n",
    "        [140, 140, 140],\n",
    "        [250, 10, 15],\n",
    "        [20, 255, 0],\n",
    "        [31, 255, 0],\n",
    "        [255, 31, 0],\n",
    "        [255, 224, 0],\n",
    "        [153, 255, 0],\n",
    "        [0, 0, 255],\n",
    "        [255, 71, 0],\n",
    "        [0, 235, 255],\n",
    "        [0, 173, 255],\n",
    "        [31, 0, 255],\n",
    "        [11, 200, 200],\n",
    "        [255, 82, 0],\n",
    "        [0, 255, 245],\n",
    "        [0, 61, 255],\n",
    "        [0, 255, 112],\n",
    "        [0, 255, 133],\n",
    "        [255, 0, 0],\n",
    "        [255, 163, 0],\n",
    "        [255, 102, 0],\n",
    "        [194, 255, 0],\n",
    "        [0, 143, 255],\n",
    "        [51, 255, 0],\n",
    "        [0, 82, 255],\n",
    "        [0, 255, 41],\n",
    "        [0, 255, 173],\n",
    "        [10, 0, 255],\n",
    "        [173, 255, 0],\n",
    "        [0, 255, 153],\n",
    "        [255, 92, 0],\n",
    "        [255, 0, 255],\n",
    "        [255, 0, 245],\n",
    "        [255, 0, 102],\n",
    "        [255, 173, 0],\n",
    "        [255, 0, 20],\n",
    "        [255, 184, 184],\n",
    "        [0, 31, 255],\n",
    "        [0, 255, 61],\n",
    "        [0, 71, 255],\n",
    "        [255, 0, 204],\n",
    "        [0, 255, 194],\n",
    "        [0, 255, 82],\n",
    "        [0, 10, 255],\n",
    "        [0, 112, 255],\n",
    "        [51, 0, 255],\n",
    "        [0, 194, 255],\n",
    "        [0, 122, 255],\n",
    "        [0, 255, 163],\n",
    "        [255, 153, 0],\n",
    "        [0, 255, 10],\n",
    "        [255, 112, 0],\n",
    "        [143, 255, 0],\n",
    "        [82, 0, 255],\n",
    "        [163, 255, 0],\n",
    "        [255, 235, 0],\n",
    "        [8, 184, 170],\n",
    "        [133, 0, 255],\n",
    "        [0, 255, 92],\n",
    "        [184, 0, 255],\n",
    "        [255, 0, 31],\n",
    "        [0, 184, 255],\n",
    "        [0, 214, 255],\n",
    "        [255, 0, 112],\n",
    "        [92, 255, 0],\n",
    "        [0, 224, 255],\n",
    "        [112, 224, 255],\n",
    "        [70, 184, 160],\n",
    "        [163, 0, 255],\n",
    "        [153, 0, 255],\n",
    "        [71, 255, 0],\n",
    "        [255, 0, 163],\n",
    "        [255, 204, 0],\n",
    "        [255, 0, 143],\n",
    "        [0, 255, 235],\n",
    "        [133, 255, 0],\n",
    "        [255, 0, 235],\n",
    "        [245, 0, 255],\n",
    "        [255, 0, 122],\n",
    "        [255, 245, 0],\n",
    "        [10, 190, 212],\n",
    "        [214, 255, 0],\n",
    "        [0, 204, 255],\n",
    "        [20, 0, 255],\n",
    "        [255, 255, 0],\n",
    "        [0, 153, 255],\n",
    "        [0, 41, 255],\n",
    "        [0, 255, 204],\n",
    "        [41, 0, 255],\n",
    "        [41, 255, 0],\n",
    "        [173, 0, 255],\n",
    "        [0, 245, 255],\n",
    "        [71, 0, 255],\n",
    "        [122, 0, 255],\n",
    "        [0, 255, 184],\n",
    "        [0, 92, 255],\n",
    "        [184, 255, 0],\n",
    "        [0, 133, 255],\n",
    "        [255, 214, 0],\n",
    "        [25, 194, 194],\n",
    "        [102, 255, 0],\n",
    "        [92, 0, 255],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40c757c",
   "metadata": {},
   "source": [
    "\n",
    "然后，您可以组合并绘制您的图像和预测的分割图：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38c8fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "color_seg = np.zeros((pred_seg.shape[0], pred_seg.shape[1], 3), dtype=np.uint8)\n",
    "palette = np.array(ade_palette())\n",
    "for label, color in enumerate(palette):\n",
    "    color_seg[pred_seg == label, :] = color\n",
    "color_seg = color_seg[..., ::-1]  # 转换为 BGR\n",
    "\n",
    "img = np.array(image) * 0.5 + color_seg * 0.5  # 绘制带有分割图的图像\n",
    "img = img.astype(np.uint8)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "plt.imshow(img)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635450bf",
   "metadata": {},
   "source": [
    "\n",
    "![带有分割图的卧室图像](../../resources/images/semantic-seg-preds.png)\n",
    "\n",
    "恭喜！您已经成功地微调了一个图像分割模型，并使用它进行了推理。您现在可以将这个模型用于各种图像分割任务，如自动驾驶、医学图像分析和环境监测。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
