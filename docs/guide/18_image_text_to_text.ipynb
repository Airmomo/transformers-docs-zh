{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 图像-视觉多模态理解模型 (VLM with image-input, Image-text-to-text)\n",
        "\n",
        "图像文本到文本模型，也称为视觉语言模型（VLMs），是接受图像输入的语言模型。这些模型可以处理各种任务，从视觉问答到图像分割。这个任务与图像到文本有许多相似之处，但也有一些重叠的用例，如图像描述。图像到文本模型只接受图像输入，通常完成一个特定的任务，而VLMs接受开放式的文本和图像输入，是更通用的模型。\n",
        "\n",
        "在本指南中，我们将简要概述VLMs，并展示如何使用Transformers进行推理。\n",
        "\n",
        "首先，有几种类型的VLMs：\n",
        "\n",
        "- 用于微调的基础模型\n",
        "- 用于对话的聊天微调模型\n",
        "- 用于指令的微调模型\n",
        "\n",
        "本指南重点介绍使用指令微调模型进行推理。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "让我们开始安装依赖项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "pip install -q transformers accelerate flash_attn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在初始化模型和处理器。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
        "import torch\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "model = AutoModelForImageTextToText.from_pretrained(\n",
        "    \"HuggingFaceM4/idefics2-8b\",\n",
        "    torch_dtype=torch.bfloat16,\n",
        "    attn_implementation=\"flash_attention_2\",\n",
        ").to(device)\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "这个模型有一个聊天模板，帮助用户解析聊天输出。此外，该模型还可以在单个对话或消息中接受多个图像作为输入。我们将准备输入图像，如下所示：\n",
        "\n",
        "![两只猫坐在网上](../../resources/images/cats.png)\n",
        "\n",
        "![一只蜜蜂在粉红色的花朵上](../../resources/images/bee.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "下面是聊天模板的示例。我们可以通过在模板末尾附加它来提供对话回合和最后一条消息作为输入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"What do we see in this image?\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"assistant\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"text\", \"text\": \"In this image we can see two cats on the nets.\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": \"And how about this image?\"}\n",
        "        ]\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们现在将调用处理器的`apply_chat_template()`方法来预处理其输出以及图像输入。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompt = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
        "inputs = processor(text=prompt, images=[images[0], images[1]], return_tensors=\"pt\").to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "我们现在可以将预处理后的输入传递给模型。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    generated_ids = model.generate(**inputs, max_new_tokens=500)\n",
        "generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n",
        "\n",
        "print(generated_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 流式传输\n",
        "\n",
        "我们可以使用文本流式传输来获得更好的生成体验。Transformers支持使用`TextStreamer`或`TextIteratorStreamer`类进行流式传输。我们将使用`TextIteratorStreamer`与`IDEFICS-8B`。\n",
        "\n",
        "假设我们有一个应用程序，它保留聊天历史并接收新的用户输入。我们将像往常一样预处理输入，并初始化`TextIteratorStreamer`来在单独的线程中处理生成。这允许你实时流式传输生成的文本标记。任何生成参数都可以传递给`TextIteratorStreamer`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from transformers import TextIteratorStreamer\n",
        "from threading import Thread\n",
        "\n",
        "def model_inference(\n",
        "    user_prompt,\n",
        "    chat_history,\n",
        "    max_new_tokens,\n",
        "    images\n",
        "):\n",
        "    user_prompt = {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": [\n",
        "            {\"type\": \"image\"},\n",
        "            {\"type\": \"text\", \"text\": user_prompt},\n",
        "        ]\n",
        "    }\n",
        "    chat_history.append(user_prompt)\n",
        "    streamer = TextIteratorStreamer(\n",
        "        processor.tokenizer,\n",
        "        skip_prompt=True,\n",
        "        timeout=5.0,\n",
        "    )\n",
        "\n",
        "    generation_args = {\n",
        "        \"max_new_tokens\": max_new_tokens,\n",
        "        \"streamer\": streamer,\n",
        "        \"do_sample\": False\n",
        "    }\n",
        "\n",
        "    # add_generation_prompt=True使模型生成机器人响应\n",
        "    prompt = processor.apply_chat_template(chat_history, add_generation_prompt=True)\n",
        "    inputs = processor(\n",
        "        text=prompt,\n",
        "        images=images,\n",
        "        return_tensors=\"pt\",\n",
        "    ).to(device)\n",
        "    generation_args.update(inputs)\n",
        "\n",
        "    thread = Thread(\n",
        "        target=model.generate,\n",
        "        kwargs=generation_args,\n",
        "    )\n",
        "    thread.start()\n",
        "\n",
        "    acc_text = \"\"\n",
        "    for text_token in streamer:\n",
        "        time.sleep(0.04)\n",
        "        acc_text += text_token\n",
        "        if acc_text.endswith(\"<end_of_utterance>\"):\n",
        "            acc_text = acc_text[:-18]\n",
        "        yield acc_text\n",
        "\n",
        "    thread.join()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "现在让我们调用我们创建的`model_inference`函数并流式传输值。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "generator = model_inference(\n",
        "    user_prompt=\"And what is in this image?\",\n",
        "    chat_history=messages,\n",
        "    max_new_tokens=100,\n",
        "    images=images\n",
        ")\n",
        "\n",
        "for value in generator:\n",
        "    print(value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 在较小硬件上适配模型\n",
        "\n",
        "VLMs通常很大，需要优化以适应较小的硬件。Transformers支持许多模型量化库，这里我们只展示使用Quanto的int8量化。int8量化提供了高达75％的内存改进（如果所有权重都被量化）。然而，它并不是免费的午餐，因为8位不是CUDA本机精度，权重在运行时被量化来回转换，这增加了延迟。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "首先，安装依赖项。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "shellscript"
        }
      },
      "outputs": [],
      "source": [
        "pip install -U quanto bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "要在加载期间量化模型，我们首先需要创建`QuantoConfig`。然后像往常一样加载模型，但在模型初始化期间传递`quantization_config`。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import AutoModelForImageTextToText, QuantoConfig\n",
        "\n",
        "model_id = \"HuggingFaceM4/idefics2-8b\"\n",
        "quantization_config = QuantoConfig(weights=\"int8\")\n",
        "quantized_model = AutoModelForImageTextToText.from_pretrained(\n",
        "    model_id, device_map=\"cuda\", quantization_config=quantization_config\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "就这样，我们可以使用模型，无需任何更改。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 进一步阅读\n",
        "\n",
        "这里有一些关于图像文本到文本任务的更多资源。\n",
        "\n",
        "- [图像文本到文本任务页面](https://huggingface.co/tasks/image-text-to-text)涵盖了模型类型、用例、数据集等。\n",
        "- [视觉语言模型解释](https://huggingface.co/blog/vlms)是一篇博客文章，涵盖了关于视觉语言模型和使用TRL进行监督微调的所有内容。"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
