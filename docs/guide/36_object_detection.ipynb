{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a765ab72",
   "metadata": {},
   "source": [
    "# ç›®æ ‡æ£€æµ‹\n",
    "\n",
    "ç›®æ ‡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰ä»»åŠ¡ä¸­çš„ä¸€ç§ï¼Œç”¨äºåœ¨å›¾åƒä¸­æ£€æµ‹å®ä¾‹ï¼ˆå¦‚äººã€å»ºç­‘ç‰©æˆ–æ±½è½¦ï¼‰ã€‚ç›®æ ‡æ£€æµ‹æ¨¡å‹æ¥æ”¶å›¾åƒä½œä¸ºè¾“å…¥ï¼Œå¹¶è¾“å‡ºæ£€æµ‹åˆ°çš„ç›®æ ‡çš„è¾¹ç•Œæ¡†åæ ‡å’Œå…³è”æ ‡ç­¾ã€‚ä¸€å¼ å›¾åƒå¯ä»¥åŒ…å«å¤šä¸ªç›®æ ‡ï¼Œæ¯ä¸ªç›®æ ‡æœ‰è‡ªå·±çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒä¸­å¯èƒ½åŒæ—¶åŒ…å«æ±½è½¦å’Œå»ºç­‘ç‰©ï¼‰ï¼Œå¹¶ä¸”æ¯ä¸ªç›®æ ‡å¯ä»¥åœ¨å›¾åƒçš„ä¸åŒä½ç½®å‡ºç°ï¼ˆä¾‹å¦‚ï¼Œå›¾åƒä¸­å¯èƒ½æœ‰å‡ è¾†æ±½è½¦ï¼‰ã€‚è¿™é¡¹ä»»åŠ¡å¸¸ç”¨äºè‡ªåŠ¨é©¾é©¶ä¸­ï¼Œç”¨äºæ£€æµ‹è¡Œäººã€è·¯æ ‡å’Œäº¤é€šä¿¡å·ç¯ç­‰ã€‚å…¶ä»–åº”ç”¨åœºæ™¯åŒ…æ‹¬å›¾åƒä¸­çš„ç›®æ ‡è®¡æ•°ã€å›¾åƒæœç´¢ç­‰ã€‚\n",
    "\n",
    "åœ¨è¿™ä¸ªæŒ‡å—ä¸­ï¼Œä½ å°†å­¦ä¹ å¦‚ä½•ï¼š\n",
    "\n",
    "1. åœ¨ [CPPE-5 æ•°æ®é›†](https://huggingface.co/datasets/cppe-5)ä¸Šå¾®è°ƒ [DETR](https://huggingface.co/docs/transformers/model_doc/detr)ï¼Œè¿™æ˜¯ä¸€ç§ç»“åˆäº†å·ç§¯ä¸»å¹²å’Œç¼–ç å™¨-è§£ç å™¨ Transformer çš„æ¨¡å‹ã€‚\n",
    "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "è¦æŸ¥çœ‹ä¸è¯¥ä»»åŠ¡å…¼å®¹çš„æ‰€æœ‰æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œå»ºè®®æŸ¥é˜… [ä»»åŠ¡é¡µé¢](https://huggingface.co/tasks/object-detection)ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50304fe3",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q datasets transformers accelerate timm\n",
    "pip install -q -U albumentations>=1.4.5 torchmetrics pycocotools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b27cf9",
   "metadata": {},
   "source": [
    "\n",
    "ä½ å°†ä½¿ç”¨ ğŸ¤— Datasets åŠ è½½æ¥è‡ª Hugging Face Hub çš„æ•°æ®é›†ï¼Œä½¿ç”¨ ğŸ¤— Transformers è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ `albumentations` å¢å¼ºæ•°æ®ã€‚\n",
    "\n",
    "æˆ‘ä»¬é¼“åŠ±ä½ ä¸ç¤¾åŒºåˆ†äº«ä½ çš„æ¨¡å‹ã€‚ç™»å½• Hugging Face è´¦æˆ·ä»¥å°†å…¶ä¸Šä¼ åˆ° Hubã€‚å½“æç¤ºæ—¶ï¼Œè¾“å…¥ä½ çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0462cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d75184e",
   "metadata": {},
   "source": [
    "\n",
    "é¦–å…ˆï¼Œæˆ‘ä»¬å®šä¹‰å…¨å±€å¸¸é‡ï¼Œå³æ¨¡å‹åç§°å’Œå›¾åƒå¤§å°ã€‚åœ¨è¿™ä¸ªæ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨æ¡ä»¶ DETR æ¨¡å‹ï¼Œå› ä¸ºå®ƒæ”¶æ•›é€Ÿåº¦æ›´å¿«ã€‚ä½ å¯ä»¥é€‰æ‹© `transformers` åº“ä¸­å¯ç”¨çš„ä»»ä½•ç›®æ ‡æ£€æµ‹æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d871e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"microsoft/conditional-detr-resnet-50\"  # æˆ– \"facebook/detr-resnet-50\"\n",
    "IMAGE_SIZE = 480"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19beac9",
   "metadata": {},
   "source": [
    "\n",
    "## åŠ è½½ CPPE-5 æ•°æ®é›†\n",
    "\n",
    "[CPPE-5 æ•°æ®é›†](https://huggingface.co/datasets/cppe-5)åŒ…å«æ³¨é‡Šäº†åŒ»ç–—ä¸ªäººé˜²æŠ¤è£…å¤‡ï¼ˆPPEï¼‰çš„å›¾åƒï¼Œè¿™äº›æ³¨é‡Šæ˜¯åœ¨ COVID-19 å¤§æµè¡ŒæœŸé—´ç”Ÿæˆçš„ã€‚\n",
    "\n",
    "é¦–å…ˆåŠ è½½æ•°æ®é›†å¹¶ä»è®­ç»ƒé›†ä¸­åˆ›å»ºä¸€ä¸ªéªŒè¯é›†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1146e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "cppe5 = load_dataset(\"cppe-5\")\n",
    "\n",
    "if \"validation\" not in cppe5:\n",
    "    split = cppe5[\"train\"].train_test_split(0.15, seed=1337)\n",
    "    cppe5[\"train\"] = split[\"train\"]\n",
    "    cppe5[\"validation\"] = split[\"test\"]\n",
    "\n",
    "cppe5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dd29dd",
   "metadata": {},
   "source": [
    "\n",
    "ä½ ä¼šå‘ç°è¿™ä¸ªæ•°æ®é›†æœ‰ 1000 å¼ å›¾åƒç”¨äºè®­ç»ƒå’ŒéªŒè¯é›†ï¼Œä»¥åŠä¸€ä¸ªåŒ…å« 29 å¼ å›¾åƒçš„æµ‹è¯•é›†ã€‚\n",
    "\n",
    "ä¸ºäº†ç†Ÿæ‚‰æ•°æ®ï¼Œæ¢ç´¢ä¸€ä¸‹ç¤ºä¾‹çš„æ ·å­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8706ec99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cppe5[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f62cfff3",
   "metadata": {},
   "source": [
    "\n",
    "æ•°æ®é›†ä¸­çš„ç¤ºä¾‹åŒ…å«ä»¥ä¸‹å­—æ®µï¼š\n",
    "\n",
    "- `image_id`ï¼šç¤ºä¾‹å›¾åƒçš„ ID\n",
    "- `image`ï¼šåŒ…å«å›¾åƒçš„ `PIL.Image.Image` å¯¹è±¡\n",
    "- `width`ï¼šå›¾åƒçš„å®½åº¦\n",
    "- `height`ï¼šå›¾åƒçš„é«˜åº¦\n",
    "- `objects`ï¼šåŒ…å«å›¾åƒä¸­ç›®æ ‡çš„è¾¹ç•Œæ¡†å…ƒæ•°æ®çš„å­—å…¸ï¼š\n",
    "  - `id`ï¼šæ³¨é‡Š ID\n",
    "  - `area`ï¼šè¾¹ç•Œæ¡†çš„é¢ç§¯\n",
    "  - `bbox`ï¼šç›®æ ‡çš„è¾¹ç•Œæ¡†ï¼ˆé‡‡ç”¨ [COCO æ ¼å¼](https://albumentations.ai/docs/getting_started/bounding_boxes_augmentation/#coco)ï¼‰\n",
    "  - `category`ï¼šç›®æ ‡çš„ç±»åˆ«ï¼Œå¯èƒ½çš„å€¼åŒ…æ‹¬ `Coverall (0)`ã€`Face_Shield (1)`ã€`Gloves (2)`ã€`Goggles (3)` å’Œ `Mask (4)`\n",
    "\n",
    "ä½ å¯èƒ½ä¼šæ³¨æ„åˆ° `bbox` å­—æ®µéµå¾ª COCO æ ¼å¼ï¼Œè¿™æ˜¯ DETR æ¨¡å‹æœŸæœ›çš„æ ¼å¼ã€‚ç„¶è€Œï¼Œå­—æ®µåœ¨ `objects` å†…éƒ¨çš„åˆ†ç»„ä¸ DETR è¦æ±‚çš„æ³¨é‡Šæ ¼å¼ä¸åŒã€‚åœ¨ä½¿ç”¨æ­¤æ•°æ®è¿›è¡Œè®­ç»ƒä¹‹å‰ï¼Œéœ€è¦åº”ç”¨ä¸€äº›é¢„å¤„ç†è½¬æ¢ã€‚\n",
    "\n",
    "ä¸ºäº†æ›´å¥½åœ°ç†è§£æ•°æ®ï¼Œå¯è§†åŒ–æ•°æ®é›†ä¸­çš„ä¸€ä¸ªç¤ºä¾‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa163a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "image = cppe5[\"train\"][2][\"image\"]\n",
    "annotations = cppe5[\"train\"][2][\"objects\"]\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "categories = cppe5[\"train\"].features[\"objects\"].feature[\"category\"].names\n",
    "\n",
    "id2label = {index: x for index, x in enumerate(categories, start=0)}\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "for i in range(len(annotations[\"id\"])):\n",
    "    box = annotations[\"bbox\"][i]\n",
    "    class_idx = annotations[\"category\"][i]\n",
    "    x, y, w, h = tuple(box)\n",
    "    # æ£€æŸ¥åæ ‡æ˜¯å¦å·²å½’ä¸€åŒ–\n",
    "    if max(box) > 1.0:\n",
    "        # åæ ‡æœªå½’ä¸€åŒ–ï¼Œæ— éœ€é‡æ–°ç¼©æ”¾\n",
    "        x1, y1 = int(x), int(y)\n",
    "        x2, y2 = int(x + w), int(y + h)\n",
    "    else:\n",
    "        # åæ ‡å·²å½’ä¸€åŒ–ï¼Œé‡æ–°ç¼©æ”¾\n",
    "        x1 = int(x * width)\n",
    "        y1 = int(y * height)\n",
    "        x2 = int((x + w) * width)\n",
    "        y2 = int((y + h) * height)\n",
    "    draw.rectangle((x, y, x + w, y + h), outline=\"red\", width=1)\n",
    "    draw.text((x, y), id2label[class_idx], fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f70691",
   "metadata": {},
   "source": [
    "\n",
    "ä¸ºäº†å¯è§†åŒ–å¸¦æœ‰å…³è”æ ‡ç­¾çš„è¾¹ç•Œæ¡†ï¼Œä½ å¯ä»¥ä»æ•°æ®é›†çš„å…ƒæ•°æ®ä¸­è·å–æ ‡ç­¾ï¼Œç‰¹åˆ«æ˜¯ `category` å­—æ®µã€‚ä½ è¿˜å¸Œæœ›åˆ›å»ºå°†æ ‡ç­¾ ID æ˜ å°„åˆ°æ ‡ç­¾ç±»çš„å­—å…¸ï¼ˆ`id2label`ï¼‰å’Œåå‘æ˜ å°„çš„å­—å…¸ï¼ˆ`label2id`ï¼‰ã€‚ç¨åè®¾ç½®æ¨¡å‹æ—¶å¯ä»¥ä½¿ç”¨è¿™äº›æ˜ å°„ã€‚åŒ…æ‹¬è¿™äº›æ˜ å°„å¯ä»¥ä½¿ä½ çš„æ¨¡å‹åœ¨ä¸Šä¼ åˆ° Hugging Face Hub åæ›´å…·å¯é‡ç”¨æ€§ã€‚è¯·æ³¨æ„ï¼Œä¸Šè¿°ä»£ç ä¸­ç»˜åˆ¶è¾¹ç•Œæ¡†çš„éƒ¨åˆ†å‡è®¾å®ƒä»¬æ˜¯ COCO æ ¼å¼ `(x_min, y_min, width, height)`ã€‚å¦‚æœè¦æ”¯æŒå…¶ä»–æ ¼å¼ï¼ˆå¦‚ `(x_min, y_min, x_max, y_max)`ï¼‰ï¼Œéœ€è¦è¿›è¡Œè°ƒæ•´ã€‚\n",
    "\n",
    "ä½œä¸ºç†Ÿæ‚‰æ•°æ®çš„æœ€åä¸€æ­¥ï¼Œæ¢ç´¢æ•°æ®ä¸­çš„æ½œåœ¨é—®é¢˜ã€‚ç›®æ ‡æ£€æµ‹æ•°æ®é›†ä¸­å¸¸è§çš„é—®é¢˜æ˜¯è¾¹ç•Œæ¡†â€œè¶…å‡ºâ€å›¾åƒè¾¹ç¼˜ã€‚è¿™ç§â€œè¶Šç•Œâ€çš„è¾¹ç•Œæ¡†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½ä¼šå¼•å‘é”™è¯¯ï¼Œåº”è¯¥è§£å†³ã€‚æ­¤æ•°æ®é›†ä¸­æœ‰å‡ ä¸ªè¿™æ ·çš„ç¤ºä¾‹ã€‚ä¸ºäº†ç®€åŒ–æœ¬æŒ‡å—ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹é¢çš„è½¬æ¢ä¸­è®¾ç½® `clip=True`ã€‚\n",
    "\n",
    "## é¢„å¤„ç†æ•°æ®\n",
    "\n",
    "ä¸ºäº†å¾®è°ƒæ¨¡å‹ï¼Œå¿…é¡»å¯¹è®¡åˆ’ä½¿ç”¨çš„æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä½¿å…¶å®Œå…¨ç¬¦åˆé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚[AutoImageProcessor](/docs/transformers/v4.46.0/en/model_doc/auto#transformers.AutoImageProcessor) ä¼šå¤„ç†å›¾åƒæ•°æ®ï¼Œç”Ÿæˆ DETR æ¨¡å‹å¯ä»¥è®­ç»ƒçš„ `pixel_values`ã€`pixel_mask` å’Œ `labels`ã€‚å›¾åƒå¤„ç†å™¨æœ‰ä¸€äº›å±æ€§ä½ ä¸éœ€è¦æ‹…å¿ƒï¼š\n",
    "\n",
    "- `image_mean = [0.485, 0.456, 0.406 ]`\n",
    "- `image_std = [0.229, 0.224, 0.225]`\n",
    "\n",
    "è¿™äº›æ˜¯åœ¨æ¨¡å‹é¢„è®­ç»ƒæœŸé—´ç”¨äºå½’ä¸€åŒ–å›¾åƒçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚è¿™äº›å€¼åœ¨è¿›è¡Œæ¨ç†æˆ–å¾®è°ƒé¢„è®­ç»ƒå›¾åƒæ¨¡å‹æ—¶è‡³å…³é‡è¦ã€‚\n",
    "\n",
    "ä»ä¸è¦å¾®è°ƒçš„æ¨¡å‹ç›¸åŒçš„æ£€æŸ¥ç‚¹å®ä¾‹åŒ–å›¾åƒå¤„ç†å™¨ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c160aeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "\n",
    "MAX_SIZE = IMAGE_SIZE\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    do_resize=True,\n",
    "    size={\"max_height\": MAX_SIZE, \"max_width\": MAX_SIZE},\n",
    "    do_pad=True,\n",
    "    pad_size={\"height\": MAX_SIZE, \"width\": MAX_SIZE},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fea8399",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨å°†å›¾åƒä¼ é€’ç»™ `image_processor` ä¹‹å‰ï¼Œå¯¹æ•°æ®é›†åº”ç”¨ä¸¤ä¸ªé¢„å¤„ç†è½¬æ¢ï¼š\n",
    "\n",
    "1. å¢å¼ºå›¾åƒ\n",
    "2. è°ƒæ•´æ³¨é‡Šä»¥æ»¡è¶³ DETR çš„æœŸæœ›\n",
    "\n",
    "é¦–å…ˆï¼Œä¸ºäº†é¿å…æ¨¡å‹åœ¨è®­ç»ƒæ•°æ®ä¸Šè¿‡æ‹Ÿåˆï¼Œå¯ä»¥ä½¿ç”¨ä»»ä½•æ•°æ®å¢å¼ºåº“å¯¹å›¾åƒè¿›è¡Œå¢å¼ºã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ [Albumentations](https://albumentations.ai/docs/)ã€‚è¿™ä¸ªåº“ç¡®ä¿è½¬æ¢ä¼šå½±å“å›¾åƒå¹¶ç›¸åº”åœ°æ›´æ–°è¾¹ç•Œæ¡†ã€‚ğŸ¤— Datasets æ–‡æ¡£ä¸­æœ‰è¯¦ç»†æŒ‡å—ä»‹ç»å¦‚ä½•ä¸ºå¯¹è±¡æ£€æµ‹å¢å¼ºå›¾åƒï¼Œå¹¶ä½¿ç”¨äº†ç›¸åŒçš„ç¤ºä¾‹æ•°æ®é›†ã€‚å¯¹å›¾åƒåº”ç”¨ä¸€äº›å‡ ä½•å’Œé¢œè‰²å˜æ¢ã€‚æœ‰å…³æ›´å¤šå¢å¼ºé€‰é¡¹ï¼Œå¯ä»¥æ¢ç´¢ [Albumentations ç¤ºä¾‹ç©ºé—´](https://huggingface.co/spaces/qubvel-hf/albumentations-demo)ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8615db7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import albumentations as A\n",
    "\n",
    "train_augment_and_transform = A.Compose(\n",
    "    [\n",
    "        A.Perspective(p=0.1),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(p=0.5),\n",
    "        A.HueSaturationValue(p=0.1),\n",
    "    ],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True, min_area=25),\n",
    ")\n",
    "\n",
    "validation_transform = A.Compose(\n",
    "    [A.NoOp()],\n",
    "    bbox_params=A.BboxParams(format=\"coco\", label_fields=[\"category\"], clip=True),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99aa5f9",
   "metadata": {},
   "source": [
    "\n",
    "`image_processor` æœŸæœ›æ³¨é‡Šçš„æ ¼å¼å¦‚ä¸‹ï¼š`{'image_id': int, 'annotations': List[Dict]}`ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸æ˜¯ä¸€ä¸ª COCO å¯¹è±¡æ³¨é‡Šã€‚æ·»åŠ ä¸€ä¸ªå‡½æ•°æ¥é‡æ–°æ ¼å¼åŒ–å•ä¸ªç¤ºä¾‹çš„æ³¨é‡Šï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2690d40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_image_annotations_as_coco(image_id, categories, areas, bboxes):\n",
    "    \"\"\"å°†ä¸€ç»„å›¾åƒæ³¨é‡Šæ ¼å¼åŒ–ä¸º COCO æ ¼å¼\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        image_id (str): å›¾åƒ IDã€‚ä¾‹å¦‚ï¼š\"0001\"\n",
    "        categories (List[int]): ä¸æä¾›çš„è¾¹ç•Œæ¡†å¯¹åº”çš„ç±»åˆ«/ç±»æ ‡ç­¾åˆ—è¡¨\n",
    "        areas (List[float]): ä¸æä¾›çš„è¾¹ç•Œæ¡†å¯¹åº”çš„é¢ç§¯åˆ—è¡¨\n",
    "        bboxes (List[Tuple[float]]): ä»¥ COCO æ ¼å¼æä¾›çš„è¾¹ç•Œæ¡†åˆ—è¡¨\n",
    "            ([center_x, center_y, width, height] ä¸ºç»å¯¹åæ ‡)\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        dict: {\n",
    "            \"image_id\": å›¾åƒ ID,\n",
    "            \"annotations\": æ ¼å¼åŒ–çš„æ³¨é‡Šåˆ—è¡¨\n",
    "        }\n",
    "    \"\"\"\n",
    "    annotations = []\n",
    "    for category, area, bbox in zip(categories, areas, bboxes):\n",
    "        formatted_annotation = {\n",
    "            \"image_id\": image_id,\n",
    "            \"category_id\": category,\n",
    "            \"iscrowd\": 0,\n",
    "            \"area\": area,\n",
    "            \"bbox\": list(bbox),\n",
    "        }\n",
    "        annotations.append(formatted_annotation)\n",
    "\n",
    "    return {\n",
    "        \"image_id\": image_id,\n",
    "        \"annotations\": annotations,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f796acdc",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨å¯ä»¥ç»„åˆå›¾åƒå’Œæ³¨é‡Šå˜æ¢ï¼Œä»¥ä¾¿åœ¨ä¸€æ‰¹ç¤ºä¾‹ä¸Šä½¿ç”¨ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c7fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_and_transform_batch(examples, transform, image_processor, return_pixel_mask=False):\n",
    "    \"\"\"å¯¹å¯¹è±¡æ£€æµ‹ä»»åŠ¡åº”ç”¨å¢å¼ºå¹¶æ ¼å¼åŒ–æ³¨é‡Šä¸º COCO æ ¼å¼\"\"\"\n",
    "\n",
    "    images = []\n",
    "    annotations = []\n",
    "    for image_id, image, objects in zip(examples[\"image_id\"], examples[\"image\"], examples[\"objects\"]):\n",
    "        image = np.array(image.convert(\"RGB\"))\n",
    "\n",
    "        # åº”ç”¨å¢å¼º\n",
    "        output = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category\"])\n",
    "        images.append(output[\"image\"])\n",
    "\n",
    "        # å°†æ³¨é‡Šæ ¼å¼åŒ–ä¸º COCO æ ¼å¼\n",
    "        formatted_annotations = format_image_annotations_as_coco(\n",
    "            image_id, output[\"category\"], objects[\"area\"], output[\"bboxes\"]\n",
    "        )\n",
    "        annotations.append(formatted_annotations)\n",
    "\n",
    "    # åº”ç”¨å›¾åƒå¤„ç†å™¨å˜æ¢ï¼šè°ƒæ•´å¤§å°ã€é‡æ–°ç¼©æ”¾ã€å½’ä¸€åŒ–\n",
    "    result = image_processor(images=images, annotations=annotations, return_tensors=\"pt\")\n",
    "\n",
    "    if not return_pixel_mask:\n",
    "        result.pop(\"pixel_mask\", None)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685968f3",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ ğŸ¤— Datasets [with_transform](https://huggingface.co/docs/datasets/v3.0.2/en/package_reference/main_classes#datasets.Dataset.with_transform) æ–¹æ³•å°†æ­¤é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚æ­¤æ–¹æ³•ä¼šåœ¨åŠ è½½æ•°æ®é›†å…ƒç´ æ—¶åŠ¨æ€åº”ç”¨è½¬æ¢ã€‚\n",
    "\n",
    "æ­¤æ—¶ï¼Œå¯ä»¥æ£€æŸ¥è½¬æ¢åçš„æ•°æ®é›†ç¤ºä¾‹ã€‚ä½ åº”è¯¥çœ‹åˆ°ä¸€ä¸ªåŒ…å« `pixel_values` çš„å¼ é‡ã€ä¸€ä¸ªåŒ…å« `pixel_mask` çš„å¼ é‡å’Œ `labels`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b107bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "# ä¸ºæ‰¹å¤„ç†åˆ›å»ºè½¬æ¢å‡½æ•°å¹¶åº”ç”¨äºæ•°æ®é›†åˆ‡ç‰‡\n",
    "train_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=train_augment_and_transform, image_processor=image_processor\n",
    ")\n",
    "validation_transform_batch = partial(\n",
    "    augment_and_transform_batch, transform=validation_transform, image_processor=image_processor\n",
    ")\n",
    "\n",
    "cppe5[\"train\"] = cppe5[\"train\"].with_transform(train_transform_batch)\n",
    "cppe5[\"validation\"] = cppe5[\"validation\"].with_transform(validation_transform_batch)\n",
    "cppe5[\"test\"] = cppe5[\"test\"].with_transform(validation_transform_batch)\n",
    "\n",
    "cppe5[\"train\"][15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2959a6b9",
   "metadata": {},
   "source": [
    "\n",
    "ä½ å·²ç»æˆåŠŸå¢å¼ºäº†å•ä¸ªå›¾åƒå¹¶å‡†å¤‡äº†å®ƒä»¬çš„æ³¨é‡Šã€‚ä½†æ˜¯ï¼Œé¢„å¤„ç†å°šæœªå®Œæˆã€‚åœ¨æœ€åä¸€æ­¥ä¸­ï¼Œåˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ `collate_fn` æ¥æ‰¹é‡å¤„ç†å›¾åƒã€‚å°†å›¾åƒï¼ˆç°ä¸º `pixel_values`ï¼‰å¡«å……åˆ°æ‰¹æ¬¡ä¸­æœ€å¤§çš„å›¾åƒï¼Œå¹¶åˆ›å»ºç›¸åº”çš„ `pixel_mask` ä»¥æŒ‡ç¤ºå“ªäº›åƒç´ æ˜¯çœŸå®çš„ï¼ˆ1ï¼‰ï¼Œå“ªäº›æ˜¯å¡«å……çš„ï¼ˆ0ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e640f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def collate_fn(batch):\n",
    "    data = {}\n",
    "    data[\"pixel_values\"] = torch.stack([x[\"pixel_values\"] for x in batch])\n",
    "    data[\"labels\"] = [x[\"labels\"] for x in batch]\n",
    "    if \"pixel_mask\" in batch[0]:\n",
    "        data[\"pixel_mask\"] = torch.stack([x[\"pixel_mask\"] for x in batch])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f82bcf6",
   "metadata": {},
   "source": [
    "\n",
    "## å‡†å¤‡è®¡ç®— mAP çš„å‡½æ•°\n",
    "\n",
    "å¯¹è±¡æ£€æµ‹æ¨¡å‹é€šå¸¸ä½¿ç”¨ä¸€ç»„ [COCO é£æ ¼çš„æŒ‡æ ‡](https://cocodataset.org/#detection-eval) è¿›è¡Œè¯„ä¼°ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ `torchmetrics` è®¡ç®— `mAP`ï¼ˆå¹³å‡ç²¾åº¦ï¼‰å’Œ `mAR`ï¼ˆå¹³å‡å¬å›ç‡ï¼‰æŒ‡æ ‡ï¼Œå¹¶å°†å…¶åŒ…è£…åˆ° `compute_metrics` å‡½æ•°ä¸­ï¼Œä»¥ä¾¿åœ¨ [Trainer](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer) ä¸­è¿›è¡Œè¯„ä¼°ã€‚\n",
    "\n",
    "ç”¨äºè®­ç»ƒçš„ä¸­é—´æ ¼å¼çš„æ¡†æ˜¯ `YOLO`ï¼ˆå½’ä¸€åŒ–ï¼‰ï¼Œä½†æˆ‘ä»¬å°†è®¡ç®— `Pascal VOC`ï¼ˆç»å¯¹åæ ‡ï¼‰æ ¼å¼çš„æ¡†ï¼Œä»¥æ­£ç¡®å¤„ç†æ¡†çš„é¢ç§¯ã€‚å®šä¹‰ä¸€ä¸ªå‡½æ•°å°†è¾¹ç•Œæ¡†è½¬æ¢ä¸º `Pascal VOC` æ ¼å¼ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8130c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.image_transforms import center_to_corners_format\n",
    "\n",
    "def convert_bbox_yolo_to_pascal(boxes, image_size):\n",
    "    \"\"\"\n",
    "    å°†è¾¹ç•Œæ¡†ä» YOLO æ ¼å¼ (x_center, y_center, width, height) è½¬æ¢ä¸º Pascal VOC æ ¼å¼ (x_min, y_min, x_max, y_max)ï¼Œå¹¶åœ¨ç»å¯¹åæ ‡ä¸­è¡¨ç¤ºã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        boxes (torch.Tensor): ä»¥ YOLO æ ¼å¼çš„è¾¹ç•Œæ¡†\n",
    "        image_size (Tuple[int, int]): å›¾åƒå°ºå¯¸ï¼Œæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        torch.Tensor: ä»¥ Pascal VOC æ ¼å¼ (x_min, y_min, x_max, y_max) çš„è¾¹ç•Œæ¡†\n",
    "    \"\"\"\n",
    "    # è½¬æ¢ä¸ºä¸­å¿ƒåˆ°è§’ç‚¹æ ¼å¼\n",
    "    boxes = center_to_corners_format(boxes)\n",
    "\n",
    "    # è½¬æ¢ä¸ºç»å¯¹åæ ‡\n",
    "    height, width = image_size\n",
    "    boxes = boxes * torch.tensor([[width, height, width, height]])\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f2d9f",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œåœ¨ `compute_metrics` å‡½æ•°ä¸­æ”¶é›†è¯„ä¼°å¾ªç¯ç»“æœä¸­çš„é¢„æµ‹å’Œç›®æ ‡è¾¹ç•Œæ¡†ã€å¾—åˆ†å’Œæ ‡ç­¾ï¼Œå¹¶ä¼ é€’ç»™è¯„åˆ†å‡½æ•°ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f10aed29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "\n",
    "@dataclass\n",
    "class ModelOutput:\n",
    "    logits: torch.Tensor\n",
    "    pred_boxes: torch.Tensor\n",
    "\n",
    "@torch.no_grad()\n",
    "def compute_metrics(evaluation_results, image_processor, threshold=0.0, id2label=None):\n",
    "    \"\"\"\n",
    "    è®¡ç®—å¯¹è±¡æ£€æµ‹ä»»åŠ¡çš„å¹³å‡ mAPã€mAR åŠå…¶å˜ä½“ã€‚\n",
    "\n",
    "    å‚æ•°ï¼š\n",
    "        evaluation_results (EvalPrediction): è¯„ä¼°çš„é¢„æµ‹å’Œç›®æ ‡ã€‚\n",
    "        threshold (float, optional): é€šè¿‡ç½®ä¿¡åº¦ç­›é€‰é¢„æµ‹æ¡†çš„é˜ˆå€¼ã€‚é»˜è®¤ä¸º 0.0ã€‚\n",
    "        id2label (Optional[dict], optional): ç±»åˆ« ID åˆ°ç±»åˆ«åç§°çš„æ˜ å°„ã€‚é»˜è®¤ä¸º Noneã€‚\n",
    "\n",
    "    è¿”å›ï¼š\n",
    "        Mapping[str, float]: ä»¥å­—å…¸å½¢å¼è¡¨ç¤ºçš„æŒ‡æ ‡ {<metric_name>: <metric_value>}\n",
    "    \"\"\"\n",
    "\n",
    "    predictions, targets = evaluation_results.predictions, evaluation_results.label_ids\n",
    "\n",
    "    # ä¸ºæŒ‡æ ‡è®¡ç®—æä¾›ï¼š\n",
    "    #  - ç›®æ ‡ï¼šåˆ—è¡¨ä¸­çš„å­—å…¸ï¼Œé”®ä¸º \"boxes\" å’Œ \"labels\"\n",
    "    #  - é¢„æµ‹ï¼šåˆ—è¡¨ä¸­çš„å­—å…¸ï¼Œé”®ä¸º \"boxes\"ã€\"scores\" å’Œ \"labels\"\n",
    "\n",
    "    image_sizes = []\n",
    "    post_processed_targets = []\n",
    "    post_processed_predictions = []\n",
    "\n",
    "    # æ”¶é›†ç”¨äºæŒ‡æ ‡è®¡ç®—çš„ç›®æ ‡\n",
    "    for batch in targets:\n",
    "        # æ”¶é›†å›¾åƒå°ºå¯¸ï¼Œç”¨äºé¢„æµ‹åå¤„ç†\n",
    "        batch_image_sizes = torch.tensor(np.array([x[\"orig_size\"] for x in batch]))\n",
    "        image_sizes.append(batch_image_sizes)\n",
    "        # æ”¶é›†ç›®æ ‡ï¼Œç”¨äºæŒ‡æ ‡è®¡ç®—\n",
    "        # æ¡†å·²è¢«è½¬æ¢ä¸º YOLO æ ¼å¼ï¼Œç”¨äºæ¨¡å‹è®­ç»ƒ\n",
    "        # è¿™é‡Œå°†å®ƒä»¬è½¬æ¢ä¸º Pascal VOC æ ¼å¼ (x_min, y_min, x_max, y_max)\n",
    "        for image_target in batch:\n",
    "            boxes = torch.tensor(image_target[\"boxes\"])\n",
    "            boxes = convert_bbox_yolo_to_pascal(boxes, image_target[\"orig_size\"])\n",
    "            labels = torch.tensor(image_target[\"class_labels\"])\n",
    "            post_processed_targets.append({\"boxes\": boxes, \"labels\": labels})\n",
    "\n",
    "    # æ”¶é›†ç”¨äºæŒ‡æ ‡è®¡ç®—çš„é¢„æµ‹ï¼Œ\n",
    "    # æ¨¡å‹ç”Ÿæˆçš„æ¡†ä¸º YOLO æ ¼å¼ï¼Œç„¶åå›¾åƒå¤„ç†å™¨å°†å…¶è½¬æ¢ä¸º Pascal VOC æ ¼å¼\n",
    "    for batch, target_sizes in zip(predictions, image_sizes):\n",
    "        batch_logits, batch_boxes = batch[1], batch[2]\n",
    "        output = ModelOutput(logits=torch.tensor(batch_logits), pred_boxes=torch.tensor(batch_boxes))\n",
    "        post_processed_output = image_processor.post_process_object_detection(\n",
    "            output, threshold=threshold, target_sizes=target_sizes\n",
    "        )\n",
    "        post_processed_predictions.extend(post_processed_output)\n",
    "\n",
    "    # è®¡ç®—æŒ‡æ ‡\n",
    "    metric = MeanAveragePrecision(box_format=\"xyxy\", class_metrics=True)\n",
    "    metric.update(post_processed_predictions, post_processed_targets)\n",
    "    metrics = metric.compute()\n",
    "\n",
    "    # å°†æ¯ç±»æŒ‡æ ‡çš„åˆ—è¡¨æ›¿æ¢ä¸ºæ¯ç±»çš„å•ç‹¬æŒ‡æ ‡\n",
    "    classes = metrics.pop(\"classes\")\n",
    "    map_per_class = metrics.pop(\"map_per_class\")\n",
    "    mar_100_per_class = metrics.pop(\"mar_100_per_class\")\n",
    "    for class_id, class_map, class_mar in zip(classes, map_per_class, mar_100_per_class):\n",
    "        class_name = id2label[class_id.item()] if id2label is not None else class_id.item()\n",
    "        metrics[f\"map_{class_name}\"] = class_map\n",
    "        metrics[f\"mar_100_{class_name}\"] = class_mar\n",
    "\n",
    "    metrics = {k: round(v.item(), 4) for k, v in metrics.items()}\n",
    "\n",
    "    return metrics\n",
    "\n",
    "eval_compute_metrics_fn = partial(\n",
    "    compute_metrics, image_processor=image_processor, id2label=id2label, threshold=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40578c11",
   "metadata": {},
   "source": [
    "\n",
    "## è®­ç»ƒæ£€æµ‹æ¨¡å‹\n",
    "\n",
    "åœ¨å‰å‡ èŠ‚ä¸­ï¼Œä½ å·²ç»å®Œæˆäº†å¤§éƒ¨åˆ†ç¹é‡çš„å·¥ä½œï¼Œç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹äº†ï¼å³ä½¿ç»è¿‡è°ƒæ•´å¤§å°åï¼Œè¯¥æ•°æ®é›†ä¸­çš„å›¾åƒä»ç„¶ç›¸å½“å¤§ã€‚è¿™æ„å‘³ç€å¾®è°ƒæ­¤æ¨¡å‹è‡³å°‘éœ€è¦ä¸€ä¸ª GPUã€‚\n",
    "\n",
    "è®­ç»ƒæ¶‰åŠä»¥ä¸‹æ­¥éª¤ï¼š\n",
    "\n",
    "1. ä½¿ç”¨ [AutoModelForObjectDetection](/docs/transformers/v4.46.0/en/model_doc/auto#transformers.AutoModelForObjectDetection) ä»é¢„å¤„ç†ä¸­ä½¿ç”¨çš„ç›¸åŒæ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹ã€‚\n",
    "2. åœ¨ [TrainingArguments](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments) ä¸­å®šä¹‰è®­ç»ƒè¶…å‚æ•°ã€‚\n",
    "3. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ [Trainer](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer)ï¼Œå¹¶é™„å¸¦æ¨¡å‹ã€æ•°æ®é›†ã€å›¾åƒå¤„ç†å™¨å’Œæ•°æ®ç»„åˆå™¨ã€‚\n",
    "4. è°ƒç”¨ [train()](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer.train) ä»¥å¾®è°ƒæ¨¡å‹ã€‚\n",
    "\n",
    "ä»ç”¨äºé¢„å¤„ç†çš„ç›¸åŒæ£€æŸ¥ç‚¹åŠ è½½æ¨¡å‹æ—¶ï¼Œè®°å¾—ä¼ é€’ä»æ•°æ®é›†å…ƒæ•°æ®ä¸­åˆ›å»ºçš„ `label2id` å’Œ `id2label` æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŒ‡å®š `ignore_mismatched_sizes=True` ä»¥æ›¿æ¢ç°æœ‰çš„åˆ†ç±»å¤´ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ea571f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForObjectDetection\n",
    "\n",
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    "    ignore_mismatched_sizes=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc2708b",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨ [TrainingArguments](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments) ä¸­ä½¿ç”¨ `output_dir` æŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ï¼Œç„¶åæ ¹æ®éœ€è¦é…ç½®è¶…å‚æ•°ã€‚å¯¹äº `num_train_epochs=30`ï¼Œåœ¨ Google Colab T4 GPU ä¸Šè®­ç»ƒå¤§çº¦éœ€è¦ 35 åˆ†é’Ÿï¼Œå¢åŠ è®­ç»ƒè½®æ•°å¯ä»¥è·å¾—æ›´å¥½çš„ç»“æœã€‚\n",
    "\n",
    "é‡è¦è¯´æ˜ï¼š\n",
    "\n",
    "- ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™å°†åˆ é™¤å›¾åƒåˆ—ã€‚æ²¡æœ‰å›¾åƒåˆ—ï¼Œä½ æ— æ³•åˆ›å»º `pixel_values`ã€‚å› æ­¤ï¼Œå°† `remove_unused_columns` è®¾ç½®ä¸º `False`ã€‚\n",
    "- è®¾ç½® `eval_do_concat_batches=False` ä»¥è·å¾—æ­£ç¡®çš„è¯„ä¼°ç»“æœã€‚å›¾åƒå…·æœ‰ä¸åŒæ•°é‡çš„ç›®æ ‡æ¡†ï¼Œå¦‚æœæ‰¹æ¬¡è¢«è¿æ¥èµ·æ¥ï¼Œæˆ‘ä»¬å°†æ— æ³•ç¡®å®šå“ªä¸ªæ¡†å±äºç‰¹å®šå›¾åƒã€‚\n",
    "\n",
    "å¦‚æœä½ å¸Œæœ›å°†æ¨¡å‹æ¨é€åˆ° Hubï¼Œå°† `push_to_hub` è®¾ç½®ä¸º `True`ï¼ˆä½ å¿…é¡»ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37a280",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"detr_finetuned_cppe5\",\n",
    "    num_train_epochs=30,\n",
    "    fp16=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    dataloader_num_workers=4,\n",
    "    learning_rate=5e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=1e-4,\n",
    "    max_grad_norm=0.01,\n",
    "    metric_for_best_model=\"eval_map\",\n",
    "    greater_is_better=True,\n",
    "    load_best_model_at_end=True,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    eval_do_concat_batches=False,\n",
    "    push_to_hub=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f33748",
   "metadata": {},
   "source": [
    "\n",
    "æœ€åï¼Œå°†æ‰€æœ‰å†…å®¹æ•´åˆåœ¨ä¸€èµ·ï¼Œå¹¶è°ƒç”¨ [train()](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer.train)ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e46d474",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=cppe5[\"train\"],\n",
    "    eval_dataset=cppe5[\"validation\"],\n",
    "    processing_class=image_processor,\n",
    "    data_collator=collate_fn,\n",
    "    compute_metrics=eval_compute_metrics_fn,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "781d31db",
   "metadata": {},
   "source": [
    "\n",
    "å¦‚æœä½ åœ¨ `training_args` ä¸­è®¾ç½®äº† `push_to_hub` ä¸º `True`ï¼Œè®­ç»ƒæ£€æŸ¥ç‚¹å°†æ¨é€åˆ° Hugging Face Hubã€‚è®­ç»ƒå®Œæˆåï¼Œé€šè¿‡è°ƒç”¨ [push_to_hub()](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.Trainer.push_to_hub) æ–¹æ³•æ¨é€æœ€ç»ˆæ¨¡å‹åˆ° Hubã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "889a11b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051dc110",
   "metadata": {},
   "source": [
    "\n",
    "## è¯„ä¼°\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbceacf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "metrics = trainer.evaluate(eval_dataset=cppe5[\"test\"], metric_key_prefix=\"test\")\n",
    "pprint(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439b63e3",
   "metadata": {},
   "source": [
    "\n",
    "è¿™äº›ç»“æœå¯ä»¥é€šè¿‡è°ƒæ•´ [TrainingArguments](/docs/transformers/v4.46.0/en/main_classes/trainer#transformers.TrainingArguments) ä¸­çš„è¶…å‚æ•°è¿›ä¸€æ­¥æ”¹è¿›ã€‚è¯•è¯•çœ‹ï¼\n",
    "\n",
    "## æ¨ç†\n",
    "\n",
    "ç°åœ¨ä½ å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œè¿›è¡Œäº†è¯„ä¼°ï¼Œå¹¶å°†å…¶ä¸Šä¼ åˆ°äº† Hugging Face Hubï¼Œä½ å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7283e561",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import requests\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "\n",
    "url = \"https://images.pexels.com/photos/8413299/pexels-photo-8413299.jpeg?auto=compress&cs=tinysrgb&w=630&h=375&dpr=2\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c9db4a",
   "metadata": {},
   "source": [
    "\n",
    "ä» Hugging Face Hub åŠ è½½æ¨¡å‹å’Œå›¾åƒå¤„ç†å™¨ï¼ˆå¦‚æœè¦åœ¨å½“å‰ä¼šè¯ä¸­ä½¿ç”¨å·²è®­ç»ƒçš„æ¨¡å‹ï¼Œå¯ä»¥è·³è¿‡è¿™ä¸€æ­¥ï¼‰ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5d8102",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\"\n",
    "model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n",
    "\n",
    "image_processor = AutoImageProcessor.from_pretrained(model_repo)\n",
    "model = AutoModelForObjectDetection.from_pretrained(model_repo)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7dbd6e",
   "metadata": {},
   "source": [
    "\n",
    "æ£€æµ‹è¾¹ç•Œæ¡†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c210051e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = image_processor(images=[image], return_tensors=\"pt\")\n",
    "    outputs = model(**inputs.to(device))\n",
    "    target_sizes = torch.tensor([[image.size[1], image.size[0]]])\n",
    "    results = image_processor.post_process_object_detection(outputs, threshold=0.3, target_sizes=target_sizes)[0]\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    print(\n",
    "        f\"Detected {model.config.id2label[label.item()]} with confidence \"\n",
    "        f\"{round(score.item(), 3)} at location {box}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1adacf",
   "metadata": {},
   "source": [
    "\n",
    "è®©æˆ‘ä»¬ç»˜åˆ¶ç»“æœï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2fd235b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n",
    "    box = [round(i, 2) for i in box.tolist()]\n",
    "    x, y, x2, y2 = tuple(box)\n",
    "    draw.rectangle((x, y, x2, y2), outline=\"red\", width=1)\n",
    "    draw.text((x, y), model.config.id2label[label.item()], fill=\"white\")\n",
    "\n",
    "image"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
