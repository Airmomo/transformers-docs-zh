{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5fab6ed",
   "metadata": {},
   "source": [
    "# å›¾åƒåˆ†ç±»\n",
    "\n",
    "å›¾åƒåˆ†ç±»æ˜¯ä¸ºå›¾åƒåˆ†é…æ ‡ç­¾æˆ–ç±»åˆ«çš„ä»»åŠ¡ã€‚ä¸æ–‡æœ¬æˆ–éŸ³é¢‘åˆ†ç±»ä¸åŒï¼Œå›¾åƒåˆ†ç±»çš„è¾“å…¥æ˜¯æ„æˆå›¾åƒçš„åƒç´ å€¼ã€‚å›¾åƒåˆ†ç±»æœ‰è®¸å¤šåº”ç”¨ï¼Œä¾‹å¦‚åœ¨è‡ªç„¶ç¾å®³åæ£€æµ‹æŸåæƒ…å†µã€ç›‘æµ‹ä½œç‰©å¥åº·çŠ¶å†µæˆ–å¸®åŠ©ç­›é€‰åŒ»å­¦å›¾åƒä»¥å¯»æ‰¾ç–¾ç—…çš„è¿¹è±¡ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—å°†è¯´æ˜å¦‚ä½•ï¼š\n",
    "\n",
    "1. åœ¨ [Food-101](https://huggingface.co/datasets/food101) æ•°æ®é›†ä¸Šå¾®è°ƒ [ViT](model_doc/vit) æ¨¡å‹ï¼Œä»¥å¯¹å›¾åƒä¸­çš„é£Ÿç‰©é¡¹è¿›è¡Œåˆ†ç±»ã€‚\n",
    "2. ä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "è¦æŸ¥çœ‹æ‰€æœ‰ä¸è¯¥ä»»åŠ¡å…¼å®¹çš„æ¶æ„å’Œæ£€æŸ¥ç‚¹ï¼Œæˆ‘ä»¬å»ºè®®æŸ¥çœ‹ [ä»»åŠ¡é¡µé¢](https://huggingface.co/tasks/image-classification)ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f54240",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate accelerate pillow torchvision scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abee4597",
   "metadata": {},
   "source": [
    "\n",
    "æˆ‘ä»¬é¼“åŠ±æ‚¨ç™»å½•æ‚¨çš„ Hugging Face è´¦æˆ·ï¼Œä¸Šä¼ å¹¶ä¸ç¤¾åŒºåˆ†äº«æ‚¨çš„æ¨¡å‹ã€‚å½“æç¤ºæ—¶ï¼Œè¾“å…¥æ‚¨çš„ä»¤ç‰Œä»¥ç™»å½•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b707a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70cfb5e",
   "metadata": {},
   "source": [
    "\n",
    "## åŠ è½½ Food-101 æ•°æ®é›†\n",
    "\n",
    "é¦–å…ˆï¼Œä» ğŸ¤— Datasets åº“ä¸­åŠ è½½ Food-101 æ•°æ®é›†çš„ä¸€ä¸ªè¾ƒå°çš„å­é›†ã€‚è¿™å°†è®©æ‚¨æœ‰æœºä¼šè¿›è¡Œå®éªŒï¼Œå¹¶ç¡®ä¿ä¸€åˆ‡æ­£å¸¸å·¥ä½œï¼Œç„¶åå†åœ¨å®Œæ•´æ•°æ®é›†ä¸ŠèŠ±è´¹æ›´å¤šæ—¶é—´è¿›è¡Œè®­ç»ƒã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11da69e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "food = load_dataset(\"food101\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "545b7661",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) æ–¹æ³•å°†æ•°æ®é›†çš„ `train` åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33eac54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d225f2a2",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åæŸ¥çœ‹ä¸€ä¸ªç¤ºä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c538a92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d8e20f",
   "metadata": {},
   "source": [
    "\n",
    "æ•°æ®é›†ä¸­çš„æ¯ä¸ªç¤ºä¾‹éƒ½æœ‰ä¸¤ä¸ªå­—æ®µï¼š\n",
    "\n",
    "- `image`ï¼šé£Ÿç‰©é¡¹çš„ PIL å›¾åƒ\n",
    "- `label`ï¼šé£Ÿç‰©é¡¹çš„æ ‡ç­¾ç±»åˆ«\n",
    "\n",
    "ä¸ºäº†ä½¿æ¨¡å‹èƒ½å¤Ÿä»æ ‡ç­¾ ID è·å–æ ‡ç­¾åç§°ï¼Œåˆ›å»ºä¸€ä¸ªå°†æ ‡ç­¾åç§°æ˜ å°„åˆ°æ•´æ•°å’Œåä¹‹äº¦ç„¶çš„å­—å…¸ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d775d8af",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = food[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f841bb27",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ï¼Œæ‚¨å¯ä»¥å°†æ ‡ç­¾ ID è½¬æ¢ä¸ºæ ‡ç­¾åç§°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069de2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label[str(79)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2925a33a",
   "metadata": {},
   "source": [
    "\n",
    "## é¢„å¤„ç†\n",
    "\n",
    "ä¸‹ä¸€æ­¥æ˜¯åŠ è½½ ViT å›¾åƒå¤„ç†å™¨ï¼Œå°†å›¾åƒå¤„ç†æˆå¼ é‡ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2210f9",
   "metadata": {},
   "source": [
    "\n",
    "Pytorch\n",
    "\n",
    "å¯¹å›¾åƒåº”ç”¨ä¸€äº›å›¾åƒå˜æ¢ï¼Œä½¿æ¨¡å‹æ›´èƒ½æŠµæŠ—è¿‡æ‹Ÿåˆã€‚è¿™é‡Œæ‚¨å°†ä½¿ç”¨ torchvision çš„ `transforms` æ¨¡å—ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨æ‚¨å–œæ¬¢çš„ä»»ä½•å›¾åƒåº“ã€‚\n",
    "\n",
    "è£å‰ªå›¾åƒçš„éšæœºéƒ¨åˆ†ï¼Œè°ƒæ•´å…¶å¤§å°ï¼Œå¹¶ä½¿ç”¨å›¾åƒå‡å€¼å’Œæ ‡å‡†å·®è¿›è¡Œå½’ä¸€åŒ–ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90255a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import RandomResizedCrop, Compose, Normalize, ToTensor\n",
    "normalize = Normalize(mean=image_processor.image_mean, std=image_processor.image_std)\n",
    "size = (\n",
    "    image_processor.size[\"shortest_edge\"]\n",
    "    if \"shortest_edge\" in image_processor.size\n",
    "    else (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    ")\n",
    "_transforms = Compose([RandomResizedCrop(size), ToTensor(), normalize])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30dba3c",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶ååˆ›å»ºä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œåº”ç”¨å˜æ¢å¹¶è¿”å›å›¾åƒçš„ `pixel_values` - æ¨¡å‹çš„è¾“å…¥ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca4ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transforms(examples):\n",
    "    examples[\"pixel_values\"] = [_transforms(img.convert(\"RGB\")) for img in examples[\"image\"]]\n",
    "    del examples[\"image\"]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f325a32",
   "metadata": {},
   "source": [
    "\n",
    "è¦ä½¿ç”¨ ğŸ¤— Datasets çš„ [with_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.with_transform) æ–¹æ³•åœ¨æ•´ä¸ªæ•°æ®é›†ä¸Šåº”ç”¨é¢„å¤„ç†å‡½æ•°ã€‚å½“æ‚¨åŠ è½½æ•°æ®é›†çš„å…ƒç´ æ—¶ï¼Œå˜æ¢å°†åŠ¨æ€åº”ç”¨ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a36570e",
   "metadata": {},
   "outputs": [],
   "source": [
    "food = food.with_transform(transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3970018",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ï¼Œä½¿ç”¨ [DefaultDataCollator](/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚ä¸ ğŸ¤— Transformers ä¸­çš„å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œ`DefaultDataCollator` ä¸ä¼šåº”ç”¨é¢å¤–çš„é¢„å¤„ç†ï¼Œä¾‹å¦‚å¡«å……ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cf0363",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84cbee0f",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "ä¸ºäº†é¿å…è¿‡æ‹Ÿåˆå¹¶ä½¿æ¨¡å‹æ›´å¼ºå¤§ï¼Œè¯·åœ¨æ•°æ®é›†çš„è®­ç»ƒéƒ¨åˆ†æ·»åŠ ä¸€äº›æ•°æ®å¢å¼ºã€‚è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ Keras é¢„å¤„ç†å±‚æ¥å®šä¹‰è®­ç»ƒæ•°æ®çš„å˜æ¢ï¼ˆåŒ…æ‹¬æ•°æ®å¢å¼ºï¼‰ï¼Œä»¥åŠéªŒè¯æ•°æ®çš„å˜æ¢ï¼ˆä»…ä¸­å¿ƒè£å‰ªã€è°ƒæ•´å¤§å°å’Œå½’ä¸€åŒ–ï¼‰ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ `tf.image` æˆ–æ‚¨å–œæ¬¢çš„ä»»ä½•å…¶ä»–åº“ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0858ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n",
    "train_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(factor=0.02),\n",
    "        layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n",
    "    ],\n",
    "    name=\"train_data_augmentation\",\n",
    ")\n",
    "val_data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.CenterCrop(size[0], size[1]),\n",
    "        layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n",
    "    ],\n",
    "    name=\"val_data_augmentation\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d742642f",
   "metadata": {},
   "source": [
    "\n",
    "æ¥ä¸‹æ¥ï¼Œåˆ›å»ºå‡½æ•°ä»¥å¯¹ä¸€æ‰¹å›¾åƒè€Œä¸æ˜¯å•ä¸ªå›¾åƒåº”ç”¨é€‚å½“çš„å˜æ¢ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2bb39f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "def convert_to_tf_tensor(image: Image):\n",
    "    np_image = np.array(image)\n",
    "    tf_image = tf.convert_to_tensor(np_image)\n",
    "    # `expand_dims()` ç”¨äºæ·»åŠ ä¸€ä¸ªæ‰¹æ¬¡ç»´åº¦ï¼Œå› ä¸º\n",
    "    # TF å¢å¼ºå±‚å¯¹æ‰¹å¤„ç†è¾“å…¥è¿›è¡Œæ“ä½œã€‚\n",
    "    return tf.expand_dims(tf_image, 0)\n",
    "def preprocess_train(example_batch):\n",
    "    \"\"\"Apply train_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch\n",
    "def preprocess_val(example_batch):\n",
    "    \"\"\"Apply val_transforms across a batch.\"\"\"\n",
    "    images = [\n",
    "        val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n",
    "    ]\n",
    "    example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n",
    "    return example_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d11f0a",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ ğŸ¤— Datasets çš„ [set_transform](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.set_transform) æ–¹æ³•åœ¨åŠ è½½æ—¶åº”ç”¨å˜æ¢ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917a6fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "food[\"train\"].set_transform(preprocess_train)\n",
    "food[\"test\"].set_transform(preprocess_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d5e218",
   "metadata": {},
   "source": [
    "\n",
    "ä½œä¸ºé¢„å¤„ç†æ­¥éª¤çš„æœ€åä¸€æ­¥ï¼Œä½¿ç”¨ `DefaultDataCollator` åˆ›å»ºä¸€ä¸ªç¤ºä¾‹æ‰¹æ¬¡ã€‚ä¸ ğŸ¤— Transformers ä¸­çš„å…¶ä»–æ•°æ®æ•´ç†å™¨ä¸åŒï¼Œ`DefaultDataCollator` ä¸ä¼šåº”ç”¨é¢å¤–çš„é¢„å¤„ç†ï¼Œä¾‹å¦‚å¡«å……ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a14b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a696e5",
   "metadata": {},
   "source": [
    "\n",
    "## è¯„ä¼°\n",
    "\n",
    "åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŒ…å«ä¸€ä¸ªæŒ‡æ ‡é€šå¸¸æœ‰åŠ©äºè¯„ä¼°æ‚¨çš„æ¨¡å‹æ€§èƒ½ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) åº“å¿«é€ŸåŠ è½½è¯„ä¼°æ–¹æ³•ã€‚å¯¹äºæ­¤ä»»åŠ¡ï¼ŒåŠ è½½ [accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy) æŒ‡æ ‡ï¼ˆè¯·å‚é˜… ğŸ¤— Evaluate çš„ [å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour) ä»¥äº†è§£æ›´å¤šå…³äºå¦‚ä½•åŠ è½½å’Œè®¡ç®—æŒ‡æ ‡çš„ä¿¡æ¯ï¼‰ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486b4b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e13d2b23",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶ååˆ›å»ºä¸€ä¸ªå‡½æ•°ï¼Œå°†æ‚¨çš„é¢„æµ‹å’Œæ ‡ç­¾ä¼ é€’ç»™ [compute](https://huggingface.co/docs/evaluate/main/en/package_reference/main_classes#evaluate.EvaluationModule.compute) ä»¥è®¡ç®—å‡†ç¡®ç‡ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a40fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be13e1e2",
   "metadata": {},
   "source": [
    "\n",
    "æ‚¨çš„ `compute_metrics` å‡½æ•°ç°åœ¨å‡†å¤‡å¥½äº†ï¼Œå½“æ‚¨è®¾ç½®è®­ç»ƒæ—¶ï¼Œæ‚¨å°†è¿”å›åˆ°å®ƒã€‚\n",
    "\n",
    "## è®­ç»ƒ\n",
    "\n",
    "Pytorch\n",
    "\n",
    "å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŸ¥çœ‹ [è¿™é‡Œ](../training#train-with-pytorch-trainer) çš„åŸºæœ¬æ•™ç¨‹ï¼\n",
    "\n",
    "æ‚¨ç°åœ¨å¯ä»¥å¼€å§‹è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼ä½¿ç”¨ [AutoModelForImageClassification](/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForImageClassification) åŠ è½½ ViTã€‚æŒ‡å®šæ ‡ç­¾æ•°é‡ä»¥åŠé¢„æœŸçš„æ ‡ç­¾æ•°é‡å’Œæ ‡ç­¾æ˜ å°„ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4670e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5387a59b",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨è¿™ä¸€ç‚¹ä¸Šï¼Œåªå‰©ä¸‹ä¸‰ä¸ªæ­¥éª¤ï¼š\n",
    "\n",
    "1. åœ¨ [TrainingArguments](/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) ä¸­å®šä¹‰æ‚¨çš„è®­ç»ƒè¶…å‚æ•°ã€‚é‡è¦çš„æ˜¯æ‚¨ä¸è¦åˆ é™¤æœªä½¿ç”¨çš„åˆ—ï¼Œå› ä¸ºè¿™ä¼šåˆ é™¤ `image` åˆ—ã€‚å¦‚æœæ²¡æœ‰ `image` åˆ—ï¼Œæ‚¨å°±æ— æ³•åˆ›å»º `pixel_values`ã€‚è®¾ç½® `remove_unused_columns=False` ä»¥é˜²æ­¢è¿™ç§è¡Œä¸ºï¼å”¯ä¸€å…¶ä»–å¿…éœ€çš„å‚æ•°æ˜¯ `output_dir`ï¼Œå®ƒæŒ‡å®šä¿å­˜æ¨¡å‹çš„ä½ç½®ã€‚æ‚¨å°†é€šè¿‡è®¾ç½® `push_to_hub=True` å°†æ¨¡å‹æ¨é€åˆ° Hubï¼ˆæ‚¨éœ€è¦ç™»å½• Hugging Face æ‰èƒ½ä¸Šä¼ æ¨¡å‹ï¼‰ã€‚åœ¨æ¯ä¸ª epoch ç»“æŸæ—¶ï¼Œ[Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) å°†è¯„ä¼°å‡†ç¡®ç‡å¹¶ä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹ã€‚\n",
    "2. å°†è®­ç»ƒå‚æ•°ä¼ é€’ç»™ [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)ï¼Œä»¥åŠæ¨¡å‹ã€æ•°æ®é›†ã€æ ‡è®°å™¨ã€æ•°æ®æ•´ç†å™¨å’Œ `compute_metrics` å‡½æ•°ã€‚\n",
    "3. è°ƒç”¨ [train()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) ä»¥å¾®è°ƒæ‚¨çš„æ¨¡å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98055ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_food_model\",\n",
    "    remove_unused_columns=False,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=food[\"train\"],\n",
    "    eval_dataset=food[\"test\"],\n",
    "    processing_class=image_processor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853bc2fc",
   "metadata": {},
   "source": [
    "\n",
    "è®­ç»ƒå®Œæˆåï¼Œä½¿ç”¨ [push_to_hub()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) æ–¹æ³•å°†æ‚¨çš„æ¨¡å‹åˆ†äº«åˆ° Hubï¼Œä»¥ä¾¿æ¯ä¸ªäººéƒ½å¯ä»¥ä½¿ç”¨æ‚¨çš„æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630e4bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f945a5b7",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "å¦‚æœæ‚¨ä¸ç†Ÿæ‚‰ä½¿ç”¨ Keras å¾®è°ƒæ¨¡å‹ï¼Œè¯·å…ˆæŸ¥çœ‹ [åŸºæœ¬æ•™ç¨‹](./training#train-a-tensorflow-model-with-keras)ï¼\n",
    "\n",
    "è¦åœ¨ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹ï¼Œè¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤æ“ä½œï¼š\n",
    "\n",
    "1. å®šä¹‰è®­ç»ƒè¶…å‚æ•°ï¼Œå¹¶è®¾ç½®ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è®¡åˆ’ã€‚\n",
    "2. å®ä¾‹åŒ–é¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "3. å°† ğŸ¤— Dataset è½¬æ¢ä¸º `tf.data.Dataset`ã€‚\n",
    "4. ç¼–è¯‘æ‚¨çš„æ¨¡å‹ã€‚\n",
    "5. æ·»åŠ å›è°ƒå¹¶ä½¿ç”¨ `fit()` æ–¹æ³•è¿è¡Œè®­ç»ƒã€‚\n",
    "6. å°†æ‚¨çš„æ¨¡å‹ä¸Šä¼ åˆ° ğŸ¤— Hub ä»¥ä¸ç¤¾åŒºåˆ†äº«ã€‚\n",
    "\n",
    "é¦–å…ˆå®šä¹‰è¶…å‚æ•°ã€ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è®¡åˆ’ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d7352",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "batch_size = 16\n",
    "num_epochs = 5\n",
    "num_train_steps = len(food[\"train\"]) * num_epochs\n",
    "learning_rate = 3e-5\n",
    "weight_decay_rate = 0.01\n",
    "optimizer, lr_schedule = create_optimizer(\n",
    "    init_lr=learning_rate,\n",
    "    num_train_steps=num_train_steps,\n",
    "    weight_decay_rate=weight_decay_rate,\n",
    "    num_warmup_steps=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85ae989d",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åï¼Œä½¿ç”¨ [TFAutoModelForImageClassification](/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForImageClassification) åŠ è½½ ViT ä»¥åŠæ ‡ç­¾æ˜ å°„ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f881eb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0012db4d",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ [to_tf_dataset](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.to_tf_dataset) å’Œæ‚¨çš„ `data_collator` å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º `tf.data.Dataset` æ ¼å¼ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33486c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†è½¬æ¢ä¸º tf.data.Dataset\n",
    "tf_train_dataset = food[\"train\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")\n",
    "# å°†æˆ‘ä»¬çš„æµ‹è¯•æ•°æ®é›†è½¬æ¢ä¸º tf.data.Dataset\n",
    "tf_eval_dataset = food[\"test\"].to_tf_dataset(\n",
    "    columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c358de",
   "metadata": {},
   "source": [
    "\n",
    "ä½¿ç”¨ `compile()` é…ç½®æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127741f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "model.compile(optimizer=optimizer, loss=loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500ac07a",
   "metadata": {},
   "source": [
    "\n",
    "è¦è®¡ç®—å‡†ç¡®ç‡å¹¶å°†æ¨¡å‹æ¨é€åˆ° ğŸ¤— Hubï¼Œè¯·ä½¿ç”¨ [Keras å›è°ƒ](../main_classes/keras_callbacks)ã€‚å°†æ‚¨çš„ `compute_metrics` å‡½æ•°ä¼ é€’ç»™ [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)ï¼Œå¹¶ä½¿ç”¨ [PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback) ä¸Šä¼ æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f461b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n",
    "metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n",
    "push_to_hub_callback = PushToHubCallback(\n",
    "    output_dir=\"food_classifier\",\n",
    "    tokenizer=image_processor,\n",
    "    save_strategy=\"no\",\n",
    ")\n",
    "callbacks = [metric_callback, push_to_hub_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaa39fe7",
   "metadata": {},
   "source": [
    "\n",
    "æœ€åï¼Œæ‚¨ç°åœ¨å¯ä»¥è®­ç»ƒæ‚¨çš„æ¨¡å‹äº†ï¼è°ƒç”¨ `fit()` å¹¶ä¼ å…¥æ‚¨çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®é›†ã€epoch æ•°é‡ä»¥åŠå›è°ƒä»¥å¾®è°ƒæ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7f1a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dcf1ec5",
   "metadata": {},
   "source": [
    "\n",
    "æ­å–œï¼æ‚¨å·²ç»å¾®è°ƒäº†æ‚¨çš„æ¨¡å‹å¹¶å°†å…¶åˆ†äº«åœ¨ ğŸ¤— Hub ä¸Šã€‚ç°åœ¨ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼\n",
    "\n",
    "æœ‰å…³å¦‚ä½•ä¸ºå›¾åƒåˆ†ç±»å¾®è°ƒæ¨¡å‹çš„æ›´æ·±å…¥ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹ç›¸åº”çš„ [PyTorch ç¬”è®°æœ¬](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)ã€‚\n",
    "\n",
    "## æ¨ç†\n",
    "\n",
    "å¤ªå¥½äº†ï¼Œç°åœ¨æ‚¨å·²ç»å¾®è°ƒäº†ä¸€ä¸ªæ¨¡å‹ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å®ƒè¿›è¡Œæ¨ç†äº†ï¼\n",
    "\n",
    "åŠ è½½æ‚¨æƒ³è¦è¿è¡Œæ¨ç†çš„å›¾åƒï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"food101\", split=\"validation[:10]\")\n",
    "image = ds[\"image\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1773b363",
   "metadata": {},
   "source": [
    "\n",
    "![image of beignets](./resources/images/beignets-task-guide.png)\n",
    "\n",
    "å°è¯•ä½¿ç”¨å¾®è°ƒæ¨¡å‹è¿›è¡Œæ¨ç†çš„æœ€ç®€å•æ–¹æ³•æ˜¯å°†å…¶ç”¨äº `pipeline()`ã€‚ä¸ºå›¾åƒåˆ†ç±»å®ä¾‹åŒ–ä¸€ä¸ª `pipeline` å¹¶ä¼ å…¥æ‚¨çš„å›¾åƒï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b201b657",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"image-classification\", model=\"my_awesome_food_model\")\n",
    "classifier(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccd4c135",
   "metadata": {},
   "source": [
    "\n",
    "å¦‚æœæ‚¨æ„¿æ„ï¼Œæ‚¨ä¹Ÿå¯ä»¥æ‰‹åŠ¨å¤åˆ¶ `pipeline` çš„ç»“æœï¼š\n",
    "\n",
    "Pytorch\n",
    "\n",
    "åŠ è½½å›¾åƒå¤„ç†å™¨ä»¥é¢„å¤„ç†å›¾åƒå¹¶è¿”å› PyTorch å¼ é‡çš„ `input`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70eb6c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "import torch\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"my_awesome_food_model\")\n",
    "inputs = image_processor(image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d803355d",
   "metadata": {},
   "source": [
    "\n",
    "å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› logitsï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69edf6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "model = AutoModelForImageClassification.from_pretrained(\"my_awesome_food_model\")\n",
    "with torch.no_grad():\n",
    "    logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746870f",
   "metadata": {},
   "source": [
    "\n",
    "è·å–æ¦‚ç‡æœ€é«˜çš„é¢„æµ‹æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„ `id2label` æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc680972",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_label = logits.argmax(-1).item()\n",
    "model.config.id2label[predicted_label]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3709356",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "åŠ è½½å›¾åƒå¤„ç†å™¨ä»¥é¢„å¤„ç†å›¾åƒå¹¶è¿”å› TensorFlow å¼ é‡çš„ `input`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c70c2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor\n",
    "image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n",
    "inputs = image_processor(image, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc9fc0e",
   "metadata": {},
   "source": [
    "\n",
    "å°†æ‚¨çš„è¾“å…¥ä¼ é€’ç»™æ¨¡å‹å¹¶è¿”å› logitsï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc0b8f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForImageClassification\n",
    "model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n",
    "logits = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fea4beb",
   "metadata": {},
   "source": [
    "\n",
    "è·å–æ¦‚ç‡æœ€é«˜çš„é¢„æµ‹æ ‡ç­¾ï¼Œå¹¶ä½¿ç”¨æ¨¡å‹çš„ `id2label` æ˜ å°„å°†å…¶è½¬æ¢ä¸ºæ ‡ç­¾ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b49286c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
    "model.config.id2label[predicted_class_id]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
