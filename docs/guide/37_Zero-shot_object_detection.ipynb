{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c83d88c2",
   "metadata": {},
   "source": [
    "# 零样本目标检测\n",
    "\n",
    "在传统方式中，用于目标检测的模型需要标注的图像数据集进行训练，并且仅限于检测训练数据中的类。\n",
    "\n",
    "零样本目标检测由 [OWL-ViT](../model_doc/owlvit) 模型支持，该模型采用了一种不同的方法。OWL-ViT 是一种开放词汇目标检测器，它可以根据自由文本查询检测图像中的对象，而无需对模型进行微调以适应标注数据集。\n",
    "\n",
    "OWL-ViT 利用多模态表示来执行开放词汇检测。它结合了 [CLIP](../model_doc/clip) 与轻量级的对象分类和定位头。通过使用 CLIP 的文本编码器嵌入自由文本查询，并将其作为对象分类和定位头的输入，这些头将图像与其相应的文本描述关联起来，而 ViT 处理图像补丁作为输入。OWL-ViT 的作者首先从头开始训练 CLIP，然后在标准目标检测数据集上使用二部匹配损失对 OWL-ViT 进行端到端微调。\n",
    "\n",
    "通过这种方法，模型可以根据文本描述检测对象，而无需预先在标注数据集上进行训练。\n",
    "\n",
    "在这个指南中，你将学习如何使用 OWL-ViT：\n",
    "\n",
    "- 基于文本提示检测对象\n",
    "- 批量目标检测\n",
    "- 图像引导的目标检测\n",
    "\n",
    "在开始之前，请确保你已经安装了所有必要的库：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f028cd6c",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b325b5ce",
   "metadata": {},
   "source": [
    "\n",
    "## 零样本目标检测管道\n",
    "\n",
    "使用 OWL-ViT 进行推理的最简单方法是通过 [pipeline()](/docs/transformers/v4.46.2/en/main_classes/pipelines#transformers.pipeline)。从 [Hugging Face Hub](https://huggingface.co/models?other=owlvit) 上的一个检查点实例化一个零样本目标检测管道：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "899cfcc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "checkpoint = \"google/owlv2-base-patch16-ensemble\"\n",
    "detector = pipeline(model=checkpoint, task=\"zero-shot-object-detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6505842e",
   "metadata": {},
   "source": [
    "\n",
    "接下来，选择一张你想要检测对象的图像。这里我们将使用 NASA Great Images 数据集中的一张宇航员 Eileen Collins 的图片。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d5b310",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "image = skimage.data.astronaut()\n",
    "image = Image.fromarray(np.uint8(image)).convert(\"RGB\")\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33a151c",
   "metadata": {},
   "source": [
    "\n",
    "![宇航员 Eileen Collins](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_1.png)\n",
    "\n",
    "将图像和候选对象标签传递给管道。这里我们直接传递图像；其他合适的选项包括本地图像路径或图像 URL。我们还传递了要查询的所有项目的文本描述。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8d9c498",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = detector(\n",
    "    image,\n",
    "    candidate_labels=[\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n",
    ")\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca67c830",
   "metadata": {},
   "source": [
    "\n",
    "让我们可视化预测结果：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37bcd89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import ImageDraw\n",
    "\n",
    "draw = ImageDraw.Draw(image)\n",
    "\n",
    "for prediction in predictions:\n",
    "    box = prediction[\"box\"]\n",
    "    label = prediction[\"label\"]\n",
    "    score = prediction[\"score\"]\n",
    "\n",
    "    xmin, ymin, xmax, ymax = box.values()\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "    draw.text((xmin, ymin), f\"{label}: {round(score,2)}\", fill=\"white\")\n",
    "\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e15c2",
   "metadata": {},
   "source": [
    "\n",
    "![NASA 图像上的预测结果](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_2.png)\n",
    "\n",
    "## 手动进行基于文本提示的零样本目标检测\n",
    "\n",
    "现在你已经看到了如何使用零样本目标检测管道，让我们手动复制相同的结果。\n",
    "\n",
    "首先从 [Hugging Face Hub](https://huggingface.co/models?other=owlvit) 加载模型和相关处理器。这里我们将使用与之前相同的检查点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa20b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n",
    "\n",
    "model = AutoModelForZeroShotObjectDetection.from_pretrained(checkpoint)\n",
    "processor = AutoProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4461830",
   "metadata": {},
   "source": [
    "\n",
    "为了换换口味，我们选择一张不同的图像。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5b5c15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "url = \"https://unsplash.com/photos/oj0zeY2Ltk4/download?ixid=MnwxMjA3fDB8MXxzZWFyY2h8MTR8fHBpY25pY3xlbnwwfHx8fDE2Nzc0OTE1NDk&force=true&w=640\"\n",
    "im = Image.open(requests.get(url, stream=True).raw)\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2210c1",
   "metadata": {},
   "source": [
    "\n",
    "![海滩照片](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_3.png)\n",
    "\n",
    "使用处理器准备模型的输入。处理器结合了一个图像处理器，该处理器通过调整大小和归一化来准备图像，以及一个 [CLIPTokenizer](/docs/transformers/v4.46.2/en/model_doc/clip#transformers.CLIPTokenizer)，它负责处理文本输入。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68125105",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_queries = [\"hat\", \"book\", \"sunglasses\", \"camera\"]\n",
    "inputs = processor(text=text_queries, images=im, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "623c396c",
   "metadata": {},
   "source": [
    "\n",
    "将输入传递给模型，进行后处理并可视化结果。由于图像处理器在将图像输入模型之前进行了调整大小，因此需要使用 [post_process_object_detection()](/docs/transformers/v4.46.2/en/model_doc/owlvit#transformers.OwlViTImageProcessor.post_process_object_detection) 方法，以确保预测的边界框相对于原始图像具有正确的坐标：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e75f3c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = torch.tensor([im.size[::-1]])\n",
    "    results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)[0]\n",
    "\n",
    "draw = ImageDraw.Draw(im)\n",
    "\n",
    "scores = results[\"scores\"].tolist()\n",
    "labels = results[\"labels\"].tolist()\n",
    "boxes = results[\"boxes\"].tolist()\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "    draw.text((xmin, ymin), f\"{text_queries[label]}: {round(score,2)}\", fill=\"white\")\n",
    "\n",
    "im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45ec2b",
   "metadata": {},
   "source": [
    "\n",
    "![带有检测对象的海滩照片](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png)\n",
    "\n",
    "## 批量处理\n",
    "\n",
    "你可以传递多组图像和文本查询，以在多个图像中搜索不同的（或相同的）对象。让我们同时使用宇航员图像和海滩图像。对于批量处理，你应该将文本查询作为嵌套列表传递给处理器，将图像作为 PIL 图像、PyTorch 张量或 NumPy 数组的列表传递。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e2d1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images = [image, im]\n",
    "text_queries = [\n",
    "    [\"human face\", \"rocket\", \"nasa badge\", \"star-spangled banner\"],\n",
    "    [\"hat\", \"book\", \"sunglasses\", \"camera\"],\n",
    "]\n",
    "inputs = processor(text=text_queries, images=images, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50db25b",
   "metadata": {},
   "source": [
    "\n",
    "在后处理中，你之前传递了单个图像的大小作为张量，但你也可以传递一个元组，或者在处理多个图像时传递一个元组列表。让我们为两个示例创建预测，并可视化第二个示例（`image_idx = 1`）。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19aef95",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "    target_sizes = [x.size[::-1] for x in images]\n",
    "    results = processor.post_process_object_detection(outputs, threshold=0.1, target_sizes=target_sizes)\n",
    "\n",
    "image_idx = 1\n",
    "draw = ImageDraw.Draw(images[image_idx])\n",
    "\n",
    "scores = results[image_idx][\"scores\"].tolist()\n",
    "labels = results[image_idx][\"labels\"].tolist()\n",
    "boxes = results[image_idx][\"boxes\"].tolist()\n",
    "\n",
    "for box, score, label in zip(boxes, scores, labels):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"red\", width=1)\n",
    "    draw.text((xmin, ymin), f\"{text_queries[image_idx][label]}: {round(score,2)}\", fill=\"white\")\n",
    "\n",
    "images[image_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed155ce9",
   "metadata": {},
   "source": [
    "\n",
    "![带有检测对象的海滩照片](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_4.png)\n",
    "\n",
    "## 图像引导的目标检测\n",
    "\n",
    "除了使用文本查询进行零样本目标检测外，OWL-ViT 还提供图像引导的目标检测。这意味着你可以使用图像查询在目标图像中找到类似的对象。与文本查询不同，只允许使用一个示例图像。\n",
    "\n",
    "让我们选择一张沙发上两只猫的图像作为目标图像，一张单猫的图像作为查询图像：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d07b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n",
    "image_target = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "query_url = \"http://images.cocodataset.org/val2017/000000524280.jpg\"\n",
    "query_image = Image.open(requests.get(query_url, stream=True).raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07dcf96",
   "metadata": {},
   "source": [
    "\n",
    "让我们快速查看一下图像：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2add95ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(1, 2)\n",
    "ax[0].imshow(image_target)\n",
    "ax[1].imshow(query_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7632c332",
   "metadata": {},
   "source": [
    "\n",
    "![猫](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_5.png)\n",
    "\n",
    "在预处理步骤中，你现在需要使用 `query_images` 而不是文本查询：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573c26cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = processor(images=image_target, query_images=query_image, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb14a80",
   "metadata": {},
   "source": [
    "\n",
    "对于预测，而不是将输入传递给模型，而是将它们传递给 [image_guided_detection()](/docs/transformers/v4.46.2/en/model_doc/owlvit#transformers.OwlViTForObjectDetection.image_guided_detection)。与之前一样绘制预测结果，但这次没有标签。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6f1ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    outputs = model.image_guided_detection(**inputs)\n",
    "    target_sizes = torch.tensor([image_target.size[::-1]])\n",
    "    results = processor.post_process_image_guided_detection(outputs=outputs, target_sizes=target_sizes)[0]\n",
    "\n",
    "draw = ImageDraw.Draw(image_target)\n",
    "\n",
    "scores = results[\"scores\"].tolist()\n",
    "boxes = results[\"boxes\"].tolist()\n",
    "\n",
    "for box, score in zip(boxes, scores):\n",
    "    xmin, ymin, xmax, ymax = box\n",
    "    draw.rectangle((xmin, ymin, xmax, ymax), outline=\"white\", width=4)\n",
    "\n",
    "image_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c674f486",
   "metadata": {},
   "source": [
    "\n",
    "![带有边界框的猫](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/zero-sh-obj-detection_6.png)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
