{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb22abab",
   "metadata": {},
   "source": [
    "# 问题解答(问答任务)\n",
    "\n",
    "问题解答任务是给定一个问题，返回一个答案。如果你曾经问过像 Alexa、Siri 或 Google 这样的虚拟助手天气如何，那么你已经使用过问题解答模型了。问题解答任务主要有两种类型：\n",
    "\n",
    "- **抽取式**：从给定的上下文中提取答案。\n",
    "- **抽象式**：根据上下文生成一个正确回答问题的答案。\n",
    "\n",
    "本指南将向你展示如何：\n",
    "\n",
    "1. 在 [SQuAD](https://huggingface.co/datasets/squad) 数据集上微调 [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) 模型，用于抽取式问题解答。\n",
    "2. 使用微调后的模型进行推理。\n",
    "\n",
    "要查看所有与该任务兼容的架构和检查点，我们建议查看 [任务页面](https://huggingface.co/tasks/question-answering)。\n",
    "\n",
    "在开始之前，请确保你已经安装了所有必要的库：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a18452",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177f297b",
   "metadata": {},
   "source": [
    "\n",
    "我们鼓励你登录你的 Hugging Face 账户，这样你就可以上传并与社区分享你的模型。当提示时，输入你的令牌登录：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0daa4264",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b820257",
   "metadata": {},
   "source": [
    "\n",
    "## 加载 SQuAD 数据集\n",
    "\n",
    "首先从 🤗 Datasets 库中加载 SQuAD 数据集的一个较小的子集。这将让你有机会进行实验，并确保一切正常工作，然后再在完整数据集上花费更多时间进行训练。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ec69ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "squad = load_dataset(\"squad\", split=\"train[:5000]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a251401",
   "metadata": {},
   "source": [
    "\n",
    "使用 [train_test_split](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.train_test_split) 方法将数据集的 `train` 分割成一个训练集和测试集：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1142ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = squad.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ec3223f",
   "metadata": {},
   "source": [
    "\n",
    "然后查看一个示例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b67f0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc6340b",
   "metadata": {},
   "source": [
    "\n",
    "这里有几个重要的字段：\n",
    "\n",
    "- `answers`：答案的起始位置和答案文本。\n",
    "- `context`：模型需要从中提取答案的背景信息。\n",
    "- `question`：模型应该回答的问题。\n",
    "\n",
    "## 预处理\n",
    "\n",
    "下一步是加载 DistilBERT 分词器来处理 `question` 和 `context` 字段：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750882d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8cf830",
   "metadata": {},
   "source": [
    "\n",
    "问题解答任务有一些特殊的预处理步骤你应该注意：\n",
    "\n",
    "1. 数据集中的某些示例可能有一个非常长的 `context`，超过了模型的输入长度限制。为了处理更长的序列，只截断 `context`，设置 `truncation=\"only_second\"`。\n",
    "2. 接下来，通过设置 `return_offset_mapping=True`，将答案的起始和结束位置映射到原始 `context`。\n",
    "3. 有了映射，现在你可以找到答案的起始和结束标记。使用 [sequence_ids](https://huggingface.co/docs/tokenizers/main/en/api/encoding#tokenizers.Encoding.sequence_ids) 方法来找到偏移量中哪部分对应于 `question`，哪部分对应于 `context`。\n",
    "\n",
    "以下是如何创建一个函数来截断并将 `answer` 的起始和结束标记映射到 `context`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d9da4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # 找到上下文的开始和结束\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # 如果答案不完全在上下文中，标记为 (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # 否则它是起始和结束标记的位置\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69832219",
   "metadata": {},
   "source": [
    "\n",
    "要在整个数据集上应用预处理函数，使用 🤗 Datasets 的 [map](https://huggingface.co/docs/datasets/main/en/package_reference/main_classes#datasets.Dataset.map) 函数。你可以通过设置 `batched=True` 来加速 `map` 函数，以便一次处理数据集中的多个元素。删除你不需要的任何列：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b1cde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf9d2e9",
   "metadata": {},
   "source": [
    "\n",
    "现在使用 [DefaultDataCollator](/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) 创建一个示例批次。与其他 🤗 Transformers 中的数据整理器不同，[DefaultDataCollator](/docs/transformers/main/en/main_classes/data_collator#transformers.DefaultDataCollator) 不会应用任何额外的预处理，如填充。\n",
    "\n",
    "Pytorch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299ae3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29edb8e2",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8cf2ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DefaultDataCollator\n",
    "data_collator = DefaultDataCollator(return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03fec564",
   "metadata": {},
   "source": [
    "\n",
    "## 训练\n",
    "\n",
    "Pytorch\n",
    "\n",
    "如果你不熟悉使用 [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 微调模型，请查看这里的基本教程 [这里](../training#train-with-pytorch-trainer)！\n",
    "\n",
    "现在你准备好开始训练你的模型了！使用 [AutoModelForQuestionAnswering](/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForQuestionAnswering) 加载 DistilBERT：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661217b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0cdc70",
   "metadata": {},
   "source": [
    "\n",
    "在这一点上，只剩下三个步骤：\n",
    "\n",
    "1. 在 [TrainingArguments](/docs/transformers/main/en/main_classes/trainer#transformers.TrainingArguments) 中定义你的训练超参数。唯一需要的参数是 `output_dir`，它指定了保存模型的位置。通过设置 `push_to_hub=True` 将模型推送到 Hub（你需要登录 Hugging Face 才能上传你的模型）。\n",
    "2. 将训练参数传递给 [Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer)，以及模型、数据集、分词器和数据整理器。\n",
    "3. 调用 [train()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.train) 来微调你的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63745d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_qa_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"test\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff62ac09",
   "metadata": {},
   "source": [
    "\n",
    "训练完成后，使用 [push_to_hub()](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer.push_to_hub) 方法将你的模型分享到 Hub，以便每个人都可以使用你的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca2b3bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1db644",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "如果你不熟悉使用 Keras 微调模型，请查看这里的基本教程 [这里](../training#train-a-tensorflow-model-with-keras)！\n",
    "\n",
    "要在 TensorFlow 中微调模型，首先设置一个优化器函数、学习率计划和一些训练超参数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6e9ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import create_optimizer\n",
    "batch_size = 16\n",
    "num_epochs = 2\n",
    "total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=total_train_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c3c4ac",
   "metadata": {},
   "source": [
    "\n",
    "然后使用 [TFAutoModelForQuestionAnswering](/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForQuestionAnswering) 加载 DistilBERT：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23197e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f996e65d",
   "metadata": {},
   "source": [
    "\n",
    "使用 [prepare_tf_dataset()](/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset) 将你的数据集转换为 `tf.data.Dataset` 格式：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cc0569",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_squad[\"train\"],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")\n",
    "\n",
    "tf_validation_set = model.prepare_tf_dataset(\n",
    "    tokenized_squad[\"test\"],\n",
    "    shuffle=False,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff49983c",
   "metadata": {},
   "source": [
    "\n",
    "使用 [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) 配置模型进行训练：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3539309",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "model.compile(optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcf9a470",
   "metadata": {},
   "source": [
    "\n",
    "在开始训练之前要设置的最后一件事是提供一种将模型推送到 Hub 的方法。这可以通过在 [PushToHubCallback](/docs/transformers/main/en/main_classes/keras_callbacks#transformers.PushToHubCallback) 中指定将模型和分词器推送到哪里来实现：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151c2694",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.keras_callbacks import PushToHubCallback\n",
    "callback = PushToHubCallback(\n",
    "    output_dir=\"my_awesome_qa_model\",\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9b289d",
   "metadata": {},
   "source": [
    "\n",
    "最后，你准备好开始训练你的模型了！调用 [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)，使用你的训练和验证数据集、epoch 数量和你的回调来微调模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9fe7db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c4e6d2",
   "metadata": {},
   "source": [
    "\n",
    "训练完成后，你的模型会自动上传到 Hub，以便每个人都可以使用它！\n",
    "\n",
    "如果你想要更深入地了解如何为问题解答微调模型，请查看相应的 [PyTorch 笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb) 或 [TensorFlow 笔记本](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)。\n",
    "\n",
    "## 评估\n",
    "\n",
    "问题解答的评估需要大量的后处理。为了不占用你太多时间，本指南跳过了评估步骤。[Trainer](/docs/transformers/main/en/main_classes/trainer#transformers.Trainer) 仍然在训练过程中计算评估损失，所以你不会完全不知道你的模型的性能。\n",
    "\n",
    "如果你有更多时间，并且对如何评估你的问题解答模型感兴趣，请查看 🤗 Hugging Face 课程中的 [问题解答](https://huggingface.co/course/chapter7/7?fw=pt#post-processing) 章节！\n",
    "\n",
    "## 推理\n",
    "\n",
    "太好了，现在你已经微调了一个模型，你可以使用它进行推理了！\n",
    "\n",
    "想出一个问题和一些你想要模型预测的上下文：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f756e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many programming languages does BLOOM support?\"\n",
    "context = \"BLOOM has 176 billion parameters and can generate text in 46 natural languages and 13 programming languages.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84ab3a51",
   "metadata": {},
   "source": [
    "\n",
    "尝试你的微调模型进行推理的最简单方法是使用它在一个 [pipeline()](/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) 中。为问题解答实例化一个 `pipeline`，并将你的文本传递给它：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1757d2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "question_answerer = pipeline(\"question-answering\", model=\"my_awesome_qa_model\")\n",
    "question_answerer(question=question, context=context)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0936c2a1",
   "metadata": {},
   "source": [
    "\n",
    "如果你愿意，你也可以手动复制 `pipeline` 的结果：\n",
    "\n",
    "Pytorch\n",
    "\n",
    "对文本进行分词并返回 PyTorch 张量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed136856",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n",
    "inputs = tokenizer(question, context, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e181aa",
   "metadata": {},
   "source": [
    "\n",
    "将你的输入传递给模型并返回 `logits`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195faf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7e925b",
   "metadata": {},
   "source": [
    "\n",
    "从模型输出中获取起始和结束位置的最高概率：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1401c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = outputs.start_logits.argmax()\n",
    "answer_end_index = outputs.end_logits.argmax()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d48a5a",
   "metadata": {},
   "source": [
    "\n",
    "解码预测的标记以获得答案：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b829663",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651a280",
   "metadata": {},
   "source": [
    "\n",
    "TensorFlow\n",
    "\n",
    "对文本进行分词并返回 TensorFlow 张量：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f82566",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n",
    "inputs = tokenizer(question, text, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc710",
   "metadata": {},
   "source": [
    "\n",
    "将你的输入传递给模型并返回 `logits`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64728534",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForQuestionAnswering\n",
    "model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6555fe86",
   "metadata": {},
   "source": [
    "\n",
    "从模型输出中获取起始和结束位置的最高概率：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e10ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n",
    "answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00095b9",
   "metadata": {},
   "source": [
    "\n",
    "解码预测的标记以获得答案：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dee4423",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n",
    "tokenizer.decode(predict_answer_tokens)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
