{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb78ceca",
   "metadata": {},
   "source": [
    "# 多项选择任务\n",
    "\n",
    "多项选择任务类似于问答任务，不同之处在于提供了多个候选答案以及一些上下文，模型需要训练以选择正确的答案。\n",
    "\n",
    "本指南将向您展示如何：\n",
    "\n",
    "1. 在 SWAG 数据集的 `regular` 配置上微调 BERT，以在给定的多个选项和一些上下文中选择最佳答案。\n",
    "2. 使用您微调的模型进行推理。\n",
    "\n",
    "在开始之前，请确保您已安装所有必要的库：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "886502b7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers datasets evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272f3b2f",
   "metadata": {},
   "source": [
    "\n",
    "我们鼓励您登录您的 Hugging Face 账户，这样您就可以上传并与社区分享您的模型。当提示时，输入您的令牌以登录：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e76abb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6dae22",
   "metadata": {},
   "source": [
    "\n",
    "## 加载 SWAG 数据集\n",
    "\n",
    "首先从 🤗 Datasets 库中加载 SWAG 数据集的 `regular` 配置：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1937bf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "swag = load_dataset(\"swag\", \"regular\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b93717c",
   "metadata": {},
   "source": [
    "\n",
    "然后查看一个示例：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989cd04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "swag[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6ffaa1",
   "metadata": {},
   "source": [
    "\n",
    "虽然这里看起来有很多字段，但实际上非常简单：\n",
    "\n",
    "- `sent1` 和 `sent2`：这些字段显示了句子是如何开始的，如果您将这两个字段放在一起，就会得到 `startphrase` 字段。\n",
    "- `ending`：建议一个可能的句子结束方式，但只有一个是正确的。\n",
    "- `label`：标识正确的句子结束。\n",
    "\n",
    "## 预处理\n",
    "\n",
    "下一步是加载 BERT 分词器来处理句子开头和四个可能的结尾：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdebe4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa842b3c",
   "metadata": {},
   "source": [
    "\n",
    "您需要创建的预处理函数需要执行以下操作：\n",
    "\n",
    "1. 制作 `sent1` 字段的四份副本，并将每个副本与 `sent2` 结合以重新创建句子开头的方式。\n",
    "2. 将 `sent2` 与每个可能的句子结尾结合。\n",
    "3. 扁平化这两个列表以便进行分词，然后在分词后进行反扁平化，以便每个示例都有相应的 `input_ids`、`attention_mask` 和 `labels` 字段。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cda2cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ending_names = [\"ending0\", \"ending1\", \"ending2\", \"ending3\"]\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    first_sentences = [[context] * 4 for context in examples[\"sent1\"]]\n",
    "    question_headers = examples[\"sent2\"]\n",
    "    second_sentences = [\n",
    "        [f\"{header} {examples[end][i]}\" for end in ending_names] for i, header in enumerate(question_headers)\n",
    "    ]\n",
    "\n",
    "    first_sentences = sum(first_sentences, [])\n",
    "    second_sentences = sum(second_sentences, [])\n",
    "\n",
    "    tokenized_examples = tokenizer(first_sentences, second_sentences, truncation=True)\n",
    "    return {k: [v[i:i + 4] for i in range(0, len(v), 4)] for k, v in tokenized_examples.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b661a2",
   "metadata": {},
   "source": [
    "\n",
    "要在整个数据集上应用预处理函数，请使用 🤗 Datasets 的 `map` 方法。您可以通过设置 `batched=True` 来加速 `map` 函数，以便一次处理数据集中的多个元素：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a481e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_swag = swag.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f601fbf9",
   "metadata": {},
   "source": [
    "\n",
    "🤗 Transformers 没有为多项选择提供数据整理器，因此您需要调整 `DataCollatorWithPadding` 来创建一个示例批次。在整理过程中，动态地将句子填充到批次中最长的长度，而不是将整个数据集填充到最大长度，这样做更有效率。\n",
    "\n",
    "`DataCollatorForMultipleChoice` 扁平化所有模型输入，应用填充，然后反扁平化结果：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edee75b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n",
    "from typing import Optional, Union\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMultipleChoice:\n",
    "    \"\"\"\n",
    "    Data collator that will dynamically pad the inputs for multiple choice received.\n",
    "    \"\"\"\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    padding: Union[bool, str, PaddingStrategy] = True\n",
    "    max_length: Optional[int] = None\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "\n",
    "    def __call__(self, features):\n",
    "        label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n",
    "        labels = [feature.pop(label_name) for feature in features]\n",
    "        batch_size = len(features)\n",
    "        num_choices = len(features[0][\"input_ids\"])\n",
    "        flattened_features = [\n",
    "            [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n",
    "        ]\n",
    "        flattened_features = sum(flattened_features, [])\n",
    "\n",
    "        batch = self.tokenizer.pad(\n",
    "            flattened_features,\n",
    "            padding=self.padding,\n",
    "            max_length=self.max_length,\n",
    "            pad_to_multiple_of=self.pad_to_multiple_of,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        batch = {k: v.view(batch_size, num_choices, -1) for k, v in batch.items()}\n",
    "        batch[\"labels\"] = torch.tensor(labels, dtype=torch.int64)\n",
    "        return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc7d539",
   "metadata": {},
   "source": [
    "\n",
    "## 评估\n",
    "\n",
    "在训练过程中包含一个指标通常有助于评估您的模型性能。您可以使用 🤗 Evaluate 库快速加载一个评估方法。对于此任务，加载准确性指标（请参阅 🤗 Evaluate 快速入门以了解更多关于如何加载和计算指标的信息）：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b963100e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "accuracy = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a10d4a",
   "metadata": {},
   "source": [
    "\n",
    "然后创建一个函数，将您的预测和标签传递给 `compute` 以计算准确性：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b382023",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return accuracy.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647d1e4b",
   "metadata": {},
   "source": [
    "\n",
    "您的 `compute_metrics` 函数现在准备好了，当您设置训练时，您将返回到它。\n",
    "\n",
    "## 训练\n",
    "\n",
    "如果您不熟悉使用 `Trainer` 微调模型，请查看基本教程！\n",
    "\n",
    "您现在可以开始训练您的模型了！使用 `AutoModelForMultipleChoice` 加载 BERT：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f937816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice, TrainingArguments, Trainer\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a22c02b",
   "metadata": {},
   "source": [
    "\n",
    "在这一点上，只剩下三个步骤：\n",
    "\n",
    "1. 在 `TrainingArguments` 中定义您的训练超参数。唯一需要的参数是 `output_dir`，它指定了保存模型的位置。您可以通过设置 `push_to_hub=True` 将模型推送到 Hub（您需要登录 Hugging Face 才能上传模型）。在每个 epoch 结束时，`Trainer` 将评估准确性并保存训练检查点。\n",
    "2. 将训练参数传递给 `Trainer`，以及模型、数据集、分词器、数据整理器和 `compute_metrics` 函数。\n",
    "3. 调用 `train()` 以微调您的模型。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3437cb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"my_awesome_swag_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_swag[\"train\"],\n",
    "    eval_dataset=tokenized_swag[\"validation\"],\n",
    "    processing_class=tokenizer,\n",
    "    data_collator=DataCollatorForMultipleChoice(tokenizer=tokenizer),\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "808f0734",
   "metadata": {},
   "source": [
    "\n",
    "训练完成后，使用 `push_to_hub()` 方法将您的模型分享到 Hub，以便每个人都可以使用您的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677d6e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709504d9",
   "metadata": {},
   "source": [
    "\n",
    "## 推理\n",
    "\n",
    "太好了，现在您已经微调了一个模型，您可以使用它进行推理！\n",
    "\n",
    "想出一些文本和两个候选答案：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81656dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"France has a bread law, Le Décret Pain, with strict rules on what is allowed in a traditional baguette.\"\n",
    "candidate1 = \"The law does not apply to croissants and brioche.\"\n",
    "candidate2 = \"The law applies to baguettes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c550aeb3",
   "metadata": {},
   "source": [
    "\n",
    "对每个提示和候选答案对进行分词，并返回 PyTorch 张量。您还应该创建一些 `labels`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea28070",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n",
    "inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"pt\", padding=True)\n",
    "labels = torch.tensor(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06b46c9",
   "metadata": {},
   "source": [
    "\n",
    "将您的输入和标签传递给模型，并返回 `logits`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0b0e97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMultipleChoice\n",
    "model = AutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n",
    "outputs = model(**{k: v.unsqueeze(0) for k, v in inputs.items()}, labels=labels)\n",
    "logits = outputs.logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9b3f028",
   "metadata": {},
   "source": [
    "\n",
    "获取概率最高的类别：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04b8535",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_class = logits.argmax().item()\n",
    "predicted_class"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
