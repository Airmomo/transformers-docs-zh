{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ LLMs ç”Ÿæˆ\n",
    "\n",
    "å¤§è¯­è¨€æ¨¡å‹(LLMs)æ˜¯æ–‡æœ¬ç”ŸæˆèƒŒåçš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚ç®€å•æ¥è¯´ï¼Œå®ƒä»¬æ˜¯ç»è¿‡å¤§è§„æ¨¡é¢„è®­ç»ƒçš„ transformer æ¨¡å‹ï¼Œç”¨äº**æ ¹æ®ç»™å®šçš„è¾“å…¥æ–‡æœ¬é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼ˆä¸‹ä¸€ä¸ªtokenï¼‰ã€‚**ç”±äºå®ƒä»¬**ä¸€æ¬¡åªé¢„æµ‹ä¸€ä¸ªtoken**ï¼Œå› æ­¤é™¤äº†è°ƒç”¨æ¨¡å‹ä¹‹å¤–ï¼Œè¿˜éœ€è¦æ‰§è¡Œè‡ªå›å½’ç”Ÿæˆæ¥ç”Ÿæˆæ–°çš„å¥å­ã€‚\n",
    "\n",
    "`è‡ªå›å½’ç”Ÿæˆ`æ˜¯åœ¨ç»™å®šä¸€äº›åˆå§‹è¾“å…¥ï¼Œé€šè¿‡è¿­ä»£è°ƒç”¨æ¨¡å‹åŠå…¶è‡ªèº«çš„ç”Ÿæˆè¾“å‡ºæ¥ç”Ÿæˆæ–‡æœ¬çš„æ¨ç†è¿‡ç¨‹ã€‚åœ¨ğŸ¤— Transformersä¸­ï¼Œè¿™ç”± [generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate) æ–¹æ³•å¤„ç†ï¼Œæ‰€æœ‰å…·æœ‰ç”Ÿæˆèƒ½åŠ›çš„æ¨¡å‹éƒ½å¯ä»¥ä½¿ç”¨è¯¥æ–¹æ³•ã€‚\n",
    "\n",
    "æœ¬æ•™ç¨‹å°†å‘ä½ å±•ç¤ºå¦‚ä½•ï¼š\n",
    "- ä½¿ç”¨ LLM ç”Ÿæˆæ–‡æœ¬\n",
    "- é¿å…å¸¸è§çš„é™·é˜±\n",
    "- å¸®åŠ©ä½ å……åˆ†åˆ©ç”¨LLMè¿›è¡Œä¸‹ä¸€æ­¥æŒ‡å¯¼\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£…æ‰€æœ‰å¿…è¦çš„åº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers bitsandbytes>=0.39.0 -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç”Ÿæˆæ–‡æœ¬\n",
    "\n",
    "ä¸€ä¸ªç”¨äº[å› æœè¯­è¨€å»ºæ¨¡](https://huggingface.co/docs/transformers/main/en/tasks/language_modeling)è®­ç»ƒçš„è¯­è¨€æ¨¡å‹ï¼Œå°†æ–‡æœ¬çš„ tokens åºåˆ—ä½œä¸ºè¾“å…¥ï¼Œå¹¶è¿”å›ä¸‹ä¸€ä¸ª token çš„æ¦‚ç‡åˆ†å¸ƒã€‚\n",
    "\n",
    "LLMçš„å‰å‘ä¼ é€’ï¼š\n",
    "\n",
    "<video src=\"../../resources/show/gif/gif_1_1080p.mov\" loop autoplay controls></video>\n",
    "\n",
    "ä½¿ç”¨LLMè¿›è¡Œè‡ªå›å½’ç”Ÿæˆçš„ä¸€ä¸ªå…³é”®æ–¹é¢æ˜¯**å¦‚ä½•ä»è¿™ä¸ªæ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ª token**ã€‚è¿™ä¸ªæ­¥éª¤æ˜¯å–å†³äºä½ çš„éœ€æ±‚çš„ï¼Œå¯ä»¥æ˜¯ç®€å•åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©æœ€å¯èƒ½çš„ tokenï¼Œä¹Ÿå¯ä»¥æ˜¯å¤æ‚åœ°å¯¹ç»“æœåˆ†å¸ƒåº”ç”¨å¤šç§å˜æ¢åå†ä½œé€‰æ‹©ï¼Œåªè¦æœ€ç»ˆèƒ½å¤Ÿå¾—åˆ°ä¸‹ä¸€ä¸ªè¿­ä»£çš„ tokenã€‚\n",
    "\n",
    "è‡ªå›å½’ç”Ÿæˆè¿­ä»£åœ°ä»æ¦‚ç‡åˆ†å¸ƒä¸­é€‰æ‹©ä¸‹ä¸€ä¸ªtokenä»¥ç”Ÿæˆæ–‡æœ¬ï¼š\n",
    "\n",
    "<video src=\"../../resources/show/gif/gif_2_1080p.mov\" loop autoplay controls></video>\n",
    "\n",
    "ä¸Šè¿°è¿‡ç¨‹æ˜¯å¯ä»¥è¿­ä»£é‡å¤çš„ï¼Œç›´åˆ°è¾¾åˆ°æŸä¸ªåœæ­¢æ¡ä»¶ã€‚åœ¨ç†æƒ³æƒ…å†µä¸‹ï¼Œåœæ­¢æ¡ä»¶æ˜¯ç”±æ¨¡å‹å†³å®šçš„ï¼Œè¯¥æ¨¡å‹éœ€è¦å­¦ä¼šåœ¨ä½•æ—¶è¾“å‡ºä¸€ä¸ªç»“æŸåºåˆ—ï¼ˆEOSï¼‰æ ‡è®°ã€‚å¦‚æœä¸æ˜¯è¿™ç§æƒ…å†µï¼Œç”Ÿæˆå°†åœ¨è¾¾åˆ°æŸä¸ªé¢„å®šä¹‰çš„æœ€å¤§é•¿åº¦æ—¶åœæ­¢ã€‚\n",
    "\n",
    "**æ­£ç¡®è®¾ç½®ä¸‹ä¸€ä¸ª token çš„é€‰æ‹©æ­¥éª¤å’Œåœæ­¢æ¡ä»¶å¯¹äºè®©ä½ çš„æ¨¡å‹æŒ‰ç…§é¢„æœŸçš„æ–¹å¼æ‰§è¡Œä»»åŠ¡è‡³å…³é‡è¦ã€‚**è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæ¯ä¸ªæ¨¡å‹éƒ½è¦æœ‰ä¸€ä¸ª`generation.GenerationConfig`æ–‡ä»¶ï¼Œå®ƒé»˜è®¤é…ç½®äº†ä¸€ä¸ªæ•ˆæœä¸é”™çš„ç”Ÿæˆå‚æ•°ï¼Œä¼šå’Œä½ çš„æ¨¡å‹ä¸€èµ·åŠ è½½ã€‚\n",
    "\n",
    "å¦‚æœä½ å¯¹åŸºæœ¬çš„LLMä½¿ç”¨æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬é«˜çº§çš„`Pipeline`æ¥å£æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„èµ·ç‚¹ã€‚ç„¶è€Œï¼ŒLLMsé€šå¸¸éœ€è¦åƒ`quantization`å’Œå¯¹`token`é€‰æ‹©æ­¥éª¤çš„ç²¾ç»†æ§åˆ¶ç­‰é«˜çº§åŠŸèƒ½ï¼Œæ‰€ä»¥æœ€å¥½é€šè¿‡`generate()`æ¥å®Œæˆã€‚ä½¿ç”¨LLMè¿›è¡Œè‡ªå›å½’ç”Ÿæˆä¹Ÿæ˜¯èµ„æºå¯†é›†å‹çš„æ“ä½œï¼Œéœ€è¦åœ¨GPUä¸Šæ‰§è¡Œæ‰èƒ½è·å¾—è¶³å¤Ÿçš„ååé‡ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "é¦–å…ˆï¼Œä½ éœ€è¦åŠ è½½æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨`from_pretrained`è°ƒç”¨ä¸­çš„æœ‰ä¸¤ä¸ªå‚æ•°ï¼š\n",
    "\n",
    "- `device_map=\"auto\"`èƒ½å¤Ÿç¡®ä¿æ¨¡å‹è¢«ç§»åŠ¨åˆ°ä½ çš„GPU(s)ä¸Šã€‚\n",
    "- `load_in_4bit`è¡¨ç¤ºåº”ç”¨4ä½åŠ¨æ€é‡åŒ–æ¥æå¤§åœ°å‡å°‘èµ„æºéœ€æ±‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œä½ éœ€è¦ä½¿ç”¨ä¸€ä¸ª`tokenizer`æ¥é¢„å¤„ç†ä½ çš„æ–‡æœ¬è¾“å…¥ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model_inputs`å˜é‡ä¿å­˜ç€åˆ†è¯åçš„æ–‡æœ¬è¾“å…¥ä»¥åŠæ³¨æ„åŠ›æ©ç ã€‚`æ³¨æ„åŠ›æ©ç `èƒ½å¤Ÿç¡®ä¿æ¨¡å‹åªå…³æ³¨æœ‰æ•ˆçš„è¾“å…¥ tokensï¼Œè€Œå¿½ç•¥å¡«å…… tokensï¼Œä»è€Œä¿è¯ç”Ÿæˆçš„å‡†ç¡®æ€§å’Œè´¨é‡ã€‚\n",
    "\n",
    "å°½ç®¡`generate()`åœ¨æœªä¼ é€’æ³¨æ„åŠ›æ©ç æ—¶ä¼šå°½å…¶æ‰€èƒ½æ¨æ–­å‡ºæ³¨æ„åŠ›æ©ç ï¼Œä½†å»ºè®®å°½å¯èƒ½åœ°ä¼ é€’å®ƒä»¥è·å¾—æœ€ä½³ç»“æœã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨å¯¹è¾“å…¥è¿›è¡Œåˆ†è¯åï¼Œå¯ä»¥è°ƒç”¨`generate()`æ–¹æ³•æ¥è¿”å›ç”Ÿæˆçš„ tokensã€‚ç”Ÿæˆçš„ tokens éœ€è¦åœ¨æ‰“å°ä¹‹å‰è½¬æ¢ä¸ºæˆ‘ä»¬èƒ½å¤Ÿç†è§£çš„æ–‡æœ¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨æœ€åï¼Œä½ ä¸è¦ä¸€æ¬¡å¤„ç†åªä¸€ä¸ªåºåˆ—ï¼ä½ å¯ä»¥æ‰¹é‡è¾“å…¥ï¼Œè¿™ä¼šåœ¨å°å»¶è¿Ÿå’Œä½å†…å­˜æˆæœ¬ä¸‹æ˜¾è‘—æé«˜ååé‡ï¼Œä½ åªéœ€è¦ç¡®ä¿èƒ½å¤Ÿæ­£ç¡®åœ°å¡«å……ä½ çš„è¾“å…¥ï¼ˆè¯¦è§ä¸‹æ–‡ï¼‰ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token = tokenizer.eos_token  # å¤§å¤šæ•°LLMé»˜è®¤æƒ…å†µä¸‹æ²¡æœ‰è®¾ç½®å¡«å……æ ‡è®°\n",
    "model_inputs = tokenizer(\n",
    "    [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¸¸è§é™·é˜±\n",
    "\n",
    "å¯¹äºè®¸å¤šä¸åŒçš„[ç”Ÿæˆç­–ç•¥](https://huggingface.co/docs/transformers/main/en/generation_strategies)ï¼Œæœ‰æ—¶é»˜è®¤å€¼å¹¶ä¸é€‚åˆä½ çš„ç”¨ä¾‹ã€‚å¦‚æœæ¨¡å‹çš„è¾“å‡ºä¸ä½ æœŸæœ›çš„ç»“æœä¸åŒ¹é…ï¼Œæˆ‘ä»¬æ”¶é›†äº†ä¸€ä¸ªæœ€å¸¸è§çš„é™·é˜±åˆ—è¡¨æ¥ä»‹ç»å¦‚ä½•é¿å…å®ƒä»¬ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # å¤§å¤šæ•°LLMé»˜è®¤æƒ…å†µä¸‹æ²¡æœ‰å¡«å……æ ‡è®°\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", device_map=\"auto\", load_in_4bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ç”Ÿæˆçš„è¾“å‡ºå¤ªçŸ­/å¤ªé•¿\n",
    "å¦‚æœåœ¨`GenerationConfig`æ–‡ä»¶ä¸­æ²¡æœ‰æŒ‡å®šè¿”å› tokens çš„æœ€å¤§æ•°é‡ï¼Œ`generate()`é»˜è®¤åªè¿”å›20ä¸ª tokensã€‚\n",
    "\n",
    "å»ºè®®åœ¨ä½ çš„`generate`è°ƒç”¨ä¸­æ‰‹åŠ¨è®¾ç½®`max_new_tokens`ä»¥æ§åˆ¶å®ƒå¯ä»¥è¿”å›çš„`æ–°tokens`çš„æœ€å¤§æ•°é‡ã€‚**è¯·æ³¨æ„ï¼Œå¯¹äºä»…è§£ç å™¨æ¶æ„çš„LLMs(https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)ä¼šå°†è¾“å…¥æç¤ºä½œä¸ºè¾“å‡ºçš„ä¸€éƒ¨åˆ†è¿”å›ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# é»˜è®¤æƒ…å†µä¸‹ï¼Œè¾“å‡ºå°†æœ€å¤šåŒ…å«20ä¸ªæ ‡è®°ã€‚\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# è®¾ç½®`max_new_tokens`æ¥æ§åˆ¶è¾“å‡ºçš„æ–°tokensçš„æœ€å¤§é•¿åº¦\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=50)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„ç”Ÿæˆæ¨¡å¼(æ§åˆ¶æ¨¡å‹é€‰æ‹© Token çš„æ­¥éª¤)\n",
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œé™¤éåœ¨`[GenerationConfig](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationConfig)`æ–‡ä»¶ä¸­æŒ‡å®šï¼Œå¦åˆ™`generate()`ä¼šåœ¨æ¯ä¸ªè¿­ä»£ä¸­é€‰æ‹©æœ€å¯èƒ½çš„ tokenï¼ˆè´ªå©ªè§£ç ï¼‰ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼ŒåƒèŠå¤©æœºå™¨äººæˆ–å†™ä½œæ–‡ç« è¿™æ ·çš„åˆ›é€ æ€§ä»»åŠ¡ï¼Œè´ªå©ªè§£ç å¹¶ä¸æ˜¯æœ€ç†æƒ³çš„ç”Ÿæˆæ¨¡å¼ï¼›åƒéŸ³é¢‘è½¬å½•æˆ–ç¿»è¯‘è¿™æ ·çš„åŸºäºè¾“å…¥çš„ä»»åŠ¡ï¼Œè´ªå©ªè§£ç åˆ™æ˜¯ç›¸å¯¹ç†æƒ³çš„ç”Ÿæˆæ¨¡å¼ã€‚ä½ å¯ä»¥åœ¨è¿™ç¯‡[åšå®¢æ–‡ç« ](https://huggingface.co/blog/how-to-generate)ä¸­äº†è§£æ›´å¤šå…³äºè¿™ä¸ªè¯é¢˜çš„ä¿¡æ¯ã€‚\n",
    "\n",
    "åœ¨æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸­ï¼Œå°¤å…¶æ˜¯åœ¨è‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰ä»»åŠ¡ä¸­ï¼Œ`do_sample` å‚æ•°é€šå¸¸ç”¨äºæ§åˆ¶ç”Ÿæˆæ–‡æœ¬æ—¶çš„é‡‡æ ·ç­–ç•¥ã€‚å…·ä½“æ¥è¯´ï¼Œ`do_sample=True` çš„ä½œç”¨å¦‚ä¸‹ï¼š\n",
    "- å¯ç”¨éšæœºé‡‡æ ·ï¼š\n",
    "    å½“ `do_sample=True` æ—¶ï¼Œæ¨¡å‹åœ¨ç”Ÿæˆæ–‡æœ¬æ—¶ä¼šé‡‡ç”¨**éšæœºé‡‡æ ·**ç­–ç•¥ã€‚è¿™æ„å‘³ç€æ¨¡å‹ä¼šæ ¹æ®è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªè¯æˆ–æ ‡è®°ï¼Œè€Œä¸æ˜¯å§‹ç»ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ã€‚\n",
    "    è¿™ç§æ–¹æ³•å¯ä»¥å¢åŠ ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´è‡ªç„¶ï¼Œå‡å°‘é‡å¤å’Œå•è°ƒæ€§ã€‚\n",
    "- å¯¹æ¯”è´ªå©ªè§£ç ï¼š\n",
    "    å½“ `do_sample=False`æ—¶ï¼ˆæˆ–é»˜è®¤æƒ…å†µä¸‹ï¼‰ï¼Œæ¨¡å‹é€šå¸¸ä¼šä½¿ç”¨**è´ªå©ªè§£ç **ç­–ç•¥ï¼Œå³å§‹ç»ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„è¯ä½œä¸ºä¸‹ä¸€ä¸ªè¯ã€‚\n",
    "    è´ªå©ªè§£ç è™½ç„¶ç®€å•ä¸”é€Ÿåº¦å¿«ï¼Œä½†å¯èƒ½å¯¼è‡´ç”Ÿæˆçš„æ–‡æœ¬è¿‡äºä¿å®ˆå’Œé‡å¤ã€‚\n",
    "- æ¸©åº¦æ§åˆ¶ï¼š\n",
    "    åœ¨éšæœºé‡‡æ ·ä¸­ï¼Œå¸¸å¸¸ä¼šç»“åˆä¸€ä¸ªç§°ä¸ºâ€œæ¸©åº¦â€ï¼ˆ`temperature`ï¼‰çš„å‚æ•°æ¥è°ƒæ•´æ¦‚ç‡åˆ†å¸ƒçš„å¹³æ»‘ç¨‹åº¦ã€‚è¾ƒé«˜çš„æ¸©åº¦ä¼šä½¿æ¦‚ç‡åˆ†å¸ƒæ›´å¹³ç¼“ï¼Œå¢åŠ éšæœºæ€§ï¼›è¾ƒä½çš„æ¸©åº¦ä¼šä½¿åˆ†å¸ƒæ›´å°–é”ï¼Œå‡å°‘éšæœºæ€§ã€‚\n",
    "    å½“ `do_sample=Tru`e æ—¶ï¼Œå¯ä»¥é€šè¿‡è®¾ç½®æ¸©åº¦å‚æ•°æ¥è¿›ä¸€æ­¥æ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_seed æ–¹æ³•åœ¨ transformers åº“ä¸­ç”¨äºè®¾ç½®éšæœºæ•°ç”Ÿæˆå™¨çš„ç§å­ã€‚å…¶ä½œç”¨æ˜¯ç¡®ä¿å®éªŒçš„å¯é‡å¤æ€§ï¼ˆreproducibilityï¼‰ï¼Œå³åœ¨ç›¸åŒçš„æ¡ä»¶ä¸‹è¿è¡Œä»£ç æ—¶ï¼Œå¯ä»¥å¾—åˆ°ç›¸åŒçš„ç»“æœã€‚\n",
    "from transformers import set_seed\n",
    "set_seed(42)\n",
    "\n",
    "model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# LLM + è´ªå©ªè§£ç  = é‡å¤ã€ä¹å‘³çš„è¾“å‡º\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# é€šè¿‡è®¾ç½®é‡‡æ ·`do_sample=True`ï¼Œè¾“å‡ºå˜å¾—æ›´æœ‰åˆ›æ„ï¼\n",
    "generated_ids = model.generate(**model_inputs, do_sample=True)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„å¡«å……ä½ç½®\n",
    "\n",
    "å¯¹äºä»…è§£ç å™¨æ¶æ„çš„LLMsï¼Œå®ƒä»¬ä¼šæŒç»­è¿­ä»£ä½ çš„è¾“å…¥æç¤ºã€‚å¦‚æœä½ çš„è¾“å…¥é•¿åº¦ä¸ç›¸åŒï¼Œåˆ™éœ€è¦å¯¹å®ƒä»¬è¿›è¡Œå¡«å……ã€‚\n",
    "\n",
    "ç”±äºæŸäº› LLMsï¼ˆå¦‚GPTç³»åˆ—ï¼‰åœ¨é¢„è®­ç»ƒè¿‡ç¨‹ä¸­å¯èƒ½æ²¡æœ‰ä½¿ç”¨`pad tokens`ï¼Œæˆ–è€…æ²¡æœ‰å¯¹ pad tokens è¿›è¡Œç‰¹åˆ«çš„å¤„ç†ã€‚**å¦‚æœæ¨¡å‹æ²¡æœ‰ç»è¿‡pad tokensçš„è®­ç»ƒï¼Œç›´æ¥ä½¿ç”¨å³å¡«å……å¯èƒ½ä¼šå¯¼è‡´æ¨¡å‹åœ¨å¤„ç†è¾“å…¥æ—¶å‡ºç°é”™è¯¯ï¼Œå› ä¸ºæ¨¡å‹å¯èƒ½ä¼šå°†å¡«å……æ ‡è®°è¯¯è®¤ä¸ºæ˜¯æœ‰æ•ˆä¿¡æ¯ã€‚**\n",
    "\n",
    "å› æ­¤åœ¨å®é™…åº”ç”¨ä¸­ï¼Œ**è¾“å…¥åºåˆ—åº”é‡‡ç”¨å·¦å¡«å……æ–¹å¼**ã€‚åœ¨ç”Ÿæˆè¾“å‡ºæ—¶ï¼Œ**å¿…é¡»ä¼ é€’æ³¨æ„åŠ›æ©ç **ï¼Œä»¥ç¡®ä¿æ¨¡å‹æ­£ç¡®åœ°å¿½ç•¥å¡«å……æ ‡è®°ï¼Œåªå…³æ³¨æœ‰æ•ˆçš„è¾“å…¥ä¿¡æ¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸Šé¢åˆå§‹åŒ–çš„åˆ†è¯å™¨åœ¨é»˜è®¤æƒ…å†µä¸‹å¯ç”¨äº†å³å¡«å……ï¼š\n",
    "# ç¬¬ä¸€ä¸ªåºåˆ—è¾ƒçŸ­ï¼Œåœ¨åºåˆ—å³ä¾§è¿›è¡Œå¡«å……ï¼Œæ¨¡å‹å°†å¡«å……æ ‡è®°è¯¯è®¤ä¸ºæ˜¯æœ‰æ•ˆä¿¡æ¯ï¼Œå¯¼è‡´è¾“å‡ºçš„ç»“æœä¸ä½³ã€‚\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "# ä½¿ç”¨å·¦å¡«å……ï¼Œæ¨¡å‹ç”Ÿæˆäº†é¢„æœŸçš„ç»“æœï¼\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n",
    "tokenizer.pad_token = tokenizer.eos_token  # å¤§å¤šæ•°LLMé»˜è®¤æƒ…å†µä¸‹æ²¡æœ‰å¡«å……æ ‡è®°\n",
    "model_inputs = tokenizer(\n",
    "    [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs)\n",
    "tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é”™è¯¯çš„æç¤º\n",
    "\n",
    "æŸäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦è¾“å…¥ç‰¹å®šçš„æç¤ºæ ¼å¼æ‰èƒ½æ­£å¸¸å·¥ä½œã€‚å½“æœªä½¿ç”¨æ ‡å‡†çš„è¾“å…¥æ ¼å¼æ—¶ï¼Œæ¨¡å‹è™½èƒ½æ­£å¸¸å·¥ä½œï¼Œä½†æ˜¯æ€§èƒ½å°†ä¼šä¸‹é™ï¼Œè¾“å‡ºæ•ˆæœä¹Ÿä¸å¦‚é¢„æœŸã€‚\n",
    "\n",
    "æœ‰å…³æç¤ºçš„æ›´å¤šä¿¡æ¯ï¼ŒåŒ…æ‹¬å“ªäº›æ¨¡å‹å’Œä»»åŠ¡éœ€è¦æ³¨æ„è¾“å…¥çš„æç¤ºæ ¼å¼ï¼Œå¯åœ¨[æŒ‡å—](https://huggingface.co/docs/transformers/v4.44.2/en/tasks/prompting)ä¸­æ‰¾åˆ°ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ª[ä½¿ç”¨äº†èŠå¤©æ¨¡æ¿çš„ LLM ç¤ºä¾‹](https://huggingface.co/docs/transformers/main/zh/chat_templating)ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceH4/zephyr-7b-alpha\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"HuggingFaceH4/zephyr-7b-alpha\", device_map=\"auto\", load_in_4bit=True\n",
    ")\n",
    "set_seed(0)\n",
    "prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n",
    "model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.input_ids.shape[1]\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n",
    "# å“¦ï¼Œå®ƒæ²¡æœ‰æŒ‰ç…§æˆ‘ä»¬çš„æŒ‡ç¤ºä»¥æš´å¾’çš„é£æ ¼è¿›è¡Œå›å¤ï¼\n",
    "# è®©æˆ‘ä»¬çœ‹çœ‹å½“æˆ‘ä»¬å†™ä¸€ä¸ªæ›´å¥½çš„æç¤ºå¹¶ä¸ºè¿™ä¸ªæ¨¡å‹ä½¿ç”¨æ­£ç¡®çš„æ¨¡æ¿æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼ˆé€šè¿‡`tokenizer.apply_chat_template`ï¼‰\n",
    "\n",
    "set_seed(0)\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"You are a friendly chatbot who always responds in the style of a thug\",\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n",
    "]\n",
    "model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "input_length = model_inputs.shape[1]\n",
    "generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n",
    "print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n",
    "# æ­£å¦‚æˆ‘ä»¬æ‰€çœ‹åˆ°çš„ï¼Œå®ƒçš„å›å¤éµå¾ªäº†é€‚å½“çš„æš´å¾’é£æ ¼ ğŸ˜"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ›´å¤šèµ„æº\n",
    "\n",
    "è™½ç„¶è‡ªå›å½’ç”Ÿæˆè¿‡ç¨‹ç›¸å¯¹ç®€å•ï¼Œä½†è¦å……åˆ†åˆ©ç”¨ LLM çš„èƒ½åŠ›å¯èƒ½æ˜¯ä¸€ä¸ªå…·æœ‰æŒ‘æˆ˜æ€§çš„ä»»åŠ¡ï¼Œå› ä¸ºå¾ˆå¤šç»„ä»¶ä¹‹é—´éƒ½æœ‰ç€å¤æ‚ä¸”å¯†åˆ‡çš„å…³è”ã€‚ä»¥ä¸‹çš„èµ„æºå¯ä»¥å¸®åŠ©ä½ æ·±å…¥äº†è§£å’Œä½¿ç”¨LLMï¼š\n",
    "\n",
    "### é«˜çº§ç”Ÿæˆç”¨æ³•\n",
    "\n",
    "1. [ä»‹ç»å¦‚ä½•æ§åˆ¶ä¸åŒçš„ç”Ÿæˆæ–¹æ³•ã€å¦‚ä½•è®¾ç½®ç”Ÿæˆé…ç½®æ–‡ä»¶ä»¥åŠå¦‚ä½•è¿›è¡Œè¾“å‡ºæµå¼ä¼ è¾“](https://huggingface.co/docs/transformers/main/en/generation_strategies)ã€‚\n",
    "2. [ä»‹ç»èŠå¤©LLMsçš„æç¤ºæ¨¡æ¿](https://huggingface.co/docs/transformers/main/zh/chat_templating)ã€‚\n",
    "3. [ä»‹ç»å¦‚ä½•å……åˆ†åˆ©ç”¨æç¤ºè®¾è®¡](https://huggingface.co/docs/transformers/main/en/tasks/prompting)ã€‚\n",
    "4. APIå‚è€ƒæ–‡æ¡£ï¼ŒåŒ…æ‹¬ [GenerationConfig](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationConfig) ã€[generate()](https://huggingface.co/docs/transformers/main/zh/main_classes/text_generation#transformers.GenerationMixin.generate)å’Œ[ä¸ç”Ÿæˆç›¸å…³çš„ç±»](https://huggingface.co/docs/transformers/main/zh/internal/generation_utils)ã€‚\n",
    "\n",
    "### LLMæ’è¡Œæ¦œ\n",
    "\n",
    "1. [Open LLM Leaderboard](https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard)ï¼šä¾§é‡äºæ¯”è¾ƒå¼€æºæ¨¡å‹çš„è´¨é‡ã€‚\n",
    "2. [Open LLM-Perf Leaderboard](https://huggingface.co/spaces/optimum/llm-perf-leaderboard)ï¼šä¾§é‡äºæ¯”è¾ƒ LLM çš„ååé‡ã€‚\n",
    "\n",
    "### å»¶è¿Ÿã€ååé‡å’Œå†…å­˜åˆ©ç”¨ç‡\n",
    "\n",
    "1. [å¦‚ä½•ä¼˜åŒ–LLMsä»¥æé«˜é€Ÿåº¦å’Œå†…å­˜åˆ©ç”¨](https://huggingface.co/docs/transformers/main/en/llm_tutorial_optimization)ã€‚\n",
    "2. å…³äº[quantization](https://huggingface.co/docs/transformers/main/zh/main_classes/quantization)ï¼Œå¦‚ bitsandbytes å’Œ autogptq çš„æŒ‡å—ï¼Œæ•™ä½ å¦‚ä½•å¤§å¹…é™ä½å†…å­˜éœ€æ±‚ã€‚\n",
    "\n",
    "### ç›¸å…³åº“\n",
    "\n",
    "1. [text-generation-inference](https://github.com/huggingface/text-generation-inference)ï¼šä¸€ä¸ªé¢å‘ç”Ÿäº§çš„LLMæœåŠ¡å™¨ã€‚\n",
    "2. [optimum](https://github.com/huggingface/optimum)ï¼šä¸€ä¸ªğŸ¤— Transformersçš„æ‰©å±•ï¼Œä¼˜åŒ–ç‰¹å®šç¡¬ä»¶è®¾å¤‡çš„æ€§èƒ½ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
