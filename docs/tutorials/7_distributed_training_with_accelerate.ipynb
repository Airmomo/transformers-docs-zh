{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 🤗 Accelerate 进行分布式训练\n",
    "\n",
    "随着模型变得越来越大，并行性已经成为在有限硬件上训练更大模型和加速训练速度的策略，增加了数个数量级。\n",
    "\n",
    "在 Hugging Face中我们创建了[🤗 加速库 Accelerate](https://huggingface.co/docs/accelerate/index)，以帮助用户在任何类型的分布式设置上轻松训练🤗 Transformers模型，无论是在一台机器上的多个GPU还是在多个机器上的多个GPU。\n",
    "\n",
    "在本教程中，将了解如何自定义原生 PyTorch 训练循环，以启用在分布式环境中的训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置\n",
    "\n",
    "通过安装 🤗 Accelerate 开始:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后导入并创建`Accelerator`对象。Accelerator 将自动检测分布式设置的类型，并初始化所有必要的训练组件。你不需要显式地设置将模型放在哪个设备上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备加速\n",
    "\n",
    "下一步是将所有相关的训练对象传递给 `prepare` 方法。这包括训练和评估的DataLoader、一个模型和一个优化器:\n",
    "\n",
    "- `train_dataloader` 用于训练阶段，加载数据并分批提供给模型进行训练。\n",
    "- `eval_dataloader` 用于评估阶段，加载数据并分批提供给模型进行性能评估。\n",
    "\n",
    "**accelerator.prepare 的作用：**\n",
    "在 Hugging Face Accelerate 库中，accelerator.prepare 方法用于将 DataLoader、模型和优化器等对象包装起来，以便在分布式训练环境中正确地管理它们。具体作用包括：\n",
    "- 自动分发：将数据、模型和优化器自动分发到不同的 GPU 或 TPU 上。\n",
    "- 同步：在分布式训练中，确保各个进程之间的数据同步。\n",
    "- 优化：提供性能优化，如自动混合精度训练。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 反向传播\n",
    "\n",
    "最后将训练循环中的典型 loss.backward（）替换为 RoundAccelerate 的 [backward](https://huggingface.co/docs/accelerate/v1.0.0rc1/en/package_reference/accelerator#accelerate.Accelerator.backward) 方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如以下的代码所见，你只需要添加四行额外的代码到你的训练循环中即可启用分布式训练！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "accelerator = Accelerator()\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# - device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "# - model.to(device)\n",
    "\n",
    "# Add patch #\n",
    "train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(\n",
    "    train_dataloader, eval_dataloader, model, optimizer\n",
    ")\n",
    "# Add patch #\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "# -     batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "# -     loss.backward()\n",
    "        accelerator.backward(loss) # Add\n",
    "        \n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "在添加了相关代码行后，可以在脚本或 notebook（如Colaboratory）中启动训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用脚本训练\n",
    "\n",
    "如果在脚本中运行训练，请运行以下命令以创建和保存配置文件:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "accelerate config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后使用以下命令启动训练:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "accelerate launch train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 用 notebook 训练\n",
    "\n",
    "使用 Colaboratory 的 TPU 支持在 notebook 中运行 Accelerate。它将所有负责训练的代码包装在一个函数中，并将其传递给 [notebook_launcher](https://huggingface.co/docs/accelerate/v1.0.0rc1/en/package_reference/launchers#accelerate.notebook_launcher)："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "from accelerate import notebook_launcher\n",
    "\n",
    "notebook_launcher(training_function)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关 [🤗 Accelerate](https://huggingface.co/docs/accelerate/index) 及其丰富功能等更多信息，可参阅文档。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
