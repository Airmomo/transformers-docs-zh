{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用 🤗 PEFT 加载 adapters\n",
    "\n",
    "PEFT（Parameter-Efficient Fine-Tuning）是一种高效微调预训练模型的方法。具体来说，在微调过程中，预训练模型的大部分参数都被冻结，通过添加少量可训练的参数（如 adapters、low-rank matrices 等），调整它们以适应特定任务，而无需对整个预训练模型进行微调。\n",
    "\n",
    "`Adapters` 是一组轻量级的、可训练的参数，通常被添加到预训练模型的特定层中。由于 adapters 是独立的参数模块，通常比完整的模型小一个数量级，所以使其具有方便共享、存储和加载的特点。\n",
    "\n",
    "如图所示，与完整尺寸的模型权重（约为700MB）相比，存储在 Hub 上的 OPTForCausalLM 模型的 adapter 权重仅约为 6MB：\n",
    "\n",
    "![PEFT-hub-screenshot](../../resources/show/PEFT-hub-screenshot.png)\n",
    "\n",
    "通过使用 PEFT 和 adapters，可以在保持预训练模型大部分参数不变的情况下，高效地微调模型以适应特定任务，从而节省资源并提高性能。如果你对学习更多关于 [🤗 PEFT库](https://huggingface.co/docs/peft/index) 感兴趣，请查看文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## 设置\n",
    "\n",
    "peft 库是一个实现了参数高效微调（PEFT）方法的库，支持 LoRA（Low-Rank Adaptation）、Adapter Tuning、Prefix Tuning 等多种方法。\n",
    "\n",
    "peft 库的核心组件：\n",
    "- 配置类：如 `LoraConfig`、`AdapterConfig` 等配置类，用于定义和配置特定的 PEFT adapter，它将所有 Adapter 相关的配置参数都封装在一个独立的对象中，使得可以轻松地调整 Adapter 的配置，而不需要修改模型的其他部分。\n",
    "- 模型接口：提供了 `add_adapter`、`freeze_model` 等方法，用于在模型中添加和配置 adapter。\n",
    "- 训练工具：提供了训练和评估工具，帮助开发者高效地进行模型微调和评估。\n",
    "\n",
    "首先安装 🤗 peft："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你想尝试全新的特性，需要源代码安装这个库："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持的 PEFT 模型\n",
    "\n",
    "Transformers 原生支持一些 PEFT 库的方法，这意味着你可以加载在本地存储或在 Hub 上的 adapter 权重，只需要使用几行代码就可以轻松地运行或训练它们。以下是受支持的方法：\n",
    "\n",
    "- [Low Rank Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n",
    "- [AdaLoRA](https://arxiv.org/abs/2303.10512)\n",
    "\n",
    "如果你想使用其他 PEFT 库的方法，例如提示学习、提示微调，或者关于通用的 [🤗 PEFT库](https://huggingface.co/docs/peft/index)，可以参阅文档。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 加载 PEFT adapter 模型\n",
    "\n",
    "想从 huggingface 的 Transformers 库中加载并使用 PEFT adapter 模型，**请确保 Hub 仓库或本地目录包含一个 `adapter_config.json` 文件和 `adapter` 权重**。\n",
    "\n",
    "确认存在后，可以使用 `AutoModelFor` 类加载 `PEFT adapter` 模型。例如，想要给一个因果语言建模加载一个 PEFT adapter 模型：\n",
    "\n",
    "- 指定 PEFT 模型的 id\n",
    "- 将其传递给 AutoModelForCausalLM 类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以使用 AutoModelFor 类或基础模型类（如 OPTForCausalLM 或 LlamaForCausalLM ）来加载一个PEFT adapter。\n",
    "\n",
    "你也可以通过 `load_adapter` 方法来加载 PEFT adapter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基于8bit或4bit进行加载\n",
    "\n",
    "`bitsandbytes` 集成支持`8bit`和`4bit`精度数据类型，这对于加载大模型非常有用，因为它可以节省内存（请参阅[bitsandbytes指南](https://huggingface.co/docs/transformers/main/quantization/overview)以了解更多信息）。\n",
    "\n",
    "想要有效地将模型分配到你的硬件，请在 `from_pretrained()` 中添加参数`load_in_8bit`或`load_in_4bit`，并设置`device_map=\"auto\"`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 添加新的 adapter\n",
    "\n",
    "你可以使用`peft.PeftModel.add_adapter`方法为一个已有 adapter 的模型添加一个新的 adapter，只要新的 adapter 的类型与当前 adapter 相同即可。\n",
    "\n",
    "例如，如果你想添加一个 LoRA adapter 到模型上："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "添加一个新的 adapter："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用相同配置连接新的 adapter，并取别名为 adapter_2\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在你可以使用 `peft.PeftModel.set_adapter` 来设置要使用的 adapter。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用 adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "# 使用 adapter_2\n",
    "model.set_adapter(\"adapter_2\")\n",
    "output_enabled = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 启用和禁用 adapters\n",
    "一旦你将 adapter 添加到模型中，你随时可以选择启用或禁用 adapter 模块。\n",
    "\n",
    "若想要启用 adapter 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "若想要禁用 adapter 模块："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练一个 PEFT adapter\n",
    "\n",
    "PEFT 适配器受 Trainer 类支持，因此你可以选择特定的用例来训练 adapter。\n",
    "\n",
    "只需要添加几行代码即可。例如，要训练一个 LoRA adapter："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 创建一个 LoraConfig 或其他**适配器配置类**的实例，并设置所需的超参数。\n",
    "\n",
    "这里使用 PEFT 库中的 LoraConfig 类来配置 LoRA adapter，其中需要配置的超参数如下：\n",
    "\n",
    "- lora_alpha：控制低秩矩阵的大小，通常称为 \"rank\" 或 \"r\"。在这个例子中，lora_alpha=16 指定了低秩矩阵的大小。\n",
    "- lora_dropout：用于指定 dropout 的概率，lora_dropout=0.1 表示 dropout 的概率为 10%。\n",
    "- r：表示低秩矩阵的秩。在这个例子中，r=64 指定了低秩矩阵的秩为 64。\n",
    "- bias：用于指定是否添加偏差项。在这个例子中，bias=\"none\" 表示不添加偏差项。\n",
    "- task_type：用于指定任务的类型。在这个例子中，task_type=\"CAUSAL_LM\" 表示任务是因果语言模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将 adapter 添加到模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 现在可以将添加了 adapter 的模型传递给 Trainer 进行训练了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(..., model=model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存训练好的 adapter 并重新加载它："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
