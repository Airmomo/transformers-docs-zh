{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ ğŸ¤— PEFT åŠ è½½ adapters\n",
    "\n",
    "PEFTï¼ˆParameter-Efficient Fine-Tuningï¼‰æ˜¯ä¸€ç§é«˜æ•ˆå¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚å…·ä½“æ¥è¯´ï¼Œåœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ï¼Œé¢„è®­ç»ƒæ¨¡å‹çš„å¤§éƒ¨åˆ†å‚æ•°éƒ½è¢«å†»ç»“ï¼Œé€šè¿‡æ·»åŠ å°‘é‡å¯è®­ç»ƒçš„å‚æ•°ï¼ˆå¦‚ adaptersã€low-rank matrices ç­‰ï¼‰ï¼Œè°ƒæ•´å®ƒä»¬ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œè€Œæ— éœ€å¯¹æ•´ä¸ªé¢„è®­ç»ƒæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚\n",
    "\n",
    "`Adapters` æ˜¯ä¸€ç»„è½»é‡çº§çš„ã€å¯è®­ç»ƒçš„å‚æ•°ï¼Œé€šå¸¸è¢«æ·»åŠ åˆ°é¢„è®­ç»ƒæ¨¡å‹çš„ç‰¹å®šå±‚ä¸­ã€‚ç”±äº adapters æ˜¯ç‹¬ç«‹çš„å‚æ•°æ¨¡å—ï¼Œé€šå¸¸æ¯”å®Œæ•´çš„æ¨¡å‹å°ä¸€ä¸ªæ•°é‡çº§ï¼Œæ‰€ä»¥ä½¿å…¶å…·æœ‰æ–¹ä¾¿å…±äº«ã€å­˜å‚¨å’ŒåŠ è½½çš„ç‰¹ç‚¹ã€‚\n",
    "\n",
    "å¦‚å›¾æ‰€ç¤ºï¼Œä¸å®Œæ•´å°ºå¯¸çš„æ¨¡å‹æƒé‡ï¼ˆçº¦ä¸º700MBï¼‰ç›¸æ¯”ï¼Œå­˜å‚¨åœ¨ Hub ä¸Šçš„ OPTForCausalLM æ¨¡å‹çš„ adapter æƒé‡ä»…çº¦ä¸º 6MBï¼š\n",
    "\n",
    "![PEFT-hub-screenshot](../../resources/show/PEFT-hub-screenshot.png)\n",
    "\n",
    "é€šè¿‡ä½¿ç”¨ PEFT å’Œ adaptersï¼Œå¯ä»¥åœ¨ä¿æŒé¢„è®­ç»ƒæ¨¡å‹å¤§éƒ¨åˆ†å‚æ•°ä¸å˜çš„æƒ…å†µä¸‹ï¼Œé«˜æ•ˆåœ°å¾®è°ƒæ¨¡å‹ä»¥é€‚åº”ç‰¹å®šä»»åŠ¡ï¼Œä»è€ŒèŠ‚çœèµ„æºå¹¶æé«˜æ€§èƒ½ã€‚å¦‚æœä½ å¯¹å­¦ä¹ æ›´å¤šå…³äº [ğŸ¤— PEFTåº“](https://huggingface.co/docs/peft/index) æ„Ÿå…´è¶£ï¼Œè¯·æŸ¥çœ‹æ–‡æ¡£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## è®¾ç½®\n",
    "\n",
    "peft åº“æ˜¯ä¸€ä¸ªå®ç°äº†å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆPEFTï¼‰æ–¹æ³•çš„åº“ï¼Œæ”¯æŒ LoRAï¼ˆLow-Rank Adaptationï¼‰ã€Adapter Tuningã€Prefix Tuning ç­‰å¤šç§æ–¹æ³•ã€‚\n",
    "\n",
    "peft åº“çš„æ ¸å¿ƒç»„ä»¶ï¼š\n",
    "- é…ç½®ç±»ï¼šå¦‚ `LoraConfig`ã€`AdapterConfig` ç­‰é…ç½®ç±»ï¼Œç”¨äºå®šä¹‰å’Œé…ç½®ç‰¹å®šçš„ PEFT adapterï¼Œå®ƒå°†æ‰€æœ‰ Adapter ç›¸å…³çš„é…ç½®å‚æ•°éƒ½å°è£…åœ¨ä¸€ä¸ªç‹¬ç«‹çš„å¯¹è±¡ä¸­ï¼Œä½¿å¾—å¯ä»¥è½»æ¾åœ°è°ƒæ•´ Adapter çš„é…ç½®ï¼Œè€Œä¸éœ€è¦ä¿®æ”¹æ¨¡å‹çš„å…¶ä»–éƒ¨åˆ†ã€‚\n",
    "- æ¨¡å‹æ¥å£ï¼šæä¾›äº† `add_adapter`ã€`freeze_model` ç­‰æ–¹æ³•ï¼Œç”¨äºåœ¨æ¨¡å‹ä¸­æ·»åŠ å’Œé…ç½® adapterã€‚\n",
    "- è®­ç»ƒå·¥å…·ï¼šæä¾›äº†è®­ç»ƒå’Œè¯„ä¼°å·¥å…·ï¼Œå¸®åŠ©å¼€å‘è€…é«˜æ•ˆåœ°è¿›è¡Œæ¨¡å‹å¾®è°ƒå’Œè¯„ä¼°ã€‚\n",
    "\n",
    "é¦–å…ˆå®‰è£… ğŸ¤— peftï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install peft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æƒ³å°è¯•å…¨æ–°çš„ç‰¹æ€§ï¼Œéœ€è¦æºä»£ç å®‰è£…è¿™ä¸ªåº“ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install git+https://github.com/huggingface/peft.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ”¯æŒçš„ PEFT æ¨¡å‹\n",
    "\n",
    "Transformers åŸç”Ÿæ”¯æŒä¸€äº› PEFT åº“çš„æ–¹æ³•ï¼Œè¿™æ„å‘³ç€ä½ å¯ä»¥åŠ è½½åœ¨æœ¬åœ°å­˜å‚¨æˆ–åœ¨ Hub ä¸Šçš„ adapter æƒé‡ï¼Œåªéœ€è¦ä½¿ç”¨å‡ è¡Œä»£ç å°±å¯ä»¥è½»æ¾åœ°è¿è¡Œæˆ–è®­ç»ƒå®ƒä»¬ã€‚ä»¥ä¸‹æ˜¯å—æ”¯æŒçš„æ–¹æ³•ï¼š\n",
    "\n",
    "- [Low Rank Adapters](https://huggingface.co/docs/peft/main/en/conceptual_guides/lora)\n",
    "- [IA3](https://huggingface.co/docs/peft/conceptual_guides/ia3)\n",
    "- [AdaLoRA](https://arxiv.org/abs/2303.10512)\n",
    "\n",
    "å¦‚æœä½ æƒ³ä½¿ç”¨å…¶ä»– PEFT åº“çš„æ–¹æ³•ï¼Œä¾‹å¦‚æç¤ºå­¦ä¹ ã€æç¤ºå¾®è°ƒï¼Œæˆ–è€…å…³äºé€šç”¨çš„ [ğŸ¤— PEFTåº“](https://huggingface.co/docs/peft/index)ï¼Œå¯ä»¥å‚é˜…æ–‡æ¡£ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# åŠ è½½ PEFT adapter æ¨¡å‹\n",
    "\n",
    "æƒ³ä» huggingface çš„ Transformers åº“ä¸­åŠ è½½å¹¶ä½¿ç”¨ PEFT adapter æ¨¡å‹ï¼Œ**è¯·ç¡®ä¿ Hub ä»“åº“æˆ–æœ¬åœ°ç›®å½•åŒ…å«ä¸€ä¸ª `adapter_config.json` æ–‡ä»¶å’Œ `adapter` æƒé‡**ã€‚\n",
    "\n",
    "ç¡®è®¤å­˜åœ¨åï¼Œå¯ä»¥ä½¿ç”¨ `AutoModelFor` ç±»åŠ è½½ `PEFT adapter` æ¨¡å‹ã€‚ä¾‹å¦‚ï¼Œæƒ³è¦ç»™ä¸€ä¸ªå› æœè¯­è¨€å»ºæ¨¡åŠ è½½ä¸€ä¸ª PEFT adapter æ¨¡å‹ï¼š\n",
    "\n",
    "- æŒ‡å®š PEFT æ¨¡å‹çš„ id\n",
    "- å°†å…¶ä¼ é€’ç»™ AutoModelForCausalLM ç±»"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ å¯ä»¥ä½¿ç”¨ AutoModelFor ç±»æˆ–åŸºç¡€æ¨¡å‹ç±»ï¼ˆå¦‚ OPTForCausalLM æˆ– LlamaForCausalLM ï¼‰æ¥åŠ è½½ä¸€ä¸ªPEFT adapterã€‚\n",
    "\n",
    "ä½ ä¹Ÿå¯ä»¥é€šè¿‡ `load_adapter` æ–¹æ³•æ¥åŠ è½½ PEFT adapterã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "model.load_adapter(peft_model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åŸºäº8bitæˆ–4bitè¿›è¡ŒåŠ è½½\n",
    "\n",
    "`bitsandbytes` é›†æˆæ”¯æŒ`8bit`å’Œ`4bit`ç²¾åº¦æ•°æ®ç±»å‹ï¼Œè¿™å¯¹äºåŠ è½½å¤§æ¨¡å‹éå¸¸æœ‰ç”¨ï¼Œå› ä¸ºå®ƒå¯ä»¥èŠ‚çœå†…å­˜ï¼ˆè¯·å‚é˜…[bitsandbytesæŒ‡å—](https://huggingface.co/docs/transformers/main/quantization/overview)ä»¥äº†è§£æ›´å¤šä¿¡æ¯ï¼‰ã€‚\n",
    "\n",
    "æƒ³è¦æœ‰æ•ˆåœ°å°†æ¨¡å‹åˆ†é…åˆ°ä½ çš„ç¡¬ä»¶ï¼Œè¯·åœ¨ `from_pretrained()` ä¸­æ·»åŠ å‚æ•°`load_in_8bit`æˆ–`load_in_4bit`ï¼Œå¹¶è®¾ç½®`device_map=\"auto\"`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "peft_model_id = \"ybelkada/opt-350m-lora\"\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_model_id, quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ·»åŠ æ–°çš„ adapter\n",
    "\n",
    "ä½ å¯ä»¥ä½¿ç”¨`peft.PeftModel.add_adapter`æ–¹æ³•ä¸ºä¸€ä¸ªå·²æœ‰ adapter çš„æ¨¡å‹æ·»åŠ ä¸€ä¸ªæ–°çš„ adapterï¼Œåªè¦æ–°çš„ adapter çš„ç±»å‹ä¸å½“å‰ adapter ç›¸åŒå³å¯ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³æ·»åŠ ä¸€ä¸ª LoRA adapter åˆ°æ¨¡å‹ä¸Šï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    target_modules=[\"q_proj\", \"k_proj\"],\n",
    "    init_lora_weights=False\n",
    ")\n",
    "\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ·»åŠ ä¸€ä¸ªæ–°çš„ adapterï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ç›¸åŒé…ç½®è¿æ¥æ–°çš„ adapterï¼Œå¹¶å–åˆ«åä¸º adapter_2\n",
    "model.add_adapter(lora_config, adapter_name=\"adapter_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨ä½ å¯ä»¥ä½¿ç”¨ `peft.PeftModel.set_adapter` æ¥è®¾ç½®è¦ä½¿ç”¨çš„ adapterã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ adapter_1\n",
    "model.set_adapter(\"adapter_1\")\n",
    "output = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_disabled[0], skip_special_tokens=True))\n",
    "\n",
    "# ä½¿ç”¨ adapter_2\n",
    "model.set_adapter(\"adapter_2\")\n",
    "output_enabled = model.generate(**inputs)\n",
    "print(tokenizer.decode(output_enabled[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¯ç”¨å’Œç¦ç”¨ adapters\n",
    "ä¸€æ—¦ä½ å°† adapter æ·»åŠ åˆ°æ¨¡å‹ä¸­ï¼Œä½ éšæ—¶å¯ä»¥é€‰æ‹©å¯ç”¨æˆ–ç¦ç”¨ adapter æ¨¡å—ã€‚\n",
    "\n",
    "è‹¥æƒ³è¦å¯ç”¨ adapter æ¨¡å—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, OPTForCausalLM, AutoTokenizer\n",
    "from peft import PeftConfig\n",
    "\n",
    "model_id = \"facebook/opt-350m\"\n",
    "adapter_model_id = \"ybelkada/opt-350m-lora\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "text = \"Hello\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "peft_config = PeftConfig.from_pretrained(adapter_model_id)\n",
    "\n",
    "# to initiate with random weights\n",
    "peft_config.init_lora_weights = False\n",
    "\n",
    "model.add_adapter(peft_config)\n",
    "model.enable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è‹¥æƒ³è¦ç¦ç”¨ adapter æ¨¡å—ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.disable_adapters()\n",
    "output = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒä¸€ä¸ª PEFT adapter\n",
    "\n",
    "PEFT é€‚é…å™¨å— Trainer ç±»æ”¯æŒï¼Œå› æ­¤ä½ å¯ä»¥é€‰æ‹©ç‰¹å®šçš„ç”¨ä¾‹æ¥è®­ç»ƒ adapterã€‚\n",
    "\n",
    "åªéœ€è¦æ·»åŠ å‡ è¡Œä»£ç å³å¯ã€‚ä¾‹å¦‚ï¼Œè¦è®­ç»ƒä¸€ä¸ª LoRA adapterï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. åˆ›å»ºä¸€ä¸ª LoraConfig æˆ–å…¶ä»–**é€‚é…å™¨é…ç½®ç±»**çš„å®ä¾‹ï¼Œå¹¶è®¾ç½®æ‰€éœ€çš„è¶…å‚æ•°ã€‚\n",
    "\n",
    "è¿™é‡Œä½¿ç”¨ PEFT åº“ä¸­çš„ LoraConfig ç±»æ¥é…ç½® LoRA adapterï¼Œå…¶ä¸­éœ€è¦é…ç½®çš„è¶…å‚æ•°å¦‚ä¸‹ï¼š\n",
    "\n",
    "- lora_alphaï¼šæ§åˆ¶ä½ç§©çŸ©é˜µçš„å¤§å°ï¼Œé€šå¸¸ç§°ä¸º \"rank\" æˆ– \"r\"ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œlora_alpha=16 æŒ‡å®šäº†ä½ç§©çŸ©é˜µçš„å¤§å°ã€‚\n",
    "- lora_dropoutï¼šç”¨äºæŒ‡å®š dropout çš„æ¦‚ç‡ï¼Œlora_dropout=0.1 è¡¨ç¤º dropout çš„æ¦‚ç‡ä¸º 10%ã€‚\n",
    "- rï¼šè¡¨ç¤ºä½ç§©çŸ©é˜µçš„ç§©ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œr=64 æŒ‡å®šäº†ä½ç§©çŸ©é˜µçš„ç§©ä¸º 64ã€‚\n",
    "- biasï¼šç”¨äºæŒ‡å®šæ˜¯å¦æ·»åŠ åå·®é¡¹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œbias=\"none\" è¡¨ç¤ºä¸æ·»åŠ åå·®é¡¹ã€‚\n",
    "- task_typeï¼šç”¨äºæŒ‡å®šä»»åŠ¡çš„ç±»å‹ã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œtask_type=\"CAUSAL_LM\" è¡¨ç¤ºä»»åŠ¡æ˜¯å› æœè¯­è¨€æ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0.1,\n",
    "    r=64,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å°† adapter æ·»åŠ åˆ°æ¨¡å‹ä¸­ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_adapter(peft_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ç°åœ¨å¯ä»¥å°†æ·»åŠ äº† adapter çš„æ¨¡å‹ä¼ é€’ç»™ Trainer è¿›è¡Œè®­ç»ƒäº†ï¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(..., model=model)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä¿å­˜è®­ç»ƒå¥½çš„ adapter å¹¶é‡æ–°åŠ è½½å®ƒï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(save_dir)\n",
    "model = AutoModelForCausalLM.from_pretrained(save_dir)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
