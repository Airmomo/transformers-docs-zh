{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹\n",
    "\n",
    "ğŸ¤— Transformers æä¾›äº†æ¶‰åŠå„ç§ä»»åŠ¡çš„æˆåƒä¸Šä¸‡çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚å½“æ‚¨ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ‚¨éœ€è¦åœ¨ä¸ä»»åŠ¡ç›¸å…³çš„æ•°æ®é›†ä¸Šè®­ç»ƒè¯¥æ¨¡å‹ã€‚è¿™ç§æ“ä½œè¢«ç§°ä¸º`å¾®è°ƒ`ï¼Œæ˜¯ä¸€ç§éå¸¸å¼ºå¤§çš„è®­ç»ƒæŠ€æœ¯ã€‚\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œå°†é€‰æ‹©æ·±åº¦å­¦ä¹ æ¡†æ¶æ¥å¾®è°ƒä¸€ä¸ªé¢„è®­ç»ƒæ¨¡å‹ï¼š\n",
    "\n",
    "- ä½¿ç”¨ ğŸ¤— Transformers çš„ `Trainer` æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "- åœ¨ TensorFlow ä¸­ä½¿ç”¨ `Keras` æ¥å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚\n",
    "- åœ¨åŸç”Ÿ PyTorch ä¸­å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‡†å¤‡æ•°æ®é›†\n",
    "\n",
    "åœ¨å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œéœ€è¦ä¸‹è½½ä¸€ä¸ªæ•°æ®é›†ä¸ºè®­ç»ƒåšå¥½å‡†å¤‡ã€‚\n",
    "\n",
    "é¦–å…ˆï¼ŒåŠ è½½[Yelpè¯„è®º](https://huggingface.co/datasets/Yelp/yelp_review_full)æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¯¹äºæ–‡æœ¬ç±»å‹çš„æ•°æ®ï¼Œéœ€è¦ä¸€ä¸ª`tokenizer`æ¥å¤„ç†æ–‡æœ¬ï¼ŒåŒ…æ‹¬å¡«å……å’Œæˆªæ–­æ“ä½œä»¥å¤„ç†é•¿åº¦å¯å˜çš„åºåˆ—ã€‚\n",
    "\n",
    "å¦‚æœæƒ³è¦ä¸€æ¬¡æ€§å¤„ç†ä½ çš„æ•°æ®é›†ï¼Œå¯ä»¥å®šä¹‰ä¸€ä¸ªé¢„å¤„ç†å‡½æ•°ï¼Œå†ä½¿ç”¨ ğŸ¤— Datasets çš„ `map` æ–¹æ³•å°†é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿˜å¯ä»¥åœ¨å®Œæ•´çš„æ•°æ®é›†ä¸­æå–ä¸€ä¸ªè¾ƒå°çš„å­é›†æ¥è¿›è¡Œå¾®è°ƒï¼Œä»¥å‡å°‘è®­ç»ƒæ‰€éœ€çš„æ—¶é—´ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è®­ç»ƒ\n",
    "\n",
    "åœ¨æ¥ä¸‹æ¥ï¼Œå¯ä»¥åº”è¯¥æ ¹æ®ä½ è®­ç»ƒæ‰€ç”¨çš„æ¡†æ¶æ¥é€‰æ‹©å¯¹åº”çš„æ•™ç¨‹ç« èŠ‚ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨ PyTorch Trainer è¿›è¡Œè®­ç»ƒ\n",
    "\n",
    "ğŸ¤— Transformers æä¾›äº†ä¸€ä¸ªä¸“ä¸ºè®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹è€Œä¼˜åŒ–çš„ `Trainer` ç±»ï¼Œä½¿æ‚¨æ— éœ€æ‰‹åŠ¨ç¼–å†™è‡ªå·±çš„è®­ç»ƒå¾ªç¯æ­¥éª¤å°±å¯ä»¥å¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚\n",
    "\n",
    "`Trainer API` è¿˜åŒ…å«äº†å„ç§è®­ç»ƒé€‰é¡¹å’ŒåŠŸèƒ½ï¼Œå¦‚æ—¥å¿—è®°å½•ã€æ¢¯åº¦ç´¯ç§¯å’Œæ··åˆç²¾åº¦ã€‚\n",
    "\n",
    "é¦–å…ˆï¼ŒåŠ è½½ä½ çš„æ¨¡å‹å¹¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ã€‚æ ¹æ® `Yelp Review` [æ•°æ®é›†å¡ç‰‡](https://huggingface.co/datasets/Yelp/yelp_review_full#data-fields)ï¼Œå¯ä»¥çŸ¥é“æœ‰äº”ä¸ªæ ‡ç­¾ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½ ä¼šçœ‹åˆ°ä¸€ä¸ªè­¦å‘Šï¼Œæåˆ°ä¸€äº›é¢„è®­ç»ƒæƒé‡æœªè¢«ä½¿ç”¨ï¼Œä»¥åŠä¸€äº›æƒé‡è¢«éšæœºåˆå§‹åŒ–ã€‚\n",
    "\n",
    "ä¸ç”¨æ‹…å¿ƒï¼Œè¿™æ˜¯å®Œå…¨æ­£å¸¸çš„ï¼`BERT` æ¨¡å‹é¢„è®­ç»ƒçš„`head`ä¼šè¢«åºŸå¼ƒï¼Œæ›¿æ¢ä¸ºä¸€ä¸ªéšæœºåˆå§‹åŒ–çš„åˆ†ç±»`head`ã€‚åœ¨ä½ çš„åºåˆ—åˆ†ç±»ä»»åŠ¡ä¸Šå¾®è°ƒè¿™ä¸ªæ–°æ¨¡å‹çš„`head`ï¼Œå°†é¢„è®­ç»ƒæ¨¡å‹çš„çŸ¥è¯†è½¬ç§»ç»™å®ƒã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒè¶…å‚æ•°\n",
    "\n",
    "æ¥ä¸‹æ¥ï¼Œåˆ›å»ºä¸€ä¸ª `TrainingArguments` ç±»ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰ä½ å¯ä»¥è°ƒæ•´çš„è¶…å‚æ•°ä»¥åŠç”¨äºæ¿€æ´»ä¸åŒè®­ç»ƒé€‰é¡¹çš„æ ‡å¿—ã€‚\n",
    "\n",
    "åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œä½ å¯ä»¥ä»é»˜è®¤çš„è®­ç»ƒ[è¶…å‚æ•°](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)å¼€å§‹ï¼Œéšæ—¶å¯ä»¥å°è¯•ä¸åŒçš„è®¾ç½®ä»¥æ‰¾åˆ°æœ€ä½³çš„è®¾ç½®ã€‚\n",
    "\n",
    "æŒ‡å®šä¿å­˜è®­ç»ƒæ£€æŸ¥ç‚¹çš„ä½ç½®ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯„ä¼°\n",
    "\n",
    "`Trainer` åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸ä¼šè‡ªåŠ¨è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½ã€‚ä½ éœ€è¦å‘ Trainer ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥è®¡ç®—å’Œå±•ç¤ºæ¨¡å‹çš„æ€§èƒ½æŒ‡æ ‡ã€‚\n",
    "\n",
    "ğŸ¤— `[Evaluate](https://huggingface.co/docs/evaluate/index)` åº“æä¾›äº†ä¸€ä¸ª `[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)` å‡½æ•°ï¼Œä½ å¯ä»¥ä½¿ç”¨ `evaluate.load` å‡½æ•°åŠ è½½å®ƒï¼ˆæœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…æ­¤[å¿«é€Ÿå…¥é—¨](https://huggingface.co/docs/evaluate/a_quick_tour)ï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ `metric` ä¸Šè°ƒç”¨ `compute` æ¥è®¡ç®—é¢„æµ‹çš„å‡†ç¡®æ€§ã€‚åœ¨å°†é¢„æµ‹ä¼ é€’ç»™ `compute` ä¹‹å‰ï¼Œè¿˜éœ€è¦å°†é¢„æµ‹è½¬æ¢ä¸º`logits`ï¼ˆè¯·è®°ä½ï¼Œæ‰€æœ‰ ğŸ¤— Transformers æ¨¡å‹éƒ½è¿”å› logitsï¼‰ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æƒ³è¦åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­ç›‘è§†è¯„ä¼°æŒ‡æ ‡ï¼Œåˆ™éœ€è¦åœ¨è®­ç»ƒå‚æ•°ä¸­è®¾ç½® `eval_strategy` å‚æ•°ï¼Œä»¥åœ¨æ¯ä¸ª`epoch`ç»“æŸæ—¶å±•ç¤ºè¯„ä¼°æŒ‡æ ‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå™¨\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ªåŒ…å«æ¨¡å‹ã€è®­ç»ƒå‚æ•°ã€è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†ä»¥åŠè¯„ä¼°å‡½æ•°çš„ `Trainer` å¯¹è±¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç„¶åè°ƒç”¨`train()`æ¥å¾®è°ƒæ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ä½¿ç”¨kerasè®­ç»ƒTensorFlowæ¨¡å‹\n",
    "\n",
    "åœ¨ TensorFlow ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ `Keras API` è®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### åŠ è½½ç”¨äº Keras çš„æ•°æ®\n",
    "\n",
    "å½“ä½¿ç”¨ `Keras API` è®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹æ—¶ï¼Œä½ éœ€è¦å…ˆå°†æ•°æ®é›†è½¬æ¢ä¸º `Keras` å¯ç†è§£çš„æ ¼å¼ã€‚\n",
    "\n",
    "å¦‚æœä½ çš„æ•°æ®é›†å¾ˆå°ï¼Œå¯ä»¥å°†æ•´ä¸ªæ•°æ®é›†éƒ½è½¬æ¢ä¸º`NumPy æ•°ç»„`å¹¶ä¼ é€’ç»™ `Keras`ã€‚\n",
    "\n",
    "é¦–å…ˆï¼Œéœ€è¦åŠ è½½ä¸€ä¸ªæ•°æ®é›†ã€‚è¿™é‡Œä¼šä½¿ç”¨ `[GLUE benchmark](https://huggingface.co/datasets/nyu-mll/glue)` ä¸­çš„ `CoLA` æ•°æ®é›†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç®€å•çš„äºŒå…ƒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "dataset = dataset[\"train\"]  # ç°åœ¨åªä½¿ç”¨è®­ç»ƒæ•°æ®é›†"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ª`tokenizer`å¹¶å°†æ•°æ®æ ‡è®°ä¸º `NumPy æ•°ç»„`ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# åˆ†è¯å™¨è¿”å›ä¸€ä¸ª BatchEncodingï¼Œè¿™é‡Œæˆ‘ä»¬å°†å…¶è½¬æ¢ä¸º Keras çš„å­—å…¸\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(dataset[\"label\"])  # æ ‡ç­¾å·²ç»æ˜¯ä¸€ä¸ªç”±0å’Œ1ç»„æˆçš„æ•°ç»„ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„ï¼Œä¸éœ€è¦è¿›è¡Œåˆ†è¯å¤„ç†ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼ŒåŠ è½½ [compile(ç¼–è¯‘)](https://keras.io/api/models/model_training_apis/#compile-method) å’Œ [fitï¼ˆè®­ç»ƒï¼‰](https://keras.io/api/models/model_training_apis/#fit-method) æ¨¡å‹ï¼š\n",
    "\n",
    "**å½“æ‚¨ä½¿ç”¨ `compile()` ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œä½ å¯ä»¥æŒ‡å®šæŸå¤±å‡½æ•°ä»¥è¦†ç›–é»˜è®¤é…ç½®ï¼Œ ä¹Ÿå¯ä»¥ä¸æŒ‡å®šï¼Œå› ä¸º Transformers æ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©é€‚åˆå…¶ä»»åŠ¡å’Œæ¨¡å‹æ¶æ„çš„æŸå¤±å‡½æ•°ã€‚**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# åŠ è½½å¹¶ç¼–è¯‘æˆ‘ä»¬çš„æ¨¡å‹\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# æŒ‡å®šè¾ƒä½çš„å­¦ä¹ ç‡(optimizer)ï¼Œé€šå¸¸æ›´é€‚åˆå¾®è°ƒæ¨¡å‹\n",
    "model.compile(optimizer=Adam(3e-5))  # æ²¡æœ‰æŒ‡å®šæŸå¤±å‡½æ•°\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¿™ç§æ–¹æ³•å¯¹äºè¾ƒå°çš„æ•°æ®é›†æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºè¾ƒå¤§çš„æ•°æ®é›†ï¼Œå¯èƒ½å°±ä¼šå‡ºç°é—®é¢˜ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿ\n",
    "\n",
    "å› ä¸ºåˆ†è¯åçš„æ•°ç»„å’Œæ ‡ç­¾å¿…é¡»è¢«å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œç”±äº NumPy æ— æ³•å¤„ç†â€œä¸è§„åˆ™çš„æ•°ç»„â€ï¼Œæ‰€ä»¥æ¯ä¸ªåˆ†è¯åçš„æ ·æœ¬é•¿åº¦éƒ½å¿…é¡»è¢«å¡«å……åˆ°ä¸æ•°æ®é›†ä¸­æœ€é•¿æ ·æœ¬ç›¸åŒçš„é•¿åº¦ã€‚è¿™ä¼šå¯¼è‡´æ•°ç»„å˜å¾—æ›´å¤§ï¼Œè€Œè¿™äº›`padding tokens`ä¹Ÿä¼šå‡æ…¢è®­ç»ƒçš„é€Ÿåº¦ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### å°†æ•°æ®åŠ è½½ä¸º tf.data.Dataset\n",
    "\n",
    "ä¸ºäº†é¿å…è®­ç»ƒé€Ÿåº¦å‡æ…¢ï¼Œå¯ä»¥å°†æ•°æ®åŠ è½½ä¸º `tf.data.Dataset`ã€‚ä½ å¯ä»¥è‡ªå®šä¹‰ `tf.data` æµæ°´çº¿ï¼Œä¹Ÿå¯ä»¥é€‰æ‹©ä»¥ä¸‹ä¸¤ç§æ–¹ä¾¿çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ï¼š\n",
    "\n",
    "- [prepare_tf_dataset()](https://huggingface.co/docs/transformers/v4.44.2/zh/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)ï¼šåœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ¨èä½¿ç”¨è¯¥æ–¹æ³•ï¼Œå› ä¸ºå®ƒæ˜¯æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œå¯ä»¥é€šè¿‡æ£€æŸ¥æ¨¡å‹æ¥è‡ªåŠ¨ç¡®å®šå“ªäº›åˆ—å¯ç”¨ä½œæ¨¡å‹çš„è¾“å…¥ï¼Œå¹¶ä¸¢å¼ƒå…¶ä»–åˆ—ä»¥åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ€§èƒ½æ›´å¥½çš„æ•°æ®é›†ã€‚\n",
    "- `to_tf_dataset`ï¼šè¿™ä¸ªæ–¹æ³•æ›´ä½çº§ï¼Œåœ¨ä½ æƒ³è¦å®Œå…¨æ§åˆ¶æ•°æ®é›†çš„åˆ›å»ºæ–¹å¼æ—¶ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šè¦åŒ…å«çš„ç¡®åˆ‡ `columns` å’Œ `label_cols` æ¥å®ç°ã€‚\n",
    "\n",
    "åœ¨ä½¿ç”¨ `prepare_tf_dataset()` ä¹‹å‰ï¼Œè¿˜éœ€è¦å°† `tokenizer` çš„è¾“å‡ºä½œä¸ºåˆ—æ•°æ®æ·»åŠ åˆ°æ•°æ®é›†ï¼Œå¦‚ä¸‹ä»£ç ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data):\n",
    "    # è¿”å›çš„é”®å°†è¢«æ·»åŠ åˆ°æ•°æ®é›†ä¸­ä½œä¸ºåˆ—ã€‚\n",
    "    return tokenizer(data[\"text\"])\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ³¨æ„ï¼Hugging Face çš„æ•°æ®é›†å­˜å‚¨é»˜è®¤åœ¨ç¡¬ç›˜ä¸Šï¼Œå› æ­¤è¿™ä¸ä¼šå¢åŠ å†…å­˜ä½¿ç”¨ï¼ä¸€æ—¦åˆ—è¢«æ·»åŠ ï¼Œä½ å¯ä»¥ä»æ•°æ®é›†ä¸­æµå¼åœ°ä¼ è¾“æ‰¹æ¬¡æ•°æ®ï¼Œå¹¶ä¸ºæ¯ä¸ªæ‰¹æ¬¡æ·»åŠ `padding tokens`ï¼Œè¿™ä¸ä¸€æ¬¡æ€§ä¸ºæ•´ä¸ªæ•°æ®é›†æ·»åŠ  padding tokens ç›¸æ¯”ï¼Œå¤§å¤§å‡å°‘äº† padding tokens çš„æ•°é‡ï¼Œé¿å…å‡æ…¢è®­ç»ƒçš„é€Ÿåº¦ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åœ¨ä¸Šé¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œä½ éœ€è¦å°† `tokenizer` ä¼ é€’ç»™ `prepare_tf_dataset()`ï¼Œä»¥ä¾¿å®ƒå¯ä»¥åœ¨åŠ è½½æ‰¹æ¬¡æ—¶æ­£ç¡®åœ°å¡«å……æ•°æ®é›†ã€‚\n",
    "\n",
    "- å¦‚æœæ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦è€Œä¸”ä¸éœ€è¦å¡«å……ï¼Œåˆ™å¯ä»¥è·³è¿‡æ­¤å‚æ•°ã€‚\n",
    "- å¦‚æœéœ€è¦æ‰§è¡Œæ¯”å¡«å……æ ·æœ¬æ›´å¤æ‚çš„æ“ä½œï¼ˆä¾‹å¦‚ï¼Œç”¨äºæ©ç è¯­è¨€æ¨¡å‹çš„tokens æ›¿æ¢ï¼‰ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ `collate_fn` å‚æ•°ï¼Œè€Œä¸æ˜¯ä¼ é€’ä¸€ä¸ªè‡ªå®šä¹‰å‡½æ•°æ¥å°†æ ·æœ¬åˆ—è¡¨è½¬æ¢ä¸ºæ‰¹æ¬¡å¹¶åº”ç”¨ä»»ä½•æ‰€éœ€çš„é¢„å¤„ç†æ“ä½œã€‚\n",
    "\n",
    "åœ¨åˆ›å»ºäº† `tf.data.Dataset` åï¼Œä½ å¯ä»¥åƒä»¥å‰ä¸€æ ·ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‡å®šè¾ƒä½çš„å­¦ä¹ ç‡(optimizer)ï¼Œé€šå¸¸æ›´é€‚åˆå¾®è°ƒæ¨¡å‹\n",
    "model.compile(optimizer=Adam(3e-5))  # æ²¡æœ‰æŒ‡å®šæŸå¤±å‡½æ•°\n",
    "# è®­ç»ƒæ¨¡å‹\n",
    "model.fit(tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨åŸç”Ÿ PyTorch ä¸­è®­ç»ƒ\n",
    "\n",
    "Trainer è´Ÿè´£è®­ç»ƒå¾ªç¯ï¼Œå…è®¸æ‚¨åœ¨ä¸€è¡Œä»£ç ä¸­å¾®è°ƒæ¨¡å‹ã€‚\n",
    "\n",
    "å¯¹äºå–œæ¬¢ç¼–å†™è‡ªå®šä¹‰çš„è®­ç»ƒå¾ªç¯çš„ç”¨æˆ·ï¼Œä¹Ÿå¯ä»¥åœ¨åŸç”Ÿ PyTorch ä¸­å¾®è°ƒ ğŸ¤— Transformers æ¨¡å‹ã€‚\n",
    "\n",
    "ç°åœ¨ï¼Œä½ å¯èƒ½éœ€è¦é‡æ–°å¯åŠ¨ notebookï¼Œæˆ–æ‰§è¡Œä»¥ä¸‹ä»£ç ä»¥é‡Šæ”¾ä¸€äº›å†…å­˜ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ä¸‹æ¥ï¼Œæ‰‹åŠ¨å¤„ç† tokenized_dataset ä»¥å‡†å¤‡è¿›è¡Œè®­ç»ƒï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ç§»é™¤ text åˆ—ï¼Œå› ä¸ºæ¨¡å‹ä¸æ¥å—åŸå§‹æ–‡æœ¬ä½œä¸ºè¾“å…¥ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å°† label åˆ—é‡å‘½åä¸º labelsï¼Œå› ä¸ºæ¨¡å‹æœŸæœ›å‚æ•°çš„åç§°æ˜¯ labelsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. è®¾ç½®æ•°æ®é›†è¿”å›çš„æ ¼å¼æ˜¯ PyTorchå¼ é‡ è€Œä¸æ˜¯ listsï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¥ç€ï¼Œåˆ›å»ºä¸€ä¸ªå…ˆå‰å±•ç¤ºçš„æ•°æ®é›†çš„è¾ƒå°å­é›†ï¼Œä»¥åŠ é€Ÿå¾®è°ƒè¿‡ç¨‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### é¢„å¤„ç†æ•°æ® - DataLoader\n",
    "\n",
    "ä¸ºä½ çš„è®­ç»ƒå’Œæµ‹è¯•æ•°æ®é›†åˆ›å»ºä¸€ä¸ª DataLoader ç±»ï¼Œä»¥ä¾¿åœ¨è¿­ä»£æ—¶å¯ä»¥å¤„ç†æ•°æ®æ‰¹æ¬¡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åŠ è½½æ‚¨çš„æ¨¡å‹ï¼Œå¹¶æŒ‡å®šæœŸæœ›çš„æ ‡ç­¾æ•°é‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä¼˜åŒ–å™¨ï¼ˆOptimizerï¼‰å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLearning Rate Schedulerï¼‰\n",
    "\n",
    "- Optimizer : ä¼˜åŒ–å™¨æ˜¯ç”¨äºè°ƒæ•´æ¨¡å‹å‚æ•°ä»¥æœ€å°åŒ–æŸå¤±å‡½æ•°çš„ç®—æ³•ï¼Œå¸¸è§çš„ç®—æ³•æœ‰SGDã€Adamã€RMSpropç­‰ã€‚åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä¼˜åŒ–å™¨æ ¹æ®æŸå¤±å‡½æ•°çš„æ¢¯åº¦æ¥æ›´æ–°æ¨¡å‹çš„æƒé‡å’Œåç½®ï¼Œä»è€Œé€æ­¥æ”¹è¿›æ¨¡å‹çš„æ€§èƒ½ã€‚  \n",
    "- Learning Rate Scheduler : å­¦ä¹ ç‡è°ƒåº¦å™¨ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­åŠ¨æ€è°ƒæ•´å­¦ä¹ ç‡ï¼Œå¸¸è§çš„æœ‰StepLRã€ExponentialLRã€ReduceLROnPlateauç­‰ã€‚é€‚å½“è°ƒæ•´å­¦ä¹ ç‡å¯ä»¥å¸®åŠ©æ¨¡å‹æ›´å¿«åœ°æ”¶æ•›ï¼Œå¹¶é¿å…è¿‡æ‹Ÿåˆã€‚\n",
    "\n",
    "åˆ›å»ºä¸€ä¸ª`optimizer`å’Œ`learning rate scheduler`ä»¥è¿›è¡Œæ¨¡å‹å¾®è°ƒã€‚è®©æˆ‘ä»¬ä½¿ç”¨ PyTorch ä¸­çš„ `[AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)` ä¼˜åŒ–å™¨ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "åˆ›å»ºæ¥è‡ª `Trainer` çš„é»˜è®¤ `learning rate scheduler`ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æœ€åï¼ŒæŒ‡å®š `device` ä»¥ä½¿ç”¨ GPUï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰ã€‚å¦åˆ™ï¼Œä½¿ç”¨ CPU è¿›è¡Œè®­ç»ƒå¯èƒ½éœ€è¦å‡ ä¸ªå°æ—¶ã€‚\n",
    "\n",
    "`Metal Performance Shaders (MPS)`ï¼šPyTorch é€šè¿‡ MPS åç«¯å®ç°å¯¹ Apple GPU çš„åŠ é€Ÿã€‚MPS æ˜¯ Apple æä¾›çš„ä¸€å¥—ç”¨äºé«˜æ€§èƒ½å›¾å½¢å’Œè®¡ç®—ä»»åŠ¡çš„æ¡†æ¶ï¼Œä½† PyTorch å¯¹ MPS çš„æ”¯æŒä»å¤„äºå®éªŒé˜¶æ®µï¼Œå¯é æ€§æœªå¾—åˆ°ä¿è¯ã€‚\n",
    "`Compute Unified Device Architecture (CUDA)`ï¼šé€šè¿‡ CUDAï¼ŒPyTorch èƒ½å¤Ÿåœ¨ NVIDIA çš„ GPU ä¸Šè¿›è¡Œé«˜æ•ˆçš„å¹¶è¡Œè®¡ç®—ï¼Œç‰¹åˆ«æ˜¯åœ¨æ·±åº¦å­¦ä¹ å’Œé«˜æ€§èƒ½è®¡ç®—ä»»åŠ¡ä¸­ã€‚**ç¡®ä¿ä½ çš„ PyTorch ç‰ˆæœ¬ä¸å®‰è£…çš„ CUDA ç‰ˆæœ¬å…¼å®¹ã€‚**\n",
    "\n",
    "ä¸‹é¢æ˜¯åœ¨ä¸åŒæ“ä½œç³»ç»Ÿä¸­ä½¿ç”¨ä¸åŒGPUåŠ é€Ÿæ¨¡å‹è®­ç»ƒçš„ç¤ºä¾‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA GPU - CUDA\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple GPU - MPS\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœæ²¡æœ‰ GPUï¼Œå¯ä»¥é€šè¿‡notebookå¹³å°å¦‚ [Colaboratory](https://colab.research.google.com/) æˆ– [SageMaker StudioLab](https://studiolab.sagemaker.aws/) æ¥å…è´¹è·å¾—äº‘ç«¯GPUä½¿ç”¨ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®­ç»ƒå¾ªç¯\n",
    "\n",
    "ä¸ºäº†è·Ÿè¸ªè®­ç»ƒè¿›åº¦ï¼Œä½¿ç”¨ [tqdm](https://tqdm.github.io/) åº“æ¥æ·»åŠ ä¸€ä¸ªè¿›åº¦æ¡ï¼Œæ˜¾ç¤ºè®­ç»ƒæ­¥æ•°çš„è¿›å±•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è¯„ä¼°\n",
    "\n",
    "å°±åƒæ‚¨åœ¨ Trainer ä¸­æ·»åŠ äº†ä¸€ä¸ªè¯„ä¼°å‡½æ•°ä¸€æ ·ï¼Œå½“ä½ ç¼–å†™è‡ªå®šä¹‰çš„è®­ç»ƒå¾ªç¯æ—¶ï¼Œä¹Ÿéœ€è¦åšåŒæ ·çš„äº‹æƒ…ã€‚ä½†ä¸åœ¨æ¯ä¸ª`epoch`ç»“æŸæ—¶è®¡ç®—å’Œå±•ç¤ºæŒ‡æ ‡ä¸åŒï¼Œè¿™ä¸€æ¬¡å°†ä½¿ç”¨ `add_batch` ç´¯ç§¯æ‰€æœ‰æ‰¹æ¬¡ï¼Œåœ¨æœ€åè®¡ç®—æŒ‡æ ‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é™„åŠ èµ„æº\n",
    "\n",
    "æ›´å¤šå¾®è°ƒä¾‹å­å¯å‚è€ƒå¦‚ä¸‹é“¾æ¥ï¼š\n",
    "\n",
    "[ğŸ¤— Transformers ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples) åŒ…å«ç”¨äºåœ¨ PyTorch å’Œ TensorFlow ä¸­è®­ç»ƒå¸¸è§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è„šæœ¬ã€‚\n",
    "\n",
    "[ğŸ¤— Transformers ç¬”è®°](https://huggingface.co/docs/transformers/main/en/notebooks) åŒ…å«é’ˆå¯¹ç‰¹å®šä»»åŠ¡åœ¨ PyTorch å’Œ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹çš„å„ç§ notebookã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
