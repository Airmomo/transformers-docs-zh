{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 微调预训练模型\n",
    "\n",
    "🤗 Transformers 提供了涉及各种任务的成千上万的预训练模型。当您使用预训练模型时，您需要在与任务相关的数据集上训练该模型。这种操作被称为`微调`，是一种非常强大的训练技术。\n",
    "\n",
    "在本教程中，将选择深度学习框架来微调一个预训练模型：\n",
    "\n",
    "- 使用 🤗 Transformers 的 `Trainer` 来微调预训练模型。\n",
    "- 在 TensorFlow 中使用 `Keras` 来微调预训练模型。\n",
    "- 在原生 PyTorch 中微调预训练模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 准备数据集\n",
    "\n",
    "在微调预训练模型之前，需要下载一个数据集为训练做好准备。\n",
    "\n",
    "首先，加载[Yelp评论](https://huggingface.co/datasets/Yelp/yelp_review_full)数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "dataset[\"train\"][100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于文本类型的数据，需要一个`tokenizer`来处理文本，包括填充和截断操作以处理长度可变的序列。\n",
    "\n",
    "如果想要一次性处理你的数据集，可以定义一个预处理函数，再使用 🤗 Datasets 的 `map` 方法将预处理函数应用于整个数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "还可以在完整的数据集中提取一个较小的子集来进行微调，以减少训练所需的时间："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练\n",
    "\n",
    "在接下来，可以应该根据你训练所用的框架来选择对应的教程章节。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用 PyTorch Trainer 进行训练\n",
    "\n",
    "🤗 Transformers 提供了一个专为训练 🤗 Transformers 模型而优化的 `Trainer` 类，使您无需手动编写自己的训练循环步骤就可以开始训练模型。\n",
    "\n",
    "`Trainer API` 还包含了各种训练选项和功能，如日志记录、梯度累积和混合精度。\n",
    "\n",
    "首先，加载你的模型并指定期望的标签数量。根据 `Yelp Review` [数据集卡片](https://huggingface.co/datasets/Yelp/yelp_review_full#data-fields)，可以知道有五个标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会看到一个警告，提到一些预训练权重未被使用，以及一些权重被随机初始化。\n",
    "\n",
    "不用担心，这是完全正常的！`BERT` 模型预训练的`head`会被废弃，替换为一个随机初始化的分类`head`。在你的序列分类任务上微调这个新模型的`head`，将预训练模型的知识转移给它。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练超参数\n",
    "\n",
    "接下来，创建一个 `TrainingArguments` 类，其中包含所有你可以调整的超参数以及用于激活不同训练选项的标志。\n",
    "\n",
    "在本教程中，你可以从默认的训练[超参数](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)开始，随时可以尝试不同的设置以找到最佳的设置。\n",
    "\n",
    "指定保存训练检查点的位置："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "\n",
    "`Trainer` 在训练过程中不会自动评估模型的性能。你需要向 Trainer 传递一个函数来计算和展示模型的性能指标。\n",
    "\n",
    "🤗 `[Evaluate](https://huggingface.co/docs/evaluate/index)` 库提供了一个 `[accuracy](https://huggingface.co/spaces/evaluate-metric/accuracy)` 函数，你可以使用 `evaluate.load` 函数加载它（有关更多信息，请参阅此[快速入门](https://huggingface.co/docs/evaluate/a_quick_tour)）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 `metric` 上调用 `compute` 来计算预测的准确性。在将预测传递给 `compute` 之前，还需要将预测转换为`logits`（请记住，所有 🤗 Transformers 模型都返回 logits）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你想要在微调过程中监视评估指标，则需要在训练参数中设置 `eval_strategy` 参数，以在每个`epoch`结束时展示评估指标："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", eval_strategy=\"epoch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练器\n",
    "\n",
    "创建一个包含模型、训练参数、训练和测试数据集以及评估函数的 `Trainer` 对象："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后调用`train()`来微调模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 使用keras训练TensorFlow模型\n",
    "\n",
    "在 TensorFlow 中，可以使用 `Keras API` 训练 🤗 Transformers 模型！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加载用于 Keras 的数据\n",
    "\n",
    "当使用 `Keras API` 训练 🤗 Transformers 模型时，你需要先将数据集转换为 `Keras` 可理解的格式。\n",
    "\n",
    "如果你的数据集很小，可以将整个数据集都转换为`NumPy 数组`并传递给 `Keras`。\n",
    "\n",
    "首先，需要加载一个数据集。这里会使用 `[GLUE benchmark](https://huggingface.co/datasets/nyu-mll/glue)` 中的 `CoLA` 数据集，因为它是一个简单的二元文本分类任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"glue\", \"cola\")\n",
    "dataset = dataset[\"train\"]  # 现在只使用训练数据集"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，加载一个`tokenizer`并将数据标记为 `NumPy 数组`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n",
    "# 分词器返回一个 BatchEncoding，这里我们将其转换为 Keras 的字典\n",
    "tokenized_data = dict(tokenized_data)\n",
    "\n",
    "labels = np.array(dataset[\"label\"])  # 标签已经是一个由0和1组成的数组，因此我们可以直接将其转换为 NumPy 数组，不需要进行分词处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，加载 [compile(编译)](https://keras.io/api/models/model_training_apis/#compile-method) 和 [fit（训练）](https://keras.io/api/models/model_training_apis/#fit-method) 模型：\n",
    "\n",
    "**当您使用 `compile()` 编译模型时，你可以指定损失函数以覆盖默认配置， 也可以不指定，因为 Transformers 模型会自动选择适合其任务和模型架构的损失函数。**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForSequenceClassification\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# 加载并编译我们的模型\n",
    "model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n",
    "# 指定较低的学习率(optimizer)，通常更适合微调模型\n",
    "model.compile(optimizer=Adam(3e-5))  # 没有指定损失函数\n",
    "# 训练模型\n",
    "model.fit(tokenized_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种方法对于较小的数据集效果很好，但对于较大的数据集，可能就会出现问题。为什么呢？\n",
    "\n",
    "因为分词后的数组和标签必须被完全加载到内存中，由于 NumPy 无法处理“不规则的数组”，所以每个分词后的样本长度都必须被填充到与数据集中最长样本相同的长度。这会导致数组变得更大，而这些`padding tokens`也会减慢训练的速度！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 将数据加载为 tf.data.Dataset\n",
    "\n",
    "为了避免训练速度减慢，可以将数据加载为 `tf.data.Dataset`。你可以自定义 `tf.data` 流水线，也可以选择以下两种方便的方法来实现这一点：\n",
    "\n",
    "- [prepare_tf_dataset()](https://huggingface.co/docs/transformers/v4.44.2/zh/main_classes/model#transformers.TFPreTrainedModel.prepare_tf_dataset)：在大多数情况下推荐使用该方法，因为它是模型上的一个方法，可以通过检查模型来自动确定哪些列可用作模型的输入，并丢弃其他列以创建一个更简单、性能更好的数据集。\n",
    "- `to_tf_dataset`：这个方法更低级，在你想要完全控制数据集的创建方式时，可以通过指定要包含的确切 `columns` 和 `label_cols` 来实现。\n",
    "\n",
    "在使用 `prepare_tf_dataset()` 之前，还需要将 `tokenizer` 的输出作为列数据添加到数据集，如下代码示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(data):\n",
    "    # 返回的键将被添加到数据集中作为列。\n",
    "    return tokenizer(data[\"text\"])\n",
    "\n",
    "\n",
    "dataset = dataset.map(tokenize_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意！Hugging Face 的数据集存储默认在硬盘上，因此这不会增加内存使用！一旦列被添加，你可以从数据集中流式地传输批次数据，并为每个批次添加`padding tokens`，这与一次性为整个数据集添加 padding tokens 相比，大大减少了 padding tokens 的数量，避免减慢训练的速度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的代码示例中，你需要将 `tokenizer` 传递给 `prepare_tf_dataset()`，以便它可以在加载批次时正确地填充数据集。\n",
    "\n",
    "- 如果数据集中的所有样本都具有相同的长度而且不需要填充，则可以跳过此参数。\n",
    "- 如果需要执行比填充样本更复杂的操作（例如，用于掩码语言模型的tokens 替换），则可以使用 `collate_fn` 参数，而不是传递一个自定义函数来将样本列表转换为批次并应用任何所需的预处理操作。\n",
    "\n",
    "在创建了 `tf.data.Dataset` 后，你可以像以前一样编译和训练模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 指定较低的学习率(optimizer)，通常更适合微调模型\n",
    "model.compile(optimizer=Adam(3e-5))  # 没有指定损失函数\n",
    "# 训练模型\n",
    "model.fit(tf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在原生 PyTorch 中训练\n",
    "\n",
    "Trainer 负责训练循环，允许您在一行代码中微调模型。\n",
    "\n",
    "对于喜欢编写自定义的训练循环的用户，也可以在原生 PyTorch 中微调 🤗 Transformers 模型。\n",
    "\n",
    "现在，你可能需要重新启动 notebook，或执行以下代码以释放一些内存："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model\n",
    "del trainer\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，手动处理 tokenized_dataset 以准备进行训练："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 移除 text 列，因为模型不接受原始文本作为输入："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. 将 label 列重命名为 labels，因为模型期望参数的名称是 labels："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 设置数据集返回的格式是 PyTorch张量 而不是 lists："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着，创建一个先前展示的数据集的较小子集，以加速微调过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 预处理数据 - DataLoader\n",
    "\n",
    "为你的训练和测试数据集创建一个 DataLoader 类，以便在迭代时可以处理数据批次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(small_train_dataset, shuffle=True, batch_size=8)\n",
    "eval_dataloader = DataLoader(small_eval_dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载您的模型，并指定期望的标签数量："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 优化器（Optimizer）和学习率调度器（Learning Rate Scheduler）\n",
    "\n",
    "- Optimizer : 优化器是用于调整模型参数以最小化损失函数的算法，常见的算法有SGD、Adam、RMSprop等。在训练过程中，优化器根据损失函数的梯度来更新模型的权重和偏置，从而逐步改进模型的性能。  \n",
    "- Learning Rate Scheduler : 学习率调度器用于在训练过程中动态调整学习率，常见的有StepLR、ExponentialLR、ReduceLROnPlateau等。适当调整学习率可以帮助模型更快地收敛，并避免过拟合。\n",
    "\n",
    "创建一个`optimizer`和`learning rate scheduler`以进行模型微调。让我们使用 PyTorch 中的 `[AdamW](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html)` 优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建来自 `Trainer` 的默认 `learning rate scheduler`："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，指定 `device` 以使用 GPU（如果有的话）。否则，使用 CPU 进行训练可能需要几个小时。\n",
    "\n",
    "`Metal Performance Shaders (MPS)`：PyTorch 通过 MPS 后端实现对 Apple GPU 的加速。MPS 是 Apple 提供的一套用于高性能图形和计算任务的框架，但 PyTorch 对 MPS 的支持仍处于实验阶段，可靠性未得到保证。\n",
    "`Compute Unified Device Architecture (CUDA)`：通过 CUDA，PyTorch 能够在 NVIDIA 的 GPU 上进行高效的并行计算，特别是在深度学习和高性能计算任务中。**确保你的 PyTorch 版本与安装的 CUDA 版本兼容。**\n",
    "\n",
    "下面是在不同操作系统中使用不同GPU加速模型训练的示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NVIDIA GPU - CUDA\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apple GPU - MPS\n",
    "import torch\n",
    "\n",
    "device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果没有 GPU，可以通过notebook平台如 [Colaboratory](https://colab.research.google.com/) 或 [SageMaker StudioLab](https://studiolab.sagemaker.aws/) 来免费获得云端GPU使用。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练循环\n",
    "\n",
    "为了跟踪训练进度，使用 [tqdm](https://tqdm.github.io/) 库来添加一个进度条，显示训练步数的进展："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评估\n",
    "\n",
    "就像您在 Trainer 中添加了一个评估函数一样，当你编写自定义的训练循环时，也需要做同样的事情。但与在每个`epoch`结束时计算和展示指标不同，这一次将使用 `add_batch` 累积所有批次，在最后计算指标。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "model.eval()\n",
    "for batch in eval_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions = torch.argmax(logits, dim=-1)\n",
    "    metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "\n",
    "metric.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 附加资源\n",
    "\n",
    "更多微调例子可参考如下链接：\n",
    "\n",
    "[🤗 Transformers 示例](https://github.com/huggingface/transformers/tree/main/examples) 包含用于在 PyTorch 和 TensorFlow 中训练常见自然语言处理任务的脚本。\n",
    "\n",
    "[🤗 Transformers 笔记](https://huggingface.co/docs/transformers/main/en/notebooks) 包含针对特定任务在 PyTorch 和 TensorFlow 中微调模型的各种 notebook。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
