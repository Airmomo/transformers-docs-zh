{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a2b66b4",
   "metadata": {},
   "source": [
    "# BERTology —— 基于 BERT 进行的相关研究\n",
    "\n",
    "有一个新兴的研究领域专注于研究像 BERT 这样的大规模转换器模型的内部工作原理（有些人称之为“BERTology”）。以下是一些这个领域的优秀例子：\n",
    "\n",
    "* Ian Tenney、Dipanjan Das 和 Ellie Pavlick 的《BERT 重新发现经典 NLP 流水线》：[链接](https://arxiv.org/abs/1905.05950)\n",
    "* Paul Michel、Omer Levy 和 Graham Neubig 的《十六个注意力头真的比一个好吗？》：[链接](https://arxiv.org/abs/1905.10650)\n",
    "* Kevin Clark、Urvashi Khandelwal、Omer Levy 和 Christopher D. Manning 的《BERT 关注什么？对 BERT 注意力的分析》：[链接](https://arxiv.org/abs/1906.04341)\n",
    "* 《CAT 探针：一种基于指标的方法来解释预训练编程语言模型如何关注代码结构》：[链接](https://arxiv.org/abs/2210.04633)\n",
    "\n",
    "为了帮助这一新领域的发展，我们在 BERT/GPT/GPT-2 模型中添加了一些额外的功能，主要是从 Paul Michel 的卓越工作中改编而来（[论文链接](https://arxiv.org/abs/1905.10650)）：\n",
    "\n",
    "* 访问 BERT/GPT/GPT-2 的所有隐藏状态，\n",
    "* 访问 BERT/GPT/GPT-2 每个注意力头的所有注意力权重，\n",
    "* 获取注意力头的输出值和梯度，以便计算头的重要性分数并修剪头（如[论文](https://arxiv.org/abs/1905.10650)中所述）。\n",
    "\n",
    "为了帮助您理解和使用这些功能，我们添加了一个特定的示例脚本：[bertology.py](https://github.com/huggingface/transformers/tree/main/examples/research_projects/bertology/run_bertology.py)，这个脚本可以从预训练的 GLUE 模型中提取信息并进行剪枝。\n",
    "\n",
    "---\n",
    "\n",
    "**注释：**\n",
    "\n",
    "* **隐藏状态**：在神经网络中，隐藏状态是每一层的输出，这些输出会传递给下一层。通过访问这些隐藏状态，研究人员可以更好地理解模型内部的工作机制。\n",
    "* **注意力权重**：在注意力机制中，每个位置（或“头”）对其他位置的关注程度用一个权重表示。通过分析这些权重，可以了解模型在处理不同任务时如何分配注意力。\n",
    "* **修剪头（Pruning Heads）**：通过计算每个注意力头的重要性分数，可以移除对模型性能影响较小的头，从而简化模型结构并提高效率。\n",
    "\n",
    "这些功能和方法为研究 BERT 等模型的内部工作原理提供了有力的支持。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
