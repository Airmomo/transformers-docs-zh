{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfbc857",
   "metadata": {},
   "source": [
    "# å¤§å‹è¯­è¨€æ¨¡å‹æ¨ç†ä¼˜åŒ–\n",
    "\n",
    "å¤§å‹è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰é€šè¿‡ç”Ÿæˆå…·æœ‰é«˜åº¦ç†è§£å’Œæµç•…æ€§çš„æ–‡æœ¬ï¼Œå°†èŠå¤©å’Œä»£ç è¡¥å…¨ç­‰æ–‡æœ¬ç”Ÿæˆåº”ç”¨æå‡åˆ°äº†æ–°çš„æ°´å¹³ã€‚ç„¶è€Œï¼Œä½¿ LLMs å¦‚æ­¤å¼ºå¤§çš„ä¸€ä¸ªå› ç´ â€”â€”å®ƒä»¬çš„ä½“ç§¯â€”â€”ä¹Ÿä¸ºæ¨ç†å¸¦æ¥äº†æŒ‘æˆ˜ã€‚\n",
    "\n",
    "åŸºæœ¬çš„æ¨ç†é€Ÿåº¦è¾ƒæ…¢ï¼Œå› ä¸ºæ¯æ¬¡ç”Ÿæˆä¸‹ä¸€ä¸ªæ ‡è®°æ—¶éƒ½éœ€è¦åå¤è°ƒç”¨ LLMã€‚éšç€ç”Ÿæˆçš„è¿›è¡Œï¼Œè¾“å…¥åºåˆ—ä¼šé€æ¸å˜é•¿ï¼Œè¿™ä½¿å¾— LLM çš„å¤„ç†æ—¶é—´è¶Šæ¥è¶Šé•¿ã€‚æ­¤å¤–ï¼ŒLLMs æ‹¥æœ‰æ•°åäº¿ä¸ªå‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨å†…å­˜ä¸­å­˜å‚¨å’Œå¤„ç†éå¸¸å›°éš¾ã€‚\n",
    "\n",
    "æœ¬æŒ‡å—å°†å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Transformers ä¸­çš„ä¼˜åŒ–æŠ€æœ¯æ¥åŠ é€Ÿ LLM æ¨ç†ã€‚\n",
    "\n",
    "Hugging Face è¿˜æä¾›äº† [æ–‡æœ¬ç”Ÿæˆæ¨ç† (TGI)](https://hf.co/docs/text-generation-inference)ï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç”¨äºéƒ¨ç½²å’Œæä¾›é«˜åº¦ä¼˜åŒ–çš„ LLM æ¨ç†çš„åº“ã€‚å®ƒåŒ…æ‹¬ä¸€äº› Transformers ä¸åŒ…å«çš„é¢å‘éƒ¨ç½²çš„ä¼˜åŒ–åŠŸèƒ½ï¼Œä¾‹å¦‚è¿ç»­æ‰¹å¤„ç†ä»¥æé«˜ååé‡å’Œå¤š GPU æ¨ç†çš„å¼ é‡å¹¶è¡Œæ€§ã€‚\n",
    "\n",
    "## é™æ€é”®å€¼ç¼“å­˜å’Œ torch.compile\n",
    "\n",
    "åœ¨è§£ç è¿‡ç¨‹ä¸­ï¼ŒLLM ä¼šè®¡ç®—æ¯ä¸ªè¾“å…¥æ ‡è®°çš„é”®å€¼å¯¹ï¼ˆkv å€¼ï¼‰ã€‚ç”±äºå®ƒæ˜¯è‡ªå›å½’çš„ï¼Œæ¯æ¬¡ç”Ÿæˆçš„è¾“å‡ºéƒ½ä¼šæˆä¸ºæ–°çš„è¾“å…¥ï¼Œå› æ­¤ä¼šé‡å¤è®¡ç®—ç›¸åŒçš„ kv å€¼ã€‚è¿™æ•ˆç‡ä¸é«˜ï¼Œå› ä¸ºå®ƒæ¯æ¬¡éƒ½åœ¨é‡æ–°è®¡ç®—ç›¸åŒçš„ kv å€¼ã€‚\n",
    "\n",
    "ä¸ºäº†ä¼˜åŒ–è¿™ä¸€ç‚¹ï¼Œå¯ä»¥ä½¿ç”¨ kv ç¼“å­˜æ¥å­˜å‚¨è¿‡å»çš„é”®å’Œå€¼ï¼Œè€Œä¸æ˜¯æ¯æ¬¡éƒ½é‡æ–°è®¡ç®—ã€‚ç„¶è€Œï¼Œç”±äº kv ç¼“å­˜åœ¨æ¯æ¬¡ç”Ÿæˆæ­¥éª¤ä¸­ä¼šå¢é•¿ï¼Œå¹¶ä¸”æ˜¯åŠ¨æ€çš„ï¼Œè¿™é˜»ç¢äº†ä½ åˆ©ç”¨ [`torch.compile`](./perf_torch_compile)ï¼Œè¿™æ˜¯ä¸€ä¸ªå¼ºå¤§çš„ä¼˜åŒ–å·¥å…·ï¼Œå¯ä»¥å°† PyTorch ä»£ç èåˆæˆå¿«é€Ÿä¸”ä¼˜åŒ–çš„å†…æ ¸ã€‚æˆ‘ä»¬æœ‰ä¸€æ•´ç¯‡å…³äº kv ç¼“å­˜çš„æŒ‡å— [åœ¨è¿™é‡Œ](./kv_cache)ã€‚\n",
    "\n",
    "é™æ€ kv ç¼“å­˜è§£å†³äº†è¿™ä¸ªé—®é¢˜ï¼Œé€šè¿‡é¢„å…ˆåˆ†é…æœ€å¤§å€¼æ¥é¢„åˆ†é… kv ç¼“å­˜çš„å¤§å°ï¼Œå…è®¸ä½ å°†å…¶ä¸ `torch.compile` ç»“åˆä½¿ç”¨ï¼Œä»è€Œå®ç°æœ€é«˜ 4 å€çš„é€Ÿåº¦æå‡ã€‚å…·ä½“é€Ÿåº¦æå‡å–å†³äºæ¨¡å‹å¤§å°ï¼ˆè¾ƒå¤§çš„æ¨¡å‹é€Ÿåº¦æå‡è¾ƒå°ï¼‰å’Œç¡¬ä»¶ã€‚\n",
    "\n",
    "ç›®å‰ï¼Œåªæœ‰ [Llama](./model_doc/llama2) å’Œå°‘æ•°å…¶ä»–æ¨¡å‹æ”¯æŒé™æ€ kv ç¼“å­˜å’Œ `torch.compile`ã€‚è¯·æŸ¥çœ‹ [è¿™ä¸ª issues](https://github.com/huggingface/transformers/issues/28981) è·å–å®æ—¶çš„æ¨¡å‹å…¼å®¹åˆ—è¡¨ã€‚\n",
    "\n",
    "æ ¹æ®ä»»åŠ¡çš„å¤æ‚æ€§ï¼Œé™æ€ kv ç¼“å­˜æœ‰ä¸‰ç§ä½¿ç”¨æ–¹å¼ï¼š\n",
    "\n",
    "1. åŸºæœ¬ä½¿ç”¨ï¼šåªéœ€åœ¨ `generation_config` ä¸­è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼ˆæ¨èï¼‰ï¼›\n",
    "2. é«˜çº§ä½¿ç”¨ï¼šå¤„ç†ä¸€ä¸ªå¤šè½®ç”Ÿæˆæˆ–è‡ªå®šä¹‰ç”Ÿæˆå¾ªç¯çš„ç¼“å­˜å¯¹è±¡ï¼›\n",
    "3. é«˜çº§ä½¿ç”¨ï¼šå¦‚æœå•å›¾å¯¹ä½ æœ‰æ„ä¹‰ï¼Œå¯ä»¥å°†æ•´ä¸ª `generate` å‡½æ•°ç¼–è¯‘æˆå•ä¸ªå›¾ã€‚\n",
    "\n",
    "é€‰æ‹©ä¸‹é¢çš„æ­£ç¡®æ ‡ç­¾ä»¥è·å–æ¯ç§æ–¹å¼çš„è¯¦ç»†è¯´æ˜ã€‚\n",
    "\n",
    "æ— è®ºä½¿ç”¨å“ªç§ç­–ç•¥ä¸ `torch.compile` ç»“åˆï¼Œå¦‚æœä½ å°† LLM è¾“å…¥å·¦å¡«å……åˆ°æœ‰é™çš„å‡ ä¸ªå€¼ï¼Œå¯ä»¥é¿å…ä¸å½¢çŠ¶ç›¸å…³çš„é‡æ–°ç¼–è¯‘ã€‚[`pad_to_multiple_of` åˆ†è¯å™¨æ ‡å¿—](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) æ˜¯ä½ çš„æœ‹å‹ï¼\n",
    "\n",
    "### åŸºæœ¬ä½¿ç”¨ï¼šgeneration_config\n",
    "\n",
    "### é«˜çº§ä½¿ç”¨ï¼šæ§åˆ¶é™æ€ç¼“å­˜\n",
    "\n",
    "### é«˜çº§ä½¿ç”¨ï¼šç«¯åˆ°ç«¯ç”Ÿæˆç¼–è¯‘\n",
    "\n",
    "ä»¥ä¸‹ç¤ºä¾‹ä½¿ç”¨ [Gemma](https://hf.co/google/gemma-2b) æ¨¡å‹ã€‚æ‰€æœ‰éœ€è¦åšçš„å°±æ˜¯ï¼š\n",
    "\n",
    "1. è®¿é—®æ¨¡å‹çš„ `generation_config` å±æ€§å¹¶å°† `cache_implementation` è®¾ç½®ä¸ºâ€œstaticâ€ï¼›\n",
    "2. è°ƒç”¨ `torch.compile` å¯¹æ¨¡å‹è¿›è¡Œç¼–è¯‘ï¼Œä»¥ä¾¿åœ¨é™æ€ kv ç¼“å­˜ä¸‹è¿è¡Œå‰å‘ä¼ æ’­ã€‚\n",
    "\n",
    "å°±æ˜¯è¿™æ ·ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # é˜²æ­¢é•¿æ—¶é—´è­¦å‘Š\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "input_text = \"The theory of special relativity states \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# è¾“å‡ºï¼š['The theory of special relativity states 1. The speed of light is constant in all inertial reference']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc272c",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨å†…éƒ¨ï¼Œ`generate` å°†å°è¯•é‡ç”¨åŒä¸€ä¸ªç¼“å­˜å¯¹è±¡ï¼Œä»è€Œé¿å…æ¯æ¬¡è°ƒç”¨æ—¶çš„é‡æ–°ç¼–è¯‘ã€‚é¿å…é‡æ–°ç¼–è¯‘å¯¹äºå……åˆ†åˆ©ç”¨ `torch.compile` è‡³å…³é‡è¦ï¼Œä½ åº”è¯¥æ³¨æ„ä»¥ä¸‹å‡ ç‚¹ï¼š\n",
    "\n",
    "1. å¦‚æœè°ƒç”¨ä¹‹é—´æ‰¹é‡å¤§å°å‘ç”Ÿå˜åŒ–æˆ–æœ€å¤§è¾“å‡ºé•¿åº¦å¢åŠ ï¼Œåˆ™ç¼“å­˜éœ€è¦é‡æ–°åˆå§‹åŒ–ï¼Œè§¦å‘æ–°çš„ç¼–è¯‘ï¼›\n",
    "2. ç¼–è¯‘å‡½æ•°çš„å‰å‡ æ¬¡è°ƒç”¨è¾ƒæ…¢ï¼Œå› ä¸ºå‡½æ•°æ­£åœ¨è¢«ç¼–è¯‘ã€‚\n",
    "\n",
    "å¯¹äºæ›´é«˜çº§çš„é™æ€ç¼“å­˜ä½¿ç”¨ï¼Œä¾‹å¦‚å¤šè½®å¯¹è¯ï¼Œæˆ‘ä»¬å»ºè®®åœ¨ [generate()](/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) å¤–éƒ¨å®ä¾‹åŒ–å’Œæ“ä½œç¼“å­˜å¯¹è±¡ã€‚è¯·å‚è§é«˜çº§ä½¿ç”¨æ ‡ç­¾ã€‚\n",
    "\n",
    "## æŠ•æœºè§£ç \n",
    "\n",
    "æœ‰å…³æ›´æ·±å…¥çš„è§£é‡Šï¼Œè¯·å‚é˜… [è¾…åŠ©ç”Ÿæˆï¼šé€šå‘ä½å»¶è¿Ÿæ–‡æœ¬ç”Ÿæˆçš„æ–°æ–¹å‘](https://hf.co/blog/assisted-generation) åšå®¢æ–‡ç« ï¼\n",
    "\n",
    "è‡ªå›å½’çš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œå¯¹äºæ¯ä¸ªè¾“å…¥æ ‡è®°ï¼Œä½ éœ€è¦åœ¨å‰å‘ä¼ æ’­æœŸé—´æ¯æ¬¡åŠ è½½æ¨¡å‹æƒé‡ã€‚è¿™å¯¹æ‹¥æœ‰æ•°åäº¿å‚æ•°çš„ LLM æ¥è¯´æ—¢æ…¢åˆç¹çã€‚æŠ•æœºè§£ç é€šè¿‡ä½¿ç”¨ç¬¬äºŒä¸ªè¾ƒå°ä¸”æ›´å¿«çš„è¾…åŠ©æ¨¡å‹æ¥ç”Ÿæˆå€™é€‰æ ‡è®°ï¼Œè¿™äº›å€™é€‰æ ‡è®°ç”±è¾ƒå¤§çš„ LLM åœ¨å•æ¬¡å‰å‘ä¼ æ’­ä¸­éªŒè¯æ¥ç¼“è§£è¿™ç§ç¼“æ…¢ã€‚å¦‚æœéªŒè¯çš„æ ‡è®°æ˜¯æ­£ç¡®çš„ï¼ŒLLM å®è´¨ä¸Šå¯ä»¥â€œå…è´¹â€è·å¾—è¿™äº›æ ‡è®°ï¼Œè€Œä¸éœ€è¦è‡ªå·±ç”Ÿæˆã€‚æ²¡æœ‰å‡†ç¡®æ€§ä¸‹é™ï¼Œå› ä¸ºéªŒè¯å‰å‘ä¼ æ’­ç¡®ä¿ç”Ÿæˆçš„è¾“å‡ºä¸ LLM è‡ªå·±ç”Ÿæˆçš„è¾“å‡ºç›¸åŒã€‚\n",
    "\n",
    "ä¸ºäº†è·å¾—æœ€å¤§çš„é€Ÿåº¦æå‡ï¼Œè¾…åŠ©æ¨¡å‹åº”è¯¥æ¯” LLM å°å¾—å¤šï¼Œè¿™æ ·å®ƒå¯ä»¥å¿«é€Ÿç”Ÿæˆæ ‡è®°ã€‚è¾…åŠ©æ¨¡å‹å’Œ LLM æ¨¡å‹è¿˜å¿…é¡»å…±äº«ç›¸åŒçš„åˆ†è¯å™¨ï¼Œä»¥é¿å…é‡æ–°ç¼–ç å’Œè§£ç æ ‡è®°ã€‚\n",
    "\n",
    "å¯ç”¨æŠ•æœºè§£ç çš„æ–¹æ³•æ˜¯åŠ è½½ä¸€ä¸ªè¾…åŠ©æ¨¡å‹å¹¶å°†å…¶ä¼ é€’ç»™ [generate()](/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) æ–¹æ³•ã€‚\n",
    "\n",
    "### è´ªå©ªæœç´¢\n",
    "\n",
    "### é‡‡æ ·\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # è‡ªåŠ¨æ£€æµ‹åº•å±‚è®¾å¤‡ç±»å‹ï¼ˆCUDA, CPU, XPU, MPS ç­‰ï¼‰\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# è¾“å‡ºï¼š[\"Einstein's theory of relativity states that the speed of light is constant.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6551b",
   "metadata": {},
   "source": [
    "\n",
    "### æç¤ºæŸ¥æ‰¾è§£ç \n",
    "\n",
    "æç¤ºæŸ¥æ‰¾è§£ç æ˜¯ä¸€ç§æŠ•æœºè§£ç çš„å˜ä½“ï¼Œä¹Ÿé€‚ç”¨äºè´ªå©ªæœç´¢å’Œé‡‡æ ·ã€‚æç¤ºæŸ¥æ‰¾ç‰¹åˆ«é€‚åˆäºè¾“å…¥ä¾èµ–çš„ä»»åŠ¡ï¼Œä¾‹å¦‚æ‘˜è¦ï¼Œå…¶ä¸­æç¤ºå’Œè¾“å‡ºä¹‹é—´ç»å¸¸æœ‰é‡å çš„å•è¯ã€‚è¿™äº›é‡å çš„ n-gram ç”¨ä½œ LLM çš„å€™é€‰æ ‡è®°ã€‚\n",
    "\n",
    "è¦å¯ç”¨æç¤ºæŸ¥æ‰¾è§£ç ï¼ŒæŒ‡å®š `prompt_lookup_num_tokens` å‚æ•°ä¸­çš„åº”é‡å çš„æ ‡è®°æ•°é‡ã€‚ç„¶åä½ å¯ä»¥å°†æ­¤å‚æ•°ä¼ é€’ç»™ [generate()](/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) æ–¹æ³•ã€‚\n",
    "\n",
    "### è´ªå©ªè§£ç \n",
    "\n",
    "### é‡‡æ ·\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # è‡ªåŠ¨æ£€æµ‹åº•å±‚è®¾å¤‡ç±»å‹ï¼ˆCUDA, CPU, XPU, MPS ç­‰ï¼‰\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "outputs = model.generate(**inputs, prompt_lookup_num_tokens=3)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# è¾“å‡ºï¼š['The second law of thermodynamics states that entropy increases with temperature.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8201b31",
   "metadata": {},
   "source": [
    "\n",
    "## æ³¨æ„åŠ›ä¼˜åŒ–\n",
    "\n",
    "å˜å‹å™¨æ¨¡å‹çš„ä¸€ä¸ªå·²çŸ¥é—®é¢˜æ˜¯ï¼Œè‡ªæ³¨æ„åŠ›æœºåˆ¶çš„è®¡ç®—å’Œå†…å­˜éœ€æ±‚éšç€è¾“å…¥æ ‡è®°æ•°é‡çš„å¢åŠ è€Œå‘ˆäºŒæ¬¡å¢é•¿ã€‚è¿™ä¸€é™åˆ¶åœ¨å¤„ç†æ›´é•¿åºåˆ—çš„ LLM ä¸­å°¤ä¸ºæ˜æ˜¾ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå¯ä»¥å°è¯• FlashAttention2 æˆ– PyTorch çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰ï¼Œè¿™ä¸¤ç§æ–¹æ³•éƒ½æ˜¯æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›å®ç°ï¼Œå¯ä»¥åŠ é€Ÿæ¨ç†ã€‚\n",
    "\n",
    "### FlashAttention-2\n",
    "\n",
    "FlashAttention å’Œ [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) å°†æ³¨æ„åŠ›è®¡ç®—åˆ†è§£æˆæ›´å°çš„éƒ¨åˆ†ï¼Œå¹¶å‡å°‘è¯»å†™ GPU å†…å­˜çš„ä¸­é—´æ“ä½œæ¬¡æ•°ï¼Œä»è€ŒåŠ é€Ÿæ¨ç†ã€‚FlashAttention-2 åœ¨åŸå§‹ FlashAttention ç®—æ³•çš„åŸºç¡€ä¸Šè¿›è¡Œäº†æ”¹è¿›ï¼Œä¸ä»…åœ¨åºåˆ—é•¿åº¦ç»´åº¦ä¸Šå¹¶è¡ŒåŒ–ï¼Œè¿˜æ›´å¥½åœ°åˆ’åˆ†äº†ç¡¬ä»¶ä¸Šçš„å·¥ä½œï¼Œå‡å°‘äº†åŒæ­¥å’Œé€šä¿¡å¼€é”€ã€‚\n",
    "\n",
    "è¦ä½¿ç”¨ FlashAttention-2ï¼Œå¯ä»¥åœ¨ [from_pretrained()](/docs/transformers/v4.47.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä¸­è®¾ç½® `attn_implementation=\"flash_attention_2\"`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef75d86",
   "metadata": {},
   "source": [
    "\n",
    "### ä½¿ç”¨ torch.compile å’Œæ— å¡«å……æ•°æ®æ”¶é›†å™¨è¿›è¡Œå¾®è°ƒ\n",
    "\n",
    "é™¤äº†ä¼˜åŒ–æ¨ç†å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡åœ¨å¾®è°ƒè¿‡ç¨‹ä¸­åˆ©ç”¨ torch.compile å’Œæ— å¡«å……æ•°æ®æ”¶é›†å™¨æ¥æé«˜å¤§å‹è¯­è¨€æ¨¡å‹çš„è®­ç»ƒæ•ˆç‡ã€‚è¿™ç§æ–¹æ³•å¯ä»¥æ˜¾è‘—åŠ å¿«è®­ç»ƒé€Ÿåº¦å¹¶å‡å°‘è®¡ç®—å¼€é”€ã€‚\n",
    "\n",
    "ä»¥ä¸‹æ˜¯å¦‚ä½•ä½¿ç”¨ TRL åº“ä¸­çš„ SFTTrainer å¾®è°ƒ Llama æ¨¡å‹ï¼ŒåŒæ—¶å¯ç”¨ torch_compile å¹¶ä½¿ç”¨æ— å¡«å……æ•°æ®æ”¶é›†å™¨ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### å¯¼å…¥ ###################\n",
    "import math\n",
    "import datasets\n",
    "import dataclasses\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "#################### åŠ è½½æ¨¡å‹å¹¶å¯ç”¨ Flash Attention ###################\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"flash_attention_2\"  # å¯ç”¨ FlashAttention-2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "#################### æ•°æ®é¢„å¤„ç†ï¼ˆæ— å¡«å……ï¼‰ ###################\n",
    "response_template = \"\\n### Label:\"\n",
    "response_template_ids = tokenizer.encode(\n",
    "    response_template, add_special_tokens=False\n",
    ")[2:]  # æ’é™¤ç‰¹æ®Šæ ‡è®°\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template_ids=response_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    "    ignore_index=-100,\n",
    "    padding_free=True  # å¯ç”¨æ— å¡«å……æ”¶é›†\n",
    ")\n",
    "\n",
    "def format_dataset(example):\n",
    "    return {\n",
    "        \"output\": example[\"output\"] + tokenizer.eos_token\n",
    "    }\n",
    "\n",
    "data_files = {\"train\": \"path/to/dataset\"}  # æ›¿æ¢ä¸ºä½ çš„æ•°æ®é›†è·¯å¾„\n",
    "json_dataset = datasets.load_dataset(\"json\", data_files=data_files)\n",
    "formatted_train_dataset = json_dataset[\"train\"].map(format_dataset)\n",
    "\n",
    "################# è®­ç»ƒé…ç½® ############################\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    include_tokens_per_second=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"output\",\n",
    "    torch_compile=True,  # å¯ç”¨ torch.compile\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\"\n",
    ")\n",
    "\n",
    "# å°† TrainingArguments è½¬æ¢ä¸º SFTConfig\n",
    "transformer_train_arg_fields = [x.name for x in dataclasses.fields(SFTConfig)]\n",
    "transformer_kwargs = {\n",
    "    k: v\n",
    "    for k, v in train_args.to_dict().items()\n",
    "    if k in transformer_train_arg_fields\n",
    "}\n",
    "training_args = SFTConfig(**transformer_kwargs)\n",
    "\n",
    "####################### å¾®è°ƒ #####################\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    dataset_text_field=\"output\",\n",
    "    args=training_args,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d7793",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "\n",
    "ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰åœ¨ PyTorch 2.0 ä¸­è‡ªåŠ¨å¯ç”¨ï¼Œå¹¶ä¸”æ”¯æŒ FlashAttentionã€xFormers å’Œ PyTorch çš„ C++ å®ç°ã€‚SDPA ä¼šåœ¨ä½¿ç”¨ CUDA åç«¯æ—¶é€‰æ‹©æ€§èƒ½æœ€é«˜çš„æ³¨æ„åŠ›ç®—æ³•ã€‚å¯¹äºå…¶ä»–åç«¯ï¼ŒSDPA é»˜è®¤ä½¿ç”¨ PyTorch çš„ C++ å®ç°ã€‚\n",
    "\n",
    "SDPA æ”¯æŒ FlashAttention-2ï¼Œåªè¦å®‰è£…äº†æœ€æ–°ç‰ˆæœ¬çš„ PyTorchã€‚\n",
    "\n",
    "ä½¿ç”¨ [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) ä¸Šä¸‹æ–‡ç®¡ç†å™¨æ¥æ˜¾å¼å¯ç”¨æˆ–ç¦ç”¨ä¸‰ä¸ªæ³¨æ„åŠ›ç®—æ³•ä¸­çš„ä»»ä½•ä¸€ä¸ªã€‚ä¾‹å¦‚ï¼Œè®¾ç½® `enable_flash=True` ä»¥å¯ç”¨ FlashAttentionã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a845e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51550f",
   "metadata": {},
   "source": [
    "\n",
    "## é‡åŒ–\n",
    "\n",
    "é‡åŒ–é€šè¿‡ä»¥è¾ƒä½ç²¾åº¦å­˜å‚¨ LLM æƒé‡æ¥å‡å°å…¶å¤§å°ã€‚è¿™é™ä½äº†å†…å­˜ä½¿ç”¨é‡ï¼Œä½¿å¾—åœ¨å—é™äº GPU å†…å­˜çš„æƒ…å†µä¸‹æ›´å®¹æ˜“åŠ è½½ LLM è¿›è¡Œæ¨ç†ã€‚å¦‚æœä½ ä¸å— GPU é™åˆ¶ï¼Œä¸ä¸€å®šéœ€è¦é‡åŒ–æ¨¡å‹ï¼Œå› ä¸ºé‡åŒ–å’Œåé‡åŒ–æƒé‡æ‰€éœ€çš„é¢å¤–æ­¥éª¤å¯èƒ½ä¼šå¸¦æ¥è½»å¾®çš„å»¶è¿Ÿæˆæœ¬ï¼ˆé™¤éä½¿ç”¨ AWQ å’Œèåˆ AWQ æ¨¡å—ï¼‰ã€‚\n",
    "\n",
    "æœ‰è®¸å¤šé‡åŒ–åº“å¯ç”¨ï¼ˆå‚è§ [é‡åŒ–](./quantization) æŒ‡å—ä»¥è·å–æ›´å¤šè¯¦ç»†ä¿¡æ¯ï¼‰ï¼Œå¦‚ Quantoã€AQLMã€AWQ å’Œ AutoGPTQã€‚ä½ å¯ä»¥å°è¯•è¿™äº›åº“ï¼Œçœ‹çœ‹å“ªä¸ªæœ€é€‚åˆä½ çš„ä½¿ç”¨åœºæ™¯ã€‚æˆ‘ä»¬ä¹Ÿæ¨èé˜…è¯» [ğŸ¤— Transformers ä¸­æœ¬æœºæ”¯æŒçš„é‡åŒ–æ–¹æ¡ˆæ¦‚è¿°](https://hf.co/blog/overview-quantization-transformers) åšå®¢æ–‡ç« ï¼Œè¯¥æ–‡ç« æ¯”è¾ƒäº† AutoGPTQ å’Œ bitsandbytesã€‚\n",
    "\n",
    "ä½¿ç”¨ä¸‹é¢çš„æ¨¡å‹å†…å­˜è®¡ç®—å™¨æ¥ä¼°ç®—å’Œæ¯”è¾ƒåŠ è½½æ¨¡å‹æ‰€éœ€çš„å†…å­˜ã€‚ä¾‹å¦‚ï¼Œå°è¯•ä¼°ç®—åŠ è½½ [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) æ‰€éœ€çš„å†…å­˜ã€‚\n",
    "\n",
    "è¦åœ¨åŠç²¾åº¦ä¸‹åŠ è½½ Mistral-7B-v0.1ï¼Œå¯ä»¥åœ¨ [from_pretrained()](/docs/transformers/v4.47.1/en/model_doc/auto#transformers.AutoModel.from_pretrained) æ–¹æ³•ä¸­å°† `torch_dtype` å‚æ•°è®¾ç½®ä¸º `torch.bfloat16`ã€‚è¿™éœ€è¦ 13.74GB çš„å†…å­˜ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31874941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33444d9",
   "metadata": {},
   "source": [
    "\n",
    "è¦åŠ è½½é‡åŒ–æ¨¡å‹ï¼ˆ8 ä½æˆ– 4 ä½ï¼‰è¿›è¡Œæ¨ç†ï¼Œå¯ä»¥å°è¯• [bitsandbytes](https://hf.co/docs/bitsandbytes)ï¼Œå¹¶å°† `load_in_4bit` æˆ– `load_in_8bit` å‚æ•°è®¾ç½®ä¸º `True`ã€‚åœ¨ 8 ä½ä¸‹åŠ è½½æ¨¡å‹åªéœ€è¦ 6.87 GB çš„å†…å­˜ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
