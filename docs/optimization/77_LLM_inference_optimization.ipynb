{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7cfbc857",
   "metadata": {},
   "source": [
    "# 大型语言模型推理优化\n",
    "\n",
    "大型语言模型（LLMs）通过生成具有高度理解和流畅性的文本，将聊天和代码补全等文本生成应用提升到了新的水平。然而，使 LLMs 如此强大的一个因素——它们的体积——也为推理带来了挑战。\n",
    "\n",
    "基本的推理速度较慢，因为每次生成下一个标记时都需要反复调用 LLM。随着生成的进行，输入序列会逐渐变长，这使得 LLM 的处理时间越来越长。此外，LLMs 拥有数十亿个参数，这些参数在内存中存储和处理非常困难。\n",
    "\n",
    "本指南将展示如何使用 Transformers 中的优化技术来加速 LLM 推理。\n",
    "\n",
    "Hugging Face 还提供了 [文本生成推理 (TGI)](https://hf.co/docs/text-generation-inference)，这是一个专门用于部署和提供高度优化的 LLM 推理的库。它包括一些 Transformers 不包含的面向部署的优化功能，例如连续批处理以提高吞吐量和多 GPU 推理的张量并行性。\n",
    "\n",
    "## 静态键值缓存和 torch.compile\n",
    "\n",
    "在解码过程中，LLM 会计算每个输入标记的键值对（kv 值）。由于它是自回归的，每次生成的输出都会成为新的输入，因此会重复计算相同的 kv 值。这效率不高，因为它每次都在重新计算相同的 kv 值。\n",
    "\n",
    "为了优化这一点，可以使用 kv 缓存来存储过去的键和值，而不是每次都重新计算。然而，由于 kv 缓存在每次生成步骤中会增长，并且是动态的，这阻碍了你利用 [`torch.compile`](./perf_torch_compile)，这是一个强大的优化工具，可以将 PyTorch 代码融合成快速且优化的内核。我们有一整篇关于 kv 缓存的指南 [在这里](./kv_cache)。\n",
    "\n",
    "静态 kv 缓存解决了这个问题，通过预先分配最大值来预分配 kv 缓存的大小，允许你将其与 `torch.compile` 结合使用，从而实现最高 4 倍的速度提升。具体速度提升取决于模型大小（较大的模型速度提升较小）和硬件。\n",
    "\n",
    "目前，只有 [Llama](./model_doc/llama2) 和少数其他模型支持静态 kv 缓存和 `torch.compile`。请查看 [这个 issues](https://github.com/huggingface/transformers/issues/28981) 获取实时的模型兼容列表。\n",
    "\n",
    "根据任务的复杂性，静态 kv 缓存有三种使用方式：\n",
    "\n",
    "1. 基本使用：只需在 `generation_config` 中设置一个标志（推荐）；\n",
    "2. 高级使用：处理一个多轮生成或自定义生成循环的缓存对象；\n",
    "3. 高级使用：如果单图对你有意义，可以将整个 `generate` 函数编译成单个图。\n",
    "\n",
    "选择下面的正确标签以获取每种方式的详细说明。\n",
    "\n",
    "无论使用哪种策略与 `torch.compile` 结合，如果你将 LLM 输入左填充到有限的几个值，可以避免与形状相关的重新编译。[`pad_to_multiple_of` 分词器标志](https://huggingface.co/docs/transformers/main_classes/tokenizer#transformers.PreTrainedTokenizer.__call__.pad_to_multiple_of) 是你的朋友！\n",
    "\n",
    "### 基本使用：generation_config\n",
    "\n",
    "### 高级使用：控制静态缓存\n",
    "\n",
    "### 高级使用：端到端生成编译\n",
    "\n",
    "以下示例使用 [Gemma](https://hf.co/google/gemma-2b) 模型。所有需要做的就是：\n",
    "\n",
    "1. 访问模型的 `generation_config` 属性并将 `cache_implementation` 设置为“static”；\n",
    "2. 调用 `torch.compile` 对模型进行编译，以便在静态 kv 缓存下运行前向传播。\n",
    "\n",
    "就是这样！\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0fd251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 防止长时间警告\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", torch_dtype=\"auto\", device_map=\"auto\")\n",
    "\n",
    "model.generation_config.cache_implementation = \"static\"\n",
    "\n",
    "model.forward = torch.compile(model.forward, mode=\"reduce-overhead\", fullgraph=True)\n",
    "input_text = \"The theory of special relativity states \"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n",
    "\n",
    "outputs = model.generate(**input_ids)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# 输出：['The theory of special relativity states 1. The speed of light is constant in all inertial reference']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cabc272c",
   "metadata": {},
   "source": [
    "\n",
    "在内部，`generate` 将尝试重用同一个缓存对象，从而避免每次调用时的重新编译。避免重新编译对于充分利用 `torch.compile` 至关重要，你应该注意以下几点：\n",
    "\n",
    "1. 如果调用之间批量大小发生变化或最大输出长度增加，则缓存需要重新初始化，触发新的编译；\n",
    "2. 编译函数的前几次调用较慢，因为函数正在被编译。\n",
    "\n",
    "对于更高级的静态缓存使用，例如多轮对话，我们建议在 [generate()](/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) 外部实例化和操作缓存对象。请参见高级使用标签。\n",
    "\n",
    "## 投机解码\n",
    "\n",
    "有关更深入的解释，请参阅 [辅助生成：通向低延迟文本生成的新方向](https://hf.co/blog/assisted-generation) 博客文章！\n",
    "\n",
    "自回归的一个问题是，对于每个输入标记，你需要在前向传播期间每次加载模型权重。这对拥有数十亿参数的 LLM 来说既慢又繁琐。投机解码通过使用第二个较小且更快的辅助模型来生成候选标记，这些候选标记由较大的 LLM 在单次前向传播中验证来缓解这种缓慢。如果验证的标记是正确的，LLM 实质上可以“免费”获得这些标记，而不需要自己生成。没有准确性下降，因为验证前向传播确保生成的输出与 LLM 自己生成的输出相同。\n",
    "\n",
    "为了获得最大的速度提升，辅助模型应该比 LLM 小得多，这样它可以快速生成标记。辅助模型和 LLM 模型还必须共享相同的分词器，以避免重新编码和解码标记。\n",
    "\n",
    "启用投机解码的方法是加载一个辅助模型并将其传递给 [generate()](/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) 方法。\n",
    "\n",
    "### 贪婪搜索\n",
    "\n",
    "### 采样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deecf0ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # 自动检测底层设备类型（CUDA, CPU, XPU, MPS 等）\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "outputs = model.generate(**inputs, assistant_model=assistant_model)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# 输出：[\"Einstein's theory of relativity states that the speed of light is constant.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2a6551b",
   "metadata": {},
   "source": [
    "\n",
    "### 提示查找解码\n",
    "\n",
    "提示查找解码是一种投机解码的变体，也适用于贪婪搜索和采样。提示查找特别适合于输入依赖的任务，例如摘要，其中提示和输出之间经常有重叠的单词。这些重叠的 n-gram 用作 LLM 的候选标记。\n",
    "\n",
    "要启用提示查找解码，指定 `prompt_lookup_num_tokens` 参数中的应重叠的标记数量。然后你可以将此参数传递给 [generate()](/docs/transformers/v4.47.1/en/main_classes/text_generation#transformers.GenerationMixin.generate) 方法。\n",
    "\n",
    "### 贪婪解码\n",
    "\n",
    "### 采样\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68b353",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # 自动检测底层设备类型（CUDA, CPU, XPU, MPS 等）\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n",
    "inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-1.3b\", torch_dtype=\"auto\").to(device)\n",
    "assistant_model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\").to(device)\n",
    "outputs = model.generate(**inputs, prompt_lookup_num_tokens=3)\n",
    "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
    "# 输出：['The second law of thermodynamics states that entropy increases with temperature.']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8201b31",
   "metadata": {},
   "source": [
    "\n",
    "## 注意力优化\n",
    "\n",
    "变压器模型的一个已知问题是，自注意力机制的计算和内存需求随着输入标记数量的增加而呈二次增长。这一限制在处理更长序列的 LLM 中尤为明显。为了解决这个问题，可以尝试 FlashAttention2 或 PyTorch 的缩放点积注意力（SDPA），这两种方法都是更高效的注意力实现，可以加速推理。\n",
    "\n",
    "### FlashAttention-2\n",
    "\n",
    "FlashAttention 和 [FlashAttention-2](./perf_infer_gpu_one#flashattention-2) 将注意力计算分解成更小的部分，并减少读写 GPU 内存的中间操作次数，从而加速推理。FlashAttention-2 在原始 FlashAttention 算法的基础上进行了改进，不仅在序列长度维度上并行化，还更好地划分了硬件上的工作，减少了同步和通信开销。\n",
    "\n",
    "要使用 FlashAttention-2，可以在 [from_pretrained()](/docs/transformers/v4.47.1/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) 方法中设置 `attn_implementation=\"flash_attention_2\"`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb65a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\",\n",
    "    quantization_config=quant_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef75d86",
   "metadata": {},
   "source": [
    "\n",
    "### 使用 torch.compile 和无填充数据收集器进行微调\n",
    "\n",
    "除了优化推理外，还可以通过在微调过程中利用 torch.compile 和无填充数据收集器来提高大型语言模型的训练效率。这种方法可以显著加快训练速度并减少计算开销。\n",
    "\n",
    "以下是如何使用 TRL 库中的 SFTTrainer 微调 Llama 模型，同时启用 torch_compile 并使用无填充数据收集器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a4d6881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################### 导入 ###################\n",
    "import math\n",
    "import datasets\n",
    "import dataclasses\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments\n",
    ")\n",
    "from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM\n",
    "\n",
    "#################### 加载模型并启用 Flash Attention ###################\n",
    "model_name = \"meta-llama/Llama-3.2-1B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    attn_implementation=\"flash_attention_2\"  # 启用 FlashAttention-2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "#################### 数据预处理（无填充） ###################\n",
    "response_template = \"\\n### Label:\"\n",
    "response_template_ids = tokenizer.encode(\n",
    "    response_template, add_special_tokens=False\n",
    ")[2:]  # 排除特殊标记\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(\n",
    "    response_template_ids=response_template_ids,\n",
    "    tokenizer=tokenizer,\n",
    "    ignore_index=-100,\n",
    "    padding_free=True  # 启用无填充收集\n",
    ")\n",
    "\n",
    "def format_dataset(example):\n",
    "    return {\n",
    "        \"output\": example[\"output\"] + tokenizer.eos_token\n",
    "    }\n",
    "\n",
    "data_files = {\"train\": \"path/to/dataset\"}  # 替换为你的数据集路径\n",
    "json_dataset = datasets.load_dataset(\"json\", data_files=data_files)\n",
    "formatted_train_dataset = json_dataset[\"train\"].map(format_dataset)\n",
    "\n",
    "################# 训练配置 ############################\n",
    "train_args = TrainingArguments(\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=1e-5,\n",
    "    weight_decay=0.0,\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    logging_steps=1,\n",
    "    include_tokens_per_second=True,\n",
    "    save_strategy=\"epoch\",\n",
    "    output_dir=\"output\",\n",
    "    torch_compile=True,  # 启用 torch.compile\n",
    "    torch_compile_backend=\"inductor\",\n",
    "    torch_compile_mode=\"default\"\n",
    ")\n",
    "\n",
    "# 将 TrainingArguments 转换为 SFTConfig\n",
    "transformer_train_arg_fields = [x.name for x in dataclasses.fields(SFTConfig)]\n",
    "transformer_kwargs = {\n",
    "    k: v\n",
    "    for k, v in train_args.to_dict().items()\n",
    "    if k in transformer_train_arg_fields\n",
    "}\n",
    "training_args = SFTConfig(**transformer_kwargs)\n",
    "\n",
    "####################### 微调 #####################\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=formatted_train_dataset,\n",
    "    data_collator=data_collator,\n",
    "    dataset_text_field=\"output\",\n",
    "    args=training_args,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183d7793",
   "metadata": {},
   "source": [
    "\n",
    "### PyTorch 缩放点积注意力\n",
    "\n",
    "缩放点积注意力（SDPA）在 PyTorch 2.0 中自动启用，并且支持 FlashAttention、xFormers 和 PyTorch 的 C++ 实现。SDPA 会在使用 CUDA 后端时选择性能最高的注意力算法。对于其他后端，SDPA 默认使用 PyTorch 的 C++ 实现。\n",
    "\n",
    "SDPA 支持 FlashAttention-2，只要安装了最新版本的 PyTorch。\n",
    "\n",
    "使用 [torch.backends.cuda.sdp_kernel](https://pytorch.org/docs/master/generated/torch.nn.functional.scaled_dot_product_attention.html) 上下文管理器来显式启用或禁用三个注意力算法中的任何一个。例如，设置 `enable_flash=True` 以启用 FlashAttention。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a845e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"google/gemma-2b\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c51550f",
   "metadata": {},
   "source": [
    "\n",
    "## 量化\n",
    "\n",
    "量化通过以较低精度存储 LLM 权重来减小其大小。这降低了内存使用量，使得在受限于 GPU 内存的情况下更容易加载 LLM 进行推理。如果你不受 GPU 限制，不一定需要量化模型，因为量化和反量化权重所需的额外步骤可能会带来轻微的延迟成本（除非使用 AWQ 和融合 AWQ 模块）。\n",
    "\n",
    "有许多量化库可用（参见 [量化](./quantization) 指南以获取更多详细信息），如 Quanto、AQLM、AWQ 和 AutoGPTQ。你可以尝试这些库，看看哪个最适合你的使用场景。我们也推荐阅读 [🤗 Transformers 中本机支持的量化方案概述](https://hf.co/blog/overview-quantization-transformers) 博客文章，该文章比较了 AutoGPTQ 和 bitsandbytes。\n",
    "\n",
    "使用下面的模型内存计算器来估算和比较加载模型所需的内存。例如，尝试估算加载 [Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1) 所需的内存。\n",
    "\n",
    "要在半精度下加载 Mistral-7B-v0.1，可以在 [from_pretrained()](/docs/transformers/v4.47.1/en/model_doc/auto#transformers.AutoModel.from_pretrained) 方法中将 `torch_dtype` 参数设置为 `torch.bfloat16`。这需要 13.74GB 的内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31874941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", torch_dtype=torch.bfloat16, device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d33444d9",
   "metadata": {},
   "source": [
    "\n",
    "要加载量化模型（8 位或 4 位）进行推理，可以尝试 [bitsandbytes](https://hf.co/docs/bitsandbytes)，并将 `load_in_4bit` 或 `load_in_8bit` 参数设置为 `True`。在 8 位下加载模型只需要 6.87 GB 的内存。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe4bf94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(load_in_8bit=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-v0.1\", quantization_config=quant_config, device_map=\"auto\"\n",
    ")"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
