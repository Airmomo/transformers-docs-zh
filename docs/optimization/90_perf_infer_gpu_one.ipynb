{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343b9b52",
   "metadata": {},
   "source": [
    "# GPU 推理优化\n",
    "\n",
    "GPU 是机器学习硬件的标准选择，与 CPU 相比，它们在内存带宽和并行性方面进行了优化。为了在现有或较旧的硬件上处理现代模型的更大规模，或加速大型模型的推理，可以使用多种优化方法。在本指南中，您将学习如何使用 FlashAttention-2（更高效的注意力机制）、BetterTransformer（PyTorch 本地快速执行路径）和 bitsandbytes 来量化模型以降低精度。最后，您将学习如何使用 🤗 Optimum 在 Nvidia 和 AMD GPU 上通过 ONNX Runtime 加速推理。\n",
    "\n",
    "大多数优化方法也适用于多 GPU 配置！\n",
    "\n",
    "## FlashAttention-2\n",
    "\n",
    "FlashAttention-2 是一种实验性的、更快、更高效的注意力机制实现，它可以通过以下方式显著加速推理：\n",
    "\n",
    "1. 在序列长度上并行计算注意力。\n",
    "2. 在 GPU 线程之间分配工作，减少线程之间的通信和共享内存的读写。\n",
    "\n",
    "FlashAttention-2 当前支持以下架构：\n",
    "- Bark\n",
    "- Bart\n",
    "- Chameleon\n",
    "- CLIP\n",
    "- Cohere\n",
    "- GLM\n",
    "- Dbrx\n",
    "- DistilBert\n",
    "- Gemma\n",
    "- Gemma2\n",
    "- GPT2\n",
    "- GPTBigCode\n",
    "- GPTNeo\n",
    "- GPTNeoX\n",
    "- GPT-J\n",
    "- Falcon\n",
    "- Llama\n",
    "- Llava\n",
    "- 和更多...\n",
    "\n",
    "您可以在 GitHub 上[请求支持其他模型](https://github.com/Dao-AILab/flash-attention/issues)。\n",
    "\n",
    "在开始之前，请确保已安装 FlashAttention-2：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db865ce7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b949d0d",
   "metadata": {},
   "source": [
    "\n",
    "要启用 FlashAttention-2，请在加载模型时传递参数 `attn_implementation=\"flash_attention_2\"`：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a257f8",
   "metadata": {},
   "source": [
    "\n",
    "FlashAttention-2 只能在模型的 `fp16` 或 `bf16` 精度下使用。确保在使用 FlashAttention-2 之前将模型转换为合适的精度并加载到支持的设备上。\n",
    "\n",
    "您可以结合其他优化技术（如量化）进一步加速推理：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-bit 量化\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# 4-bit 量化\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e46fa",
   "metadata": {},
   "source": [
    "\n",
    "### 期望的加速效果\n",
    "\n",
    "对于较长的序列，FlashAttention-2 可以显著加速推理。但是，FlashAttention-2 不支持带有填充标记的注意力分数计算，因此在批量推理时需要手动填充/去填充注意力分数，这会导致显著的减速。为了克服这一点，应该在训练期间避免填充标记（例如通过打包数据集或[连接序列](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)直到达到最大序列长度）。\n",
    "\n",
    "对于无填充标记的序列，单次前向传递的加速效果如下图所示：\n",
    "\n",
    "![falcon-7b 加速效果](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png)\n",
    "\n",
    "对于带有填充标记的序列，加速效果会有所不同：\n",
    "\n",
    "![llama-2 带填充标记的加速效果](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png)\n",
    "\n",
    "FlashAttention 更加节省内存，意味着您可以训练更长的序列而不出现内存不足的问题。对于更长的序列，内存使用量最多可减少 20 倍。更多详细信息请参阅 [flash-attention](https://github.com/Dao-AILab/flash-attention) 仓库。\n",
    "\n",
    "## PyTorch 缩放点积注意力\n",
    "\n",
    "PyTorch 的 `torch.nn.functional.scaled_dot_product_attention` (SDPA) 可以调用 FlashAttention 和高效的注意力内核。SDPA 支持正在加入 Transformers，并在 `torch>=2.1.1` 中默认使用。您也可以在加载模型时显式设置 `attn_implementation=\"sdpa\"`。\n",
    "\n",
    "SDPA 支持的架构包括：\n",
    "- Albert\n",
    "- Audio Spectrogram Transformer\n",
    "- Bart\n",
    "- Bert\n",
    "- BioGpt\n",
    "- CamemBERT\n",
    "- CLIP\n",
    "- GLM\n",
    "- Cohere\n",
    "- data2vec_audio\n",
    "- 和更多...\n",
    "\n",
    "FlashAttention 只能用于 `fp16` 或 `bf16` 精度的模型。SDPA 不支持某些注意力参数，如 `head_mask` 和 `output_attentions=True`。在这种情况下，您会看到警告信息，并会回退到较慢的 eager 实现。\n",
    "\n",
    "默认情况下，SDPA 会选择最高效的内核，但您可以使用 `torch.backends.cuda.sdp_kernel` 来检查特定设置（硬件、问题大小）下是否可用某个内核：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d192fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "input_text = \"Hello my dog is cute and\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5debb0",
   "metadata": {},
   "source": [
    "\n",
    "如果您遇到错误，可以尝试使用 PyTorch 的 nightly 版本，以获取更广泛的 FlashAttention 支持。\n",
    "\n",
    "## BetterTransformer\n",
    "\n",
    "BetterTransformer 是一种优化技术，通过 PyTorch 本地实现的快速执行路径（fastpath）加速推理。快速执行路径的两个优化点是：\n",
    "\n",
    "1. 融合（fusion），将多个连续的操作合并为一个“内核”以减少计算步骤。\n",
    "2. 跳过填充标记的固有稀疏性，以避免不必要的计算。\n",
    "\n",
    "BetterTransformer 还将所有注意力操作转换为更节省内存的缩放点积注意力（SDPA），并在后台调用优化内核（如 FlashAttention）。\n",
    "\n",
    "在使用 BetterTransformer 之前，请确保已安装 🤗 Optimum：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39317",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ba8f3",
   "metadata": {},
   "source": [
    "\n",
    "然后可以使用 `PreTrainedModel.to_bettertransformer()` 方法启用 BetterTransformer：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a06d32",
   "metadata": {},
   "source": [
    "\n",
    "如果您想恢复原始的 🤗 Transformers 模型，可以使用 `reverse_bettertransformer()` 方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f558ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.reverse_bettertransformer()\n",
    "model.save_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed28209",
   "metadata": {},
   "source": [
    "\n",
    "## bitsandbytes\n",
    "\n",
    "bitsandbytes 是一个量化库，支持 4 比特和 8 比特量化。量化可以减少模型的大小，使其更容易在内存有限的 GPU 上运行。\n",
    "\n",
    "安装 bitsandbytes 和 🤗 Accelerate：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe33e96",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install bitsandbytes>=0.39.0 accelerate>=0.20.0\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf46d35",
   "metadata": {},
   "source": [
    "\n",
    "### 4-bit\n",
    "\n",
    "要以 4 比特加载模型进行推理，使用 `load_in_4bit` 参数。`device_map` 参数是可选的，但建议设置为 `\"auto\"` 以允许 🤗 Accelerate 自动高效分配模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9582e9c",
   "metadata": {},
   "source": [
    "\n",
    "对于多 GPU 环境，您可以控制每个 GPU 分配的内存：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory_mapping = {0: \"600MB\", 1: \"1GB\"}\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\", load_in_4bit=True, max_memory=max_memory_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceee9d7",
   "metadata": {},
   "source": [
    "\n",
    "### 8-bit\n",
    "\n",
    "要以 8 比特加载模型进行推理，使用 `load_in_8bit` 参数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60c16f",
   "metadata": {},
   "source": [
    "\n",
    "对于多 GPU 环境，您可以控制每个 GPU 分配的内存：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b77390",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory_mapping = {0: \"1GB\", 1: \"2GB\"}\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01875ab7",
   "metadata": {},
   "source": [
    "\n",
    "## 🤗 Optimum\n",
    "\n",
    "🤗 Optimum 支持使用 ONNX Runtime（ORT）加速 Nvidia 和 AMD GPU 上的推理。ORT 使用诸如将常见操作融合为单个节点和常量折叠等优化技术，以减少计算步骤并加速推理。ORT 还将最密集的计算任务分配给 GPU，其余任务分配给 CPU，以智能分配工作负载。\n",
    "\n",
    "要使用 ORT，您需要使用一个特定任务的 [ORTModel](https://huggingface.co/docs/optimum/v1.23.3/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)，并指定 `provider` 参数，例如 `CUDAExecutionProvider`、`ROCMExecutionProvider` 或 `TensorrtExecutionProvider`。如果要导出模型为 ONNX 格式，可以设置 `export=True`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "  \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "  export=True,\n",
    "  provider=\"CUDAExecutionProvider\",\n",
    ")\n",
    "\n",
    "from optimum.pipelines import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "pipeline = pipeline(task=\"text-classification\", model=ort_model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "result = pipeline(\"Both the music and visual were astounding, not to mention the actors performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1afab",
   "metadata": {},
   "source": [
    "\n",
    "## 组合优化\n",
    "\n",
    "可以组合多种优化技术以获得最佳的推理性能。例如，您可以加载 4 比特量化模型并启用 BetterTransformer 和 FlashAttention：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# 加载 4 比特模型\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "# 启用 BetterTransformer\n",
    "model = model.to_bettertransformer()\n",
    "\n",
    "input_text = \"Hello my dog is cute and\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# 启用 FlashAttention\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
