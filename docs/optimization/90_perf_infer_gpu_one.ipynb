{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "343b9b52",
   "metadata": {},
   "source": [
    "# GPU æ¨ç†ä¼˜åŒ–\n",
    "\n",
    "GPU æ˜¯æœºå™¨å­¦ä¹ ç¡¬ä»¶çš„æ ‡å‡†é€‰æ‹©ï¼Œä¸ CPU ç›¸æ¯”ï¼Œå®ƒä»¬åœ¨å†…å­˜å¸¦å®½å’Œå¹¶è¡Œæ€§æ–¹é¢è¿›è¡Œäº†ä¼˜åŒ–ã€‚ä¸ºäº†åœ¨ç°æœ‰æˆ–è¾ƒæ—§çš„ç¡¬ä»¶ä¸Šå¤„ç†ç°ä»£æ¨¡å‹çš„æ›´å¤§è§„æ¨¡ï¼Œæˆ–åŠ é€Ÿå¤§å‹æ¨¡å‹çš„æ¨ç†ï¼Œå¯ä»¥ä½¿ç”¨å¤šç§ä¼˜åŒ–æ–¹æ³•ã€‚åœ¨æœ¬æŒ‡å—ä¸­ï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ FlashAttention-2ï¼ˆæ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶ï¼‰ã€BetterTransformerï¼ˆPyTorch æœ¬åœ°å¿«é€Ÿæ‰§è¡Œè·¯å¾„ï¼‰å’Œ bitsandbytes æ¥é‡åŒ–æ¨¡å‹ä»¥é™ä½ç²¾åº¦ã€‚æœ€åï¼Œæ‚¨å°†å­¦ä¹ å¦‚ä½•ä½¿ç”¨ ğŸ¤— Optimum åœ¨ Nvidia å’Œ AMD GPU ä¸Šé€šè¿‡ ONNX Runtime åŠ é€Ÿæ¨ç†ã€‚\n",
    "\n",
    "å¤§å¤šæ•°ä¼˜åŒ–æ–¹æ³•ä¹Ÿé€‚ç”¨äºå¤š GPU é…ç½®ï¼\n",
    "\n",
    "## FlashAttention-2\n",
    "\n",
    "FlashAttention-2 æ˜¯ä¸€ç§å®éªŒæ€§çš„ã€æ›´å¿«ã€æ›´é«˜æ•ˆçš„æ³¨æ„åŠ›æœºåˆ¶å®ç°ï¼Œå®ƒå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼æ˜¾è‘—åŠ é€Ÿæ¨ç†ï¼š\n",
    "\n",
    "1. åœ¨åºåˆ—é•¿åº¦ä¸Šå¹¶è¡Œè®¡ç®—æ³¨æ„åŠ›ã€‚\n",
    "2. åœ¨ GPU çº¿ç¨‹ä¹‹é—´åˆ†é…å·¥ä½œï¼Œå‡å°‘çº¿ç¨‹ä¹‹é—´çš„é€šä¿¡å’Œå…±äº«å†…å­˜çš„è¯»å†™ã€‚\n",
    "\n",
    "FlashAttention-2 å½“å‰æ”¯æŒä»¥ä¸‹æ¶æ„ï¼š\n",
    "- Bark\n",
    "- Bart\n",
    "- Chameleon\n",
    "- CLIP\n",
    "- Cohere\n",
    "- GLM\n",
    "- Dbrx\n",
    "- DistilBert\n",
    "- Gemma\n",
    "- Gemma2\n",
    "- GPT2\n",
    "- GPTBigCode\n",
    "- GPTNeo\n",
    "- GPTNeoX\n",
    "- GPT-J\n",
    "- Falcon\n",
    "- Llama\n",
    "- Llava\n",
    "- å’Œæ›´å¤š...\n",
    "\n",
    "æ‚¨å¯ä»¥åœ¨ GitHub ä¸Š[è¯·æ±‚æ”¯æŒå…¶ä»–æ¨¡å‹](https://github.com/Dao-AILab/flash-attention/issues)ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… FlashAttention-2ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db865ce7",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b949d0d",
   "metadata": {},
   "source": [
    "\n",
    "è¦å¯ç”¨ FlashAttention-2ï¼Œè¯·åœ¨åŠ è½½æ¨¡å‹æ—¶ä¼ é€’å‚æ•° `attn_implementation=\"flash_attention_2\"`ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9912ac7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"tiiuae/falcon-7b\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a257f8",
   "metadata": {},
   "source": [
    "\n",
    "FlashAttention-2 åªèƒ½åœ¨æ¨¡å‹çš„ `fp16` æˆ– `bf16` ç²¾åº¦ä¸‹ä½¿ç”¨ã€‚ç¡®ä¿åœ¨ä½¿ç”¨ FlashAttention-2 ä¹‹å‰å°†æ¨¡å‹è½¬æ¢ä¸ºåˆé€‚çš„ç²¾åº¦å¹¶åŠ è½½åˆ°æ”¯æŒçš„è®¾å¤‡ä¸Šã€‚\n",
    "\n",
    "æ‚¨å¯ä»¥ç»“åˆå…¶ä»–ä¼˜åŒ–æŠ€æœ¯ï¼ˆå¦‚é‡åŒ–ï¼‰è¿›ä¸€æ­¥åŠ é€Ÿæ¨ç†ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8235b7c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8-bit é‡åŒ–\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_8bit=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "\n",
    "# 4-bit é‡åŒ–\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    load_in_4bit=True,\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b31e46fa",
   "metadata": {},
   "source": [
    "\n",
    "### æœŸæœ›çš„åŠ é€Ÿæ•ˆæœ\n",
    "\n",
    "å¯¹äºè¾ƒé•¿çš„åºåˆ—ï¼ŒFlashAttention-2 å¯ä»¥æ˜¾è‘—åŠ é€Ÿæ¨ç†ã€‚ä½†æ˜¯ï¼ŒFlashAttention-2 ä¸æ”¯æŒå¸¦æœ‰å¡«å……æ ‡è®°çš„æ³¨æ„åŠ›åˆ†æ•°è®¡ç®—ï¼Œå› æ­¤åœ¨æ‰¹é‡æ¨ç†æ—¶éœ€è¦æ‰‹åŠ¨å¡«å……/å»å¡«å……æ³¨æ„åŠ›åˆ†æ•°ï¼Œè¿™ä¼šå¯¼è‡´æ˜¾è‘—çš„å‡é€Ÿã€‚ä¸ºäº†å…‹æœè¿™ä¸€ç‚¹ï¼Œåº”è¯¥åœ¨è®­ç»ƒæœŸé—´é¿å…å¡«å……æ ‡è®°ï¼ˆä¾‹å¦‚é€šè¿‡æ‰“åŒ…æ•°æ®é›†æˆ–[è¿æ¥åºåˆ—](https://github.com/huggingface/transformers/blob/main/examples/pytorch/language-modeling/run_clm.py#L516)ç›´åˆ°è¾¾åˆ°æœ€å¤§åºåˆ—é•¿åº¦ï¼‰ã€‚\n",
    "\n",
    "å¯¹äºæ— å¡«å……æ ‡è®°çš„åºåˆ—ï¼Œå•æ¬¡å‰å‘ä¼ é€’çš„åŠ é€Ÿæ•ˆæœå¦‚ä¸‹å›¾æ‰€ç¤ºï¼š\n",
    "\n",
    "![falcon-7b åŠ é€Ÿæ•ˆæœ](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/falcon-7b-inference-large-seqlen.png)\n",
    "\n",
    "å¯¹äºå¸¦æœ‰å¡«å……æ ‡è®°çš„åºåˆ—ï¼ŒåŠ é€Ÿæ•ˆæœä¼šæœ‰æ‰€ä¸åŒï¼š\n",
    "\n",
    "![llama-2 å¸¦å¡«å……æ ‡è®°çš„åŠ é€Ÿæ•ˆæœ](https://huggingface.co/datasets/ybelkada/documentation-images/resolve/main/llama-2-large-seqlen-padding.png)\n",
    "\n",
    "FlashAttention æ›´åŠ èŠ‚çœå†…å­˜ï¼Œæ„å‘³ç€æ‚¨å¯ä»¥è®­ç»ƒæ›´é•¿çš„åºåˆ—è€Œä¸å‡ºç°å†…å­˜ä¸è¶³çš„é—®é¢˜ã€‚å¯¹äºæ›´é•¿çš„åºåˆ—ï¼Œå†…å­˜ä½¿ç”¨é‡æœ€å¤šå¯å‡å°‘ 20 å€ã€‚æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜… [flash-attention](https://github.com/Dao-AILab/flash-attention) ä»“åº“ã€‚\n",
    "\n",
    "## PyTorch ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›\n",
    "\n",
    "PyTorch çš„ `torch.nn.functional.scaled_dot_product_attention` (SDPA) å¯ä»¥è°ƒç”¨ FlashAttention å’Œé«˜æ•ˆçš„æ³¨æ„åŠ›å†…æ ¸ã€‚SDPA æ”¯æŒæ­£åœ¨åŠ å…¥ Transformersï¼Œå¹¶åœ¨ `torch>=2.1.1` ä¸­é»˜è®¤ä½¿ç”¨ã€‚æ‚¨ä¹Ÿå¯ä»¥åœ¨åŠ è½½æ¨¡å‹æ—¶æ˜¾å¼è®¾ç½® `attn_implementation=\"sdpa\"`ã€‚\n",
    "\n",
    "SDPA æ”¯æŒçš„æ¶æ„åŒ…æ‹¬ï¼š\n",
    "- Albert\n",
    "- Audio Spectrogram Transformer\n",
    "- Bart\n",
    "- Bert\n",
    "- BioGpt\n",
    "- CamemBERT\n",
    "- CLIP\n",
    "- GLM\n",
    "- Cohere\n",
    "- data2vec_audio\n",
    "- å’Œæ›´å¤š...\n",
    "\n",
    "FlashAttention åªèƒ½ç”¨äº `fp16` æˆ– `bf16` ç²¾åº¦çš„æ¨¡å‹ã€‚SDPA ä¸æ”¯æŒæŸäº›æ³¨æ„åŠ›å‚æ•°ï¼Œå¦‚ `head_mask` å’Œ `output_attentions=True`ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæ‚¨ä¼šçœ‹åˆ°è­¦å‘Šä¿¡æ¯ï¼Œå¹¶ä¼šå›é€€åˆ°è¾ƒæ…¢çš„ eager å®ç°ã€‚\n",
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼ŒSDPA ä¼šé€‰æ‹©æœ€é«˜æ•ˆçš„å†…æ ¸ï¼Œä½†æ‚¨å¯ä»¥ä½¿ç”¨ `torch.backends.cuda.sdp_kernel` æ¥æ£€æŸ¥ç‰¹å®šè®¾ç½®ï¼ˆç¡¬ä»¶ã€é—®é¢˜å¤§å°ï¼‰ä¸‹æ˜¯å¦å¯ç”¨æŸä¸ªå†…æ ¸ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d192fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=torch.float16).to(\"cuda\")\n",
    "\n",
    "input_text = \"Hello my dog is cute and\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5debb0",
   "metadata": {},
   "source": [
    "\n",
    "å¦‚æœæ‚¨é‡åˆ°é”™è¯¯ï¼Œå¯ä»¥å°è¯•ä½¿ç”¨ PyTorch çš„ nightly ç‰ˆæœ¬ï¼Œä»¥è·å–æ›´å¹¿æ³›çš„ FlashAttention æ”¯æŒã€‚\n",
    "\n",
    "## BetterTransformer\n",
    "\n",
    "BetterTransformer æ˜¯ä¸€ç§ä¼˜åŒ–æŠ€æœ¯ï¼Œé€šè¿‡ PyTorch æœ¬åœ°å®ç°çš„å¿«é€Ÿæ‰§è¡Œè·¯å¾„ï¼ˆfastpathï¼‰åŠ é€Ÿæ¨ç†ã€‚å¿«é€Ÿæ‰§è¡Œè·¯å¾„çš„ä¸¤ä¸ªä¼˜åŒ–ç‚¹æ˜¯ï¼š\n",
    "\n",
    "1. èåˆï¼ˆfusionï¼‰ï¼Œå°†å¤šä¸ªè¿ç»­çš„æ“ä½œåˆå¹¶ä¸ºä¸€ä¸ªâ€œå†…æ ¸â€ä»¥å‡å°‘è®¡ç®—æ­¥éª¤ã€‚\n",
    "2. è·³è¿‡å¡«å……æ ‡è®°çš„å›ºæœ‰ç¨€ç–æ€§ï¼Œä»¥é¿å…ä¸å¿…è¦çš„è®¡ç®—ã€‚\n",
    "\n",
    "BetterTransformer è¿˜å°†æ‰€æœ‰æ³¨æ„åŠ›æ“ä½œè½¬æ¢ä¸ºæ›´èŠ‚çœå†…å­˜çš„ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ï¼ˆSDPAï¼‰ï¼Œå¹¶åœ¨åå°è°ƒç”¨ä¼˜åŒ–å†…æ ¸ï¼ˆå¦‚ FlashAttentionï¼‰ã€‚\n",
    "\n",
    "åœ¨ä½¿ç”¨ BetterTransformer ä¹‹å‰ï¼Œè¯·ç¡®ä¿å·²å®‰è£… ğŸ¤— Optimumï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbb39317",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install optimum"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938ba8f3",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åå¯ä»¥ä½¿ç”¨ `PreTrainedModel.to_bettertransformer()` æ–¹æ³•å¯ç”¨ BetterTransformerï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb3daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to_bettertransformer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a06d32",
   "metadata": {},
   "source": [
    "\n",
    "å¦‚æœæ‚¨æƒ³æ¢å¤åŸå§‹çš„ ğŸ¤— Transformers æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ `reverse_bettertransformer()` æ–¹æ³•ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f558ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.reverse_bettertransformer()\n",
    "model.save_pretrained(\"saved_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed28209",
   "metadata": {},
   "source": [
    "\n",
    "## bitsandbytes\n",
    "\n",
    "bitsandbytes æ˜¯ä¸€ä¸ªé‡åŒ–åº“ï¼Œæ”¯æŒ 4 æ¯”ç‰¹å’Œ 8 æ¯”ç‰¹é‡åŒ–ã€‚é‡åŒ–å¯ä»¥å‡å°‘æ¨¡å‹çš„å¤§å°ï¼Œä½¿å…¶æ›´å®¹æ˜“åœ¨å†…å­˜æœ‰é™çš„ GPU ä¸Šè¿è¡Œã€‚\n",
    "\n",
    "å®‰è£… bitsandbytes å’Œ ğŸ¤— Accelerateï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe33e96",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install bitsandbytes>=0.39.0 accelerate>=0.20.0\n",
    "pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf46d35",
   "metadata": {},
   "source": [
    "\n",
    "### 4-bit\n",
    "\n",
    "è¦ä»¥ 4 æ¯”ç‰¹åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ `load_in_4bit` å‚æ•°ã€‚`device_map` å‚æ•°æ˜¯å¯é€‰çš„ï¼Œä½†å»ºè®®è®¾ç½®ä¸º `\"auto\"` ä»¥å…è®¸ ğŸ¤— Accelerate è‡ªåŠ¨é«˜æ•ˆåˆ†é…æ¨¡å‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cd88fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", device_map=\"auto\", load_in_4bit=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9582e9c",
   "metadata": {},
   "source": [
    "\n",
    "å¯¹äºå¤š GPU ç¯å¢ƒï¼Œæ‚¨å¯ä»¥æ§åˆ¶æ¯ä¸ª GPU åˆ†é…çš„å†…å­˜ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1a3131",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory_mapping = {0: \"600MB\", 1: \"1GB\"}\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "model_4bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\", load_in_4bit=True, max_memory=max_memory_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceee9d7",
   "metadata": {},
   "source": [
    "\n",
    "### 8-bit\n",
    "\n",
    "è¦ä»¥ 8 æ¯”ç‰¹åŠ è½½æ¨¡å‹è¿›è¡Œæ¨ç†ï¼Œä½¿ç”¨ `load_in_8bit` å‚æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0b9db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_name = \"bigscience/bloom-2b5\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=\"auto\", quantization_config=BitsAndBytesConfig(load_in_8bit=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b60c16f",
   "metadata": {},
   "source": [
    "\n",
    "å¯¹äºå¤š GPU ç¯å¢ƒï¼Œæ‚¨å¯ä»¥æ§åˆ¶æ¯ä¸ª GPU åˆ†é…çš„å†…å­˜ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b77390",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_memory_mapping = {0: \"1GB\", 1: \"2GB\"}\n",
    "model_name = \"bigscience/bloom-3b\"\n",
    "model_8bit = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name, torch_dtype=\"auto\", device_map=\"auto\", load_in_8bit=True, max_memory=max_memory_mapping\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01875ab7",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ¤— Optimum\n",
    "\n",
    "ğŸ¤— Optimum æ”¯æŒä½¿ç”¨ ONNX Runtimeï¼ˆORTï¼‰åŠ é€Ÿ Nvidia å’Œ AMD GPU ä¸Šçš„æ¨ç†ã€‚ORT ä½¿ç”¨è¯¸å¦‚å°†å¸¸è§æ“ä½œèåˆä¸ºå•ä¸ªèŠ‚ç‚¹å’Œå¸¸é‡æŠ˜å ç­‰ä¼˜åŒ–æŠ€æœ¯ï¼Œä»¥å‡å°‘è®¡ç®—æ­¥éª¤å¹¶åŠ é€Ÿæ¨ç†ã€‚ORT è¿˜å°†æœ€å¯†é›†çš„è®¡ç®—ä»»åŠ¡åˆ†é…ç»™ GPUï¼Œå…¶ä½™ä»»åŠ¡åˆ†é…ç»™ CPUï¼Œä»¥æ™ºèƒ½åˆ†é…å·¥ä½œè´Ÿè½½ã€‚\n",
    "\n",
    "è¦ä½¿ç”¨ ORTï¼Œæ‚¨éœ€è¦ä½¿ç”¨ä¸€ä¸ªç‰¹å®šä»»åŠ¡çš„ [ORTModel](https://huggingface.co/docs/optimum/v1.23.3/en/onnxruntime/package_reference/modeling_ort#optimum.onnxruntime.ORTModel)ï¼Œå¹¶æŒ‡å®š `provider` å‚æ•°ï¼Œä¾‹å¦‚ `CUDAExecutionProvider`ã€`ROCMExecutionProvider` æˆ– `TensorrtExecutionProvider`ã€‚å¦‚æœè¦å¯¼å‡ºæ¨¡å‹ä¸º ONNX æ ¼å¼ï¼Œå¯ä»¥è®¾ç½® `export=True`ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(\n",
    "  \"distilbert/distilbert-base-uncased-finetuned-sst-2-english\",\n",
    "  export=True,\n",
    "  provider=\"CUDAExecutionProvider\",\n",
    ")\n",
    "\n",
    "from optimum.pipelines import pipeline\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "\n",
    "pipeline = pipeline(task=\"text-classification\", model=ort_model, tokenizer=tokenizer, device=\"cuda:0\")\n",
    "result = pipeline(\"Both the music and visual were astounding, not to mention the actors performance.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a1afab",
   "metadata": {},
   "source": [
    "\n",
    "## ç»„åˆä¼˜åŒ–\n",
    "\n",
    "å¯ä»¥ç»„åˆå¤šç§ä¼˜åŒ–æŠ€æœ¯ä»¥è·å¾—æœ€ä½³çš„æ¨ç†æ€§èƒ½ã€‚ä¾‹å¦‚ï¼Œæ‚¨å¯ä»¥åŠ è½½ 4 æ¯”ç‰¹é‡åŒ–æ¨¡å‹å¹¶å¯ç”¨ BetterTransformer å’Œ FlashAttentionï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6da721b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "\n",
    "# åŠ è½½ 4 æ¯”ç‰¹æ¨¡å‹\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", torch_dtype=\"auto\", quantization_config=quantization_config)\n",
    "\n",
    "# å¯ç”¨ BetterTransformer\n",
    "model = model.to_bettertransformer()\n",
    "\n",
    "input_text = \"Hello my dog is cute and\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "# å¯ç”¨ FlashAttention\n",
    "with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n",
    "    outputs = model.generate(**inputs)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
