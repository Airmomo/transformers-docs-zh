{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d56077f8",
   "metadata": {},
   "source": [
    "# TensorFlow æ¨¡å‹ä¸­çš„ XLA é›†æˆ\n",
    "\n",
    "![åœ¨ Colab ä¸­æ‰“å¼€](https://colab.research.google.com/assets/colab-badge.svg)\n",
    "![åœ¨ Studio Lab ä¸­æ‰“å¼€](https://studiolab.sagemaker.aws/studiolab.svg)\n",
    "\n",
    "åŠ é€Ÿçº¿æ€§ä»£æ•°ï¼ˆAccelerated Linear Algebraï¼Œç®€ç§° XLAï¼‰æ˜¯ç”¨äºåŠ é€Ÿ TensorFlow æ¨¡å‹è¿è¡Œæ—¶çš„ç¼–è¯‘å™¨ã€‚æ ¹æ®[å®˜æ–¹æ–‡æ¡£](https://www.tensorflow.org/xla)ï¼š\n",
    "\n",
    "XLA æ˜¯ä¸€ç§ç‰¹å®šé¢†åŸŸçš„çº¿æ€§ä»£æ•°ç¼–è¯‘å™¨ï¼Œå¯ä»¥åœ¨ä¸æ”¹å˜æºä»£ç çš„æƒ…å†µä¸‹åŠ é€Ÿ TensorFlow æ¨¡å‹ã€‚\n",
    "\n",
    "åœ¨ TensorFlow ä¸­ä½¿ç”¨ XLA éå¸¸ç®€å•â€”â€”å®ƒå·²ç»åŒ…å«åœ¨ `tensorflow` åº“ä¸­ï¼Œåªéœ€é€šè¿‡ `jit_compile` å‚æ•°è§¦å‘å³å¯ã€‚ä¾‹å¦‚ï¼Œåœ¨ä½¿ç”¨åƒ `tf.function` è¿™æ ·çš„å›¾åˆ›å»ºå‡½æ•°æ—¶ï¼Œæˆ–åœ¨ä½¿ç”¨ Keras çš„ `fit()` å’Œ `predict()` æ–¹æ³•æ—¶ï¼Œå¯ä»¥é€šè¿‡ä¼ é€’ `jit_compile` å‚æ•°ç»™ `model.compile()` æ¥å¯ç”¨ XLAã€‚XLA å¹¶ä¸é™äºè¿™äº›æ–¹æ³•ï¼Œè¿˜å¯ä»¥åŠ é€Ÿä»»æ„çš„ `tf.function`ã€‚\n",
    "\n",
    "ä¸€äº› ğŸ¤— Transformers åº“ä¸­çš„ TensorFlow æ–¹æ³•å·²ç»è¢«é‡å†™ä¸º XLA å…¼å®¹ï¼ŒåŒ…æ‹¬ç”¨äº [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ã€[T5](https://huggingface.co/docs/transformers/model_doc/t5) å’Œ [OPT](https://huggingface.co/docs/transformers/model_doc/opt) æ¨¡å‹çš„æ–‡æœ¬ç”Ÿæˆï¼Œä»¥åŠç”¨äº [Whisper](https://huggingface.co/docs/transformers/model_doc/whisper) æ¨¡å‹çš„è¯­éŸ³å¤„ç†ã€‚\n",
    "\n",
    "è™½ç„¶å…·ä½“çš„åŠ é€Ÿæ•ˆæœå–å†³äºæ¨¡å‹æœ¬èº«ï¼Œä½†å¯¹äº ğŸ¤— Transformers åº“ä¸­çš„ TensorFlow æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ï¼Œæˆ‘ä»¬è§‚å¯Ÿåˆ°çš„é€Ÿåº¦æå‡å¤§çº¦ä¸º 100 å€ã€‚æœ¬æ–‡å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ XLA æå‡è¿™äº›æ¨¡å‹çš„æ€§èƒ½ï¼Œå¹¶æä¾›ä¸€äº›é¢å¤–èµ„æºï¼Œå¸®åŠ©ä½ äº†è§£æ›´å¤šå…³äºåŸºå‡†æµ‹è¯•å’Œ XLA é›†æˆçš„è®¾è®¡ç†å¿µã€‚\n",
    "\n",
    "## ä½¿ç”¨ XLA è¿è¡Œ TensorFlow å‡½æ•°\n",
    "\n",
    "è®©æˆ‘ä»¬ä»¥ä¸€ä¸ªç®€å•çš„ TensorFlow æ¨¡å‹ä¸ºä¾‹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2538fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(10, input_shape=(10,), activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(5, activation=\"softmax\")\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e1b1a2",
   "metadata": {},
   "source": [
    "\n",
    "ä¸Šé¢çš„æ¨¡å‹æ¥æ”¶ç»´åº¦ä¸º `(10, )` çš„è¾“å…¥ã€‚æˆ‘ä»¬å¯ä»¥åƒè¿™æ ·è¿è¡Œå‰å‘ä¼ æ’­ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368b96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”Ÿæˆéšæœºè¾“å…¥\n",
    "batch_size = 16\n",
    "input_vector_dim = 10\n",
    "random_inputs = tf.random.normal((batch_size, input_vector_dim))\n",
    "\n",
    "# è¿è¡Œå‰å‘ä¼ æ’­\n",
    "_ = model(random_inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f3d01",
   "metadata": {},
   "source": [
    "\n",
    "è¦ä½¿ç”¨ XLA ç¼–è¯‘çš„å‡½æ•°è¿è¡Œå‰å‘ä¼ æ’­ï¼Œå¯ä»¥è¿™æ ·åšï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bebb71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xla_fn = tf.function(model, jit_compile=True)\n",
    "_ = xla_fn(random_inputs)  # ä½¿ç”¨ XLA ç¼–è¯‘çš„å‡½æ•°è¿è¡Œå‰å‘ä¼ æ’­"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dfbc8a6",
   "metadata": {},
   "source": [
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œ`model` çš„ `call()` å‡½æ•°ç”¨äºç¼–è¯‘ XLA å›¾ã€‚å¦‚æœä½ è¦ç¼–è¯‘å…¶ä»–æ¨¡å‹å‡½æ•°ï¼Œä¹Ÿå¯ä»¥è¿™æ ·åšï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0db387cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_xla_fn = tf.function(model.my_xla_fn, jit_compile=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d530357",
   "metadata": {},
   "source": [
    "\n",
    "## ä½¿ç”¨ ğŸ¤— Transformers åº“ä¸­çš„ XLA è¿è¡Œæ–‡æœ¬ç”Ÿæˆ\n",
    "\n",
    "è¦å¯ç”¨ ğŸ¤— Transformers åº“ä¸­çš„ XLA åŠ é€Ÿç”Ÿæˆï¼Œä½ éœ€è¦å®‰è£…æœ€æ–°ç‰ˆæœ¬çš„ `transformers`ã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹å‘½ä»¤å®‰è£…ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c22d717",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5080bd77",
   "metadata": {},
   "source": [
    "\n",
    "ç„¶åå¯ä»¥è¿è¡Œä»¥ä¸‹ä»£ç ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603c79c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "# ç¡®ä¿å®‰è£…äº†æœ€æ–°çš„ Transformers ç‰ˆæœ¬\n",
    "from transformers.utils import check_min_version\n",
    "check_min_version(\"4.21.0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"<</s>>\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "input_string = [\"TensorFlow is\"]\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª XLA ç”Ÿæˆå‡½æ•°\n",
    "xla_generate = tf.function(model.generate, jit_compile=True)\n",
    "\n",
    "tokenized_input = tokenizer(input_string, return_tensors=\"tf\")\n",
    "generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n",
    "\n",
    "decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "print(f\"Generated -- {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb2d84b",
   "metadata": {},
   "source": [
    "\n",
    "ä½ ä¼šå‘ç°ï¼Œå¯ç”¨ `generate()` çš„ XLA æ”¯æŒåªéœ€ä¸€è¡Œä»£ç ã€‚ä¸è¿‡ï¼Œæœ‰ä¸€äº›éœ€è¦æ³¨æ„çš„åœ°æ–¹æ‰èƒ½çœŸæ­£å®ç° XLA å¸¦æ¥çš„åŠ é€Ÿæ•ˆæœï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®ºã€‚\n",
    "\n",
    "## æ³¨æ„äº‹é¡¹\n",
    "\n",
    "å½“ä½ ç¬¬ä¸€æ¬¡æ‰§è¡Œä¸€ä¸ª XLA å¯ç”¨çš„å‡½æ•°ï¼ˆå¦‚ä¸Šé¢çš„ `xla_generate()`ï¼‰ï¼Œå®ƒä¼šå°è¯•æ¨æ–­è®¡ç®—å›¾ï¼Œè¿™ä¸ªè¿‡ç¨‹éœ€è¦æ—¶é—´ï¼Œè¢«ç§°ä¸ºâ€œè¿½è¸ªâ€ã€‚\n",
    "\n",
    "ä½ å¯èƒ½ä¼šæ³¨æ„åˆ°é¦–æ¬¡ç”Ÿæˆæ—¶é€Ÿåº¦å¹¶ä¸å¿«ã€‚ç„¶è€Œï¼Œåç»­è°ƒç”¨ `xla_generate()`ï¼ˆæˆ–ä»»ä½•å…¶ä»– XLA å¯ç”¨çš„å‡½æ•°ï¼‰æ—¶ï¼Œå¦‚æœè¾“å…¥å½¢çŠ¶ä¸åˆå§‹æ„å»ºè®¡ç®—å›¾æ—¶ä¸€è‡´ï¼Œåˆ™æ— éœ€é‡æ–°æ¨æ–­è®¡ç®—å›¾ï¼Œä»è€Œæé«˜ç”Ÿæˆé€Ÿåº¦ã€‚\n",
    "\n",
    "å¯¹äºå½¢çŠ¶å›ºå®šçš„æ¨¡æ€ï¼ˆå¦‚å›¾åƒï¼‰ï¼Œè¿™ä¸æ˜¯é—®é¢˜ã€‚ä½†å¯¹äºè¾“å…¥å½¢çŠ¶å˜åŒ–çš„æ¨¡æ€ï¼ˆå¦‚æ–‡æœ¬ï¼‰ï¼Œä½ éœ€è¦æ³¨æ„è¿™ä¸€ç‚¹ã€‚ä¸ºäº†ç¡®ä¿ `xla_generate()` å§‹ç»ˆä½¿ç”¨ç›¸åŒçš„è¾“å…¥å½¢çŠ¶ï¼Œä½ å¯ä»¥åœ¨è°ƒç”¨åˆ†è¯å™¨æ—¶æŒ‡å®š `padding` å‚æ•°ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670b9ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"<</s>>\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "input_string = [\"TensorFlow is\"]\n",
    "\n",
    "xla_generate = tf.function(model.generate, jit_compile=True)\n",
    "\n",
    "# è°ƒç”¨åˆ†è¯å™¨æ—¶æŒ‡å®šå¡«å……é€‰é¡¹\n",
    "tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n",
    "\n",
    "generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n",
    "decoded_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n",
    "print(f\"Generated -- {decoded_text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef42a6f",
   "metadata": {},
   "source": [
    "\n",
    "è¿™æ ·ï¼Œä½ å¯ä»¥ç¡®ä¿ `xla_generate()` å§‹ç»ˆä½¿ç”¨ç›¸åŒçš„è¾“å…¥å½¢çŠ¶ï¼Œä»è€Œæé«˜ç”Ÿæˆé€Ÿåº¦ã€‚ä½ å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç éªŒè¯è¿™ä¸€ç‚¹ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f960eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow as tf\n",
    "from transformers import AutoTokenizer, TFAutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\", padding_side=\"left\", pad_token=\"<</s>>\")\n",
    "model = TFAutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n",
    "\n",
    "xla_generate = tf.function(model.generate, jit_compile=True)\n",
    "\n",
    "for input_string in [\"TensorFlow is\", \"TensorFlow is a\", \"TFLite is a\"]:\n",
    "    tokenized_input = tokenizer(input_string, pad_to_multiple_of=8, padding=True, return_tensors=\"tf\")\n",
    "    start = time.time_ns()\n",
    "    generated_tokens = xla_generate(**tokenized_input, num_beams=2)\n",
    "    end = time.time_ns()\n",
    "    print(f\"Execution time -- {(end - start) / 1e6:.1f} ms\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a89b2cf",
   "metadata": {},
   "source": [
    "\n",
    "åœ¨ Tesla T4 GPU ä¸Šï¼Œä½ å¯èƒ½ä¼šçœ‹åˆ°ç±»ä¼¼ä»¥ä¸‹çš„è¾“å‡ºï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753fc941",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Execution time -- 30819.6 ms\n",
    "Execution time -- 79.0 ms\n",
    "Execution time -- 78.9 ms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac2f51b",
   "metadata": {},
   "source": [
    "\n",
    "é¦–æ¬¡è°ƒç”¨ `xla_generate()` æ—¶ç”±äºéœ€è¦è¿½è¸ªè®¡ç®—å›¾ï¼Œæ‰€ä»¥è€—æ—¶è¾ƒé•¿ï¼Œä½†åç»­è°ƒç”¨åˆ™å¿«å¾—å¤šã€‚è¯·æ³¨æ„ï¼Œä»»ä½•ç”Ÿæˆé€‰é¡¹çš„å˜åŒ–éƒ½ä¼šè§¦å‘é‡æ–°è¿½è¸ªï¼Œä»è€Œå¯¼è‡´ç”Ÿæˆæ—¶é—´å˜æ…¢ã€‚\n",
    "\n",
    "æˆ‘ä»¬æ²¡æœ‰æ¶µç›– ğŸ¤— Transformers æä¾›çš„æ‰€æœ‰æ–‡æœ¬ç”Ÿæˆé€‰é¡¹ã€‚å¦‚æœä½ æƒ³äº†è§£æ›´å¤šé«˜çº§ç”¨æ³•ï¼Œè¯·å‚è€ƒå®˜æ–¹æ–‡æ¡£ã€‚\n",
    "\n",
    "## è¿›ä¸€æ­¥èµ„æº\n",
    "\n",
    "å¦‚æœä½ æƒ³æ·±å…¥äº†è§£ ğŸ¤— Transformers å’Œ TensorFlow ä¸­çš„ XLAï¼Œä»¥ä¸‹æ˜¯ä¸€äº›é¢å¤–èµ„æºï¼š\n",
    "\n",
    "* [è¿™ä¸ª Colab Notebook](https://colab.research.google.com/github/huggingface/blog/blob/main/notebooks/91_tf_xla_generate.ipynb) æä¾›äº†ä¸€ä¸ªäº¤äº’å¼æ¼”ç¤ºï¼Œå±•ç¤ºäº† XLA å…¼å®¹çš„ç¼–ç å™¨-è§£ç å™¨ï¼ˆå¦‚ [T5](https://huggingface.co/docs/transformers/model_doc/t5)ï¼‰å’Œåªè§£ç å™¨ï¼ˆå¦‚ [GPT2](https://huggingface.co/docs/transformers/model_doc/gpt2)ï¼‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚\n",
    "* [è¿™ç¯‡åšå®¢æ–‡ç« ](https://huggingface.co/blog/tf-xla-generate) æä¾›äº† XLA å…¼å®¹æ¨¡å‹çš„åŸºå‡†æµ‹è¯•æ¯”è¾ƒï¼Œä»¥åŠå…³äº TensorFlow ä¸­ XLA çš„å‹å¥½ä»‹ç»ã€‚\n",
    "* [è¿™ç¯‡åšå®¢æ–‡ç« ](https://blog.tensorflow.org/2022/11/how-hugging-face-improved-text-generation-performance-with-xla.html) è®¨è®ºäº†æˆ‘ä»¬åœ¨ ğŸ¤— Transformers ä¸­æ·»åŠ  XLA æ”¯æŒçš„è®¾è®¡ç†å¿µã€‚\n",
    "* æ¨èçš„å­¦ä¹ èµ„æºï¼š\n",
    "    * [XLAï¼šæœºå™¨å­¦ä¹ çš„ä¼˜åŒ–ç¼–è¯‘å™¨](https://www.tensorflow.org/xla)\n",
    "    * [å›¾å½¢å’Œ tf.function çš„ä»‹ç»](https://www.tensorflow.org/guide/intro_to_graphs)\n",
    "    * [ä½¿ç”¨ tf.function æå‡æ€§èƒ½](https://www.tensorflow.org/guide/function)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
