{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1de7df8",
   "metadata": {},
   "source": [
    "# 使用 `torch.compile()` 优化推理速度\n",
    "\n",
    "本指南旨在展示使用 `torch.compile()` 在不同硬件和批处理大小下对 [🤗 Transformers](https://huggingface.co/models?pipeline_tag=image-classification&library=transformers&sort=trending) 中的计算机视觉模型进行推理加速的效果。\n",
    "\n",
    "## 使用 `torch.compile()` 的优势\n",
    "\n",
    "根据模型和 GPU 的不同，`torch.compile()` 可以使推理速度提高 30%。要使用 `torch.compile()`，只需安装 PyTorch 2.0 以上的版本即可。\n",
    "\n",
    "编译模型需要一定的时间，因此建议只编译一次，而不是每次推理都重新编译。要编译任何计算机视觉模型，只需在模型上调用 `torch.compile()`，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "927bc403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForImageClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(DEVICE)\n",
    "model = torch.compile(model)  # 编译模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b6baae6",
   "metadata": {},
   "source": [
    "\n",
    "`compile()` 提供了多种编译模式，不同模式的编译时间和推理开销有所不同。`max-autotune` 模式需要更长的编译时间，但推理速度更快。默认模式编译速度最快，但在推理时间上不如 `reduce-overhead` 模式高效。本指南使用的是默认模式。您可以了解更多关于这些模式的内容 [这里](https://pytorch.org/get-started/pytorch-2.0/#user-experience)。\n",
    "\n",
    "我们在 `torch` 版本 2.0.1 上对不同计算机视觉模型、任务、硬件类型和批处理大小进行了基准测试。\n",
    "\n",
    "## 基准测试代码\n",
    "\n",
    "以下是每个任务的基准测试代码。我们在推理前先预热 GPU，并取 300 次推理的平均时间，每次使用相同的图像。\n",
    "\n",
    "### 使用 ViT 进行图像分类\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2266d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # 自动检测底层设备类型（CUDA、CPU、XPU、MPS 等）\n",
    "url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
    "model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(device)\n",
    "model = torch.compile(model)  # 编译模型\n",
    "\n",
    "processed_input = processor(image, return_tensors='pt').to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**processed_input)  # 进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59891c3d",
   "metadata": {},
   "source": [
    "\n",
    "### 使用 DETR 进行目标检测\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b73f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoImageProcessor, AutoModelForObjectDetection\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # 自动检测底层设备类型（CUDA、CPU、XPU、MPS 等）\n",
    "processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
    "model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(device)\n",
    "model = torch.compile(model)  # 编译模型\n",
    "\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\"]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**inputs)  # 进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3636495a",
   "metadata": {},
   "source": [
    "\n",
    "### 使用 Segformer 进行图像分割\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9003d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n",
    "from accelerate.test_utils.testing import get_backend\n",
    "\n",
    "device, _, _ = get_backend()  # 自动检测底层设备类型（CUDA、CPU、XPU、MPS 等）\n",
    "processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n",
    "model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to(device)\n",
    "model = torch.compile(model)  # 编译模型\n",
    "seg_inputs = processor(images=image, return_tensors=\"pt\").to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    _ = model(**seg_inputs)  # 进行推理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c353829e",
   "metadata": {},
   "source": [
    "\n",
    "以下是我们在基准测试中使用的模型列表。\n",
    "\n",
    "### 图像分类\n",
    "\n",
    "- [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224)\n",
    "- [microsoft/beit-base-patch16-224-pt22k-ft22k](https://huggingface.co/microsoft/beit-base-patch16-224-pt22k-ft22k)\n",
    "- [facebook/convnext-large-224](https://huggingface.co/facebook/convnext-large-224)\n",
    "- [microsoft/resnet-50](https://huggingface.co/microsoft/resnet-50)\n",
    "\n",
    "### 图像分割\n",
    "\n",
    "- [nvidia/segformer-b0-finetuned-ade-512-512](https://huggingface.co/nvidia/segformer-b0-finetuned-ade-512-512)\n",
    "- [facebook/mask2former-swin-tiny-coco-panoptic](https://huggingface.co/facebook/mask2former-swin-tiny-coco-panoptic)\n",
    "- [facebook/maskformer-swin-base-ade](https://huggingface.co/facebook/maskformer-swin-base-ade)\n",
    "- [google/deeplabv3_mobilenet_v2_1.0_513](https://huggingface.co/google/deeplabv3_mobilenet_v2_1.0_513)\n",
    "\n",
    "### 目标检测\n",
    "\n",
    "- [google/owlvit-base-patch32](https://huggingface.co/google/owlvit-base-patch32)\n",
    "- [facebook/detr-resnet-101](https://huggingface.co/facebook/detr-resnet-101)\n",
    "- [microsoft/conditional-detr-resnet-50](https://huggingface.co/microsoft/conditional-detr-resnet-50)\n",
    "\n",
    "以下图表展示了使用和不使用 `torch.compile()` 的推理时间对比以及每个模型在不同硬件和批处理大小下的性能提升百分比。\n",
    "\n",
    "![A100 批量 1 比较](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_duration.png)\n",
    "![A100 批量 1 提升百分比](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/torch_compile/A100_1_percentage.png)\n",
    "\n",
    "### A100 （批量大小：1）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 9.325 | 7.584 |\n",
    "| 图像分割/Segformer | 11.759 | 10.500 |\n",
    "| 目标检测/OwlViT | 24.978 | 18.420 |\n",
    "| 图像分类/BeiT | 11.282 | 8.448 |\n",
    "| 目标检测/DETR | 34.619 | 19.040 |\n",
    "| 图像分类/ConvNeXT | 10.410 | 10.208 |\n",
    "| 图像分类/ResNet | 6.531 | 4.124 |\n",
    "| 图像分割/Mask2former | 60.188 | 49.117 |\n",
    "| 图像分割/Maskformer | 75.764 | 59.487 |\n",
    "| 图像分割/MobileNet | 8.583 | 3.974 |\n",
    "| 目标检测/Resnet-101 | 36.276 | 18.197 |\n",
    "| 目标检测/Conditional-DETR | 31.219 | 17.993 |\n",
    "\n",
    "### A100 （批量大小：4）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 14.832 | 14.499 |\n",
    "| 图像分割/Segformer | 18.838 | 16.476 |\n",
    "| 图像分类/BeiT | 13.205 | 13.048 |\n",
    "| 目标检测/DETR | 48.657 | 32.418 |\n",
    "| 图像分类/ConvNeXT | 22.940 | 21.631 |\n",
    "| 图像分类/ResNet | 6.657 | 4.268 |\n",
    "| 图像分割/Mask2former | 74.277 | 61.781 |\n",
    "| 图像分割/Maskformer | 180.700 | 159.116 |\n",
    "| 图像分割/MobileNet | 14.174 | 8.515 |\n",
    "| 目标检测/Resnet-101 | 68.101 | 44.998 |\n",
    "| 目标检测/Conditional-DETR | 56.470 | 35.552 |\n",
    "\n",
    "### A100 （批量大小：16）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 40.944 | 40.010 |\n",
    "| 图像分割/Segformer | 37.005 | 31.144 |\n",
    "| 图像分类/BeiT | 41.854 | 41.048 |\n",
    "| 目标检测/DETR | 164.382 | 161.902 |\n",
    "| 图像分类/ConvNeXT | 82.258 | 75.561 |\n",
    "| 图像分类/ResNet | 7.018 | 5.024 |\n",
    "| 图像分割/Mask2former | 178.945 | 154.814 |\n",
    "| 图像分割/Maskformer | 638.570 | 579.826 |\n",
    "| 图像分割/MobileNet | 51.693 | 30.310 |\n",
    "| 目标检测/Resnet-101 | 232.887 | 155.021 |\n",
    "| 目标检测/Conditional-DETR | 180.491 | 124.032 |\n",
    "\n",
    "### V100 （批量大小：1）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 10.495 | 6.00 |\n",
    "| 图像分割/Segformer | 13.321 | 5.862 |\n",
    "| 目标检测/OwlViT | 25.769 | 22.395 |\n",
    "| 图像分类/BeiT | 11.347 | 7.234 |\n",
    "| 目标检测/DETR | 33.951 | 19.388 |\n",
    "| 图像分类/ConvNeXT | 11.623 | 10.412 |\n",
    "| 图像分类/ResNet | 6.484 | 3.820 |\n",
    "| 图像分割/Mask2former | 64.640 | 49.873 |\n",
    "| 图像分割/Maskformer | 95.532 | 72.207 |\n",
    "| 图像分割/MobileNet | 9.217 | 4.753 |\n",
    "| 目标检测/Resnet-101 | 52.818 | 28.367 |\n",
    "| 目标检测/Conditional-DETR | 39.512 | 20.816 |\n",
    "\n",
    "### V100 （批量大小：4）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 15.181 | 14.501 |\n",
    "| 图像分割/Segformer | 16.787 | 16.188 |\n",
    "| 图像分类/BeiT | 15.171 | 14.753 |\n",
    "| 目标检测/DETR | 88.529 | 64.195 |\n",
    "| 图像分类/ConvNeXT | 29.574 | 27.085 |\n",
    "| 图像分类/ResNet | 6.109 | 4.731 |\n",
    "| 图像分割/Mask2former | 90.402 | 76.926 |\n",
    "| 图像分割/Maskformer | 234.261 | 205.456 |\n",
    "| 图像分割/MobileNet | 24.623 | 14.816 |\n",
    "| 目标检测/Resnet-101 | 134.672 | 101.304 |\n",
    "| 目标检测/Conditional-DETR | 97.464 | 69.739 |\n",
    "\n",
    "### V100 （批量大小：16）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 52.209 | 51.633 |\n",
    "| 图像分割/Segformer | 61.013 | 55.499 |\n",
    "| 图像分类/BeiT | 53.938 | 53.581 |\n",
    "| 目标检测/DETR | OOM | OOM |\n",
    "| 图像分类/ConvNeXT | 109.682 | 100.771 |\n",
    "| 图像分类/ResNet | 14.857 | 12.089 |\n",
    "| 图像分割/Mask2former | 249.605 | 222.801 |\n",
    "| 图像分割/Maskformer | 831.142 | 743.645 |\n",
    "| 图像分割/MobileNet | 93.129 | 55.365 |\n",
    "| 目标检测/Resnet-101 | 482.425 | 361.843 |\n",
    "| 目标检测/Conditional-DETR | 344.661 | 255.298 |\n",
    "\n",
    "### T4 （批量大小：1）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 16.520 | 15.786 |\n",
    "| 图像分割/Segformer | 16.116 | 14.205 |\n",
    "| 目标检测/OwlViT | 53.634 | 51.105 |\n",
    "| 图像分类/BeiT | 16.464 | 15.710 |\n",
    "| 目标检测/DETR | 73.100 | 53.99 |\n",
    "| 图像分类/ConvNeXT | 32.932 | 30.845 |\n",
    "| 图像分类/ResNet | 6.031 | 4.321 |\n",
    "| 图像分割/Mask2former | 79.192 | 66.815 |\n",
    "| 图像分割/Maskformer | 200.026 | 188.268 |\n",
    "| 图像分割/MobileNet | 18.908 | 11.997 |\n",
    "| 目标检测/Resnet-101 | 106.622 | 82.566 |\n",
    "| 目标检测/Conditional-DETR | 77.594 | 56.984 |\n",
    "\n",
    "### T4 （批量大小：4）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 43.653 | 43.626 |\n",
    "| 图像分割/Segformer | 45.327 | 42.445 |\n",
    "| 图像分类/BeiT | 52.007 | 51.354 |\n",
    "| 目标检测/DETR | 277.850 | 268.003 |\n",
    "| 图像分类/ConvNeXT | 119.259 | 105.580 |\n",
    "| 图像分类/ResNet | 13.039 | 11.388 |\n",
    "| 图像分割/Mask2former | 201.540 | 184.670 |\n",
    "| 图像分割/Maskformer | 764.052 | 711.280 |\n",
    "| 图像分割/MobileNet | 74.289 | 48.677 |\n",
    "| 目标检测/Resnet-101 | 421.859 | 357.614 |\n",
    "| 目标检测/Conditional-DETR | 289.002 | 226.945 |\n",
    "\n",
    "### T4 （批量大小：16）\n",
    "\n",
    "| **任务/模型** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: |\n",
    "| 图像分类/ViT | 163.914 | 160.907 |\n",
    "| 图像分割/Segformer | 192.412 | 163.620 |\n",
    "| 图像分类/BeiT | 188.978 | 187.976 |\n",
    "| 目标检测/DETR | OOM | OOM |\n",
    "| 图像分类/ConvNeXT | 422.886 | 388.078 |\n",
    "| 图像分类/ResNet | 44.114 | 37.604 |\n",
    "| 图像分割/Mask2former | 756.337 | 695.291 |\n",
    "| 图像分割/Maskformer | 2842.940 | 2656.88 |\n",
    "| 图像分割/MobileNet | 299.003 | 201.942 |\n",
    "| 目标检测/Resnet-101 | 1619.505 | 1262.758 |\n",
    "| 目标检测/Conditional-DETR | 1137.513 | 897.390 |\n",
    "\n",
    "## PyTorch Nightly\n",
    "\n",
    "我们还在 PyTorch Nightly（2.1.0dev 版本，[下载链接](https://download.pytorch.org/whl/nightly/cu118)）上进行了基准测试，并观察到未编译和编译模型的延迟都有所改进。\n",
    "\n",
    "### A100\n",
    "\n",
    "| **任务/模型** | **批量大小** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 图像分类/BeiT | 未批量 | 12.462 | 6.954 |\n",
    "| 图像分类/BeiT | 4   | 14.109 | 12.851 |\n",
    "| 图像分类/BeiT | 16  | 42.179 | 42.147 |\n",
    "| 目标检测/DETR | 未批量 | 30.484 | 15.221 |\n",
    "| 目标检测/DETR | 4   | 46.816 | 30.942 |\n",
    "| 目标检测/DETR | 16  | 163.749 | 163.706 |\n",
    "\n",
    "### T4\n",
    "\n",
    "| **任务/模型** | **批量大小** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 图像分类/BeiT | 未批量 | 14.408 | 14.052 |\n",
    "| 图像分类/BeiT | 4   | 47.381 | 46.604 |\n",
    "| 图像分类/BeiT | 16  | 42.179 | 42.147 |\n",
    "| 目标检测/DETR | 未批量 | 68.382 | 53.481 |\n",
    "| 目标检测/DETR | 4   | 269.615 | 204.785 |\n",
    "| 目标检测/DETR | 16  | OOM | OOM |\n",
    "\n",
    "### V100\n",
    "\n",
    "| **任务/模型** | **批量大小** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 图像分类/BeiT | 未批量 | 13.477 | 7.926 |\n",
    "| 图像分类/BeiT | 4   | 15.103 | 14.378 |\n",
    "| 图像分类/BeiT | 16  | 52.517 | 51.691 |\n",
    "| 目标检测/DETR | 未批量 | 28.706 | 19.077 |\n",
    "| 目标检测/DETR | 4   | 88.402 | 62.949 |\n",
    "| 目标检测/DETR | 16  | OOM | OOM |\n",
    "\n",
    "## 减少开销模式基准测试\n",
    "\n",
    "我们在 A100 和 T4 上使用 PyTorch Nightly 测试了 `reduce-overhead` 模式。\n",
    "\n",
    "### A100\n",
    "\n",
    "| **任务/模型** | **批量大小** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 图像分类/ConvNeXT | 未批量 | 11.758 | 7.335 |\n",
    "| 图像分类/ConvNeXT | 4   | 23.171 | 21.490 |\n",
    "| 图像分类/ResNet | 未批量 | 7.435 | 3.801 |\n",
    "| 图像分类/ResNet | 4   | 7.261 | 2.187 |\n",
    "| 目标检测/Conditional-DETR | 未批量 | 32.823 | 11.627 |\n",
    "| 目标检测/Conditional-DETR | 4   | 50.622 | 33.831 |\n",
    "| 图像分割/MobileNet | 未批量 | 9.869 | 4.244 |\n",
    "| 图像分割/MobileNet | 4   | 14.385 | 7.946 |\n",
    "\n",
    "### T4\n",
    "\n",
    "| **任务/模型** | **批量大小** | **torch 2.0 - 未编译** | **torch 2.0 - 编译** |\n",
    "| :---: | :---: | :---: | :---: |\n",
    "| 图像分类/ConvNeXT | 未批量 | 32.137 | 31.84 |\n",
    "| 图像分类/ConvNeXT | 4   | 120.944 | 110.209 |\n",
    "| 图像分类/ResNet | 未批量 | 9.761 | 7.698 |\n",
    "| 图像分类/ResNet | 4   | 15.215 | 13.871 |\n",
    "| 目标检测/Conditional-DETR | 未批量 | 72.150 | 57.660 |\n",
    "| 目标检测/Conditional-DETR | 4   | 301.494 | 247.543 |\n",
    "| 图像分割/MobileNet | 未批量 | 22.266 | 19.339 |\n",
    "| 图像分割/MobileNet | 4   | 78.311 | 50.983 |"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
