{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelineæ¨ç†\n",
    "\n",
    "`pipeline()` è®©ä½¿ç”¨ `ğŸ¤— Hugging Face Hub` ä¸Šçš„ä»»ä½•æ¨¡å‹è¿›è¡Œä»»ä½•è¯­è¨€ã€è®¡ç®—æœºè§†è§‰ã€è¯­éŸ³ä»¥åŠå¤šæ¨¡æ€ä»»åŠ¡çš„æ¨ç†éƒ½å˜å¾—éå¸¸ç®€å•ã€‚\n",
    "\n",
    "å³ä½¿ä½ å¯¹ç‰¹å®šçš„æ¨¡æ€æ²¡æœ‰ç»éªŒï¼Œæˆ–è€…ä¸ç†Ÿæ‚‰æ¨¡å‹çš„æºç ï¼Œä½ ä»ç„¶å¯ä»¥ä½¿ç”¨pipeline()è¿›è¡Œæ¨ç†ï¼\n",
    "\n",
    "æœ¬æ•™ç¨‹è¯´æ˜ï¼š\n",
    "\n",
    "1. å¦‚ä½•ä½¿ç”¨ pipeline() è¿›è¡Œæ¨ç†ã€‚\n",
    "2. å¦‚ä½•ä½¿ç”¨ç‰¹å®šçš„ tokenizer(åˆ†è¯å™¨) æˆ– model(æ¨¡å‹)ã€‚\n",
    "3. å¦‚ä½•ä½¿ç”¨ pipeline() è¿›è¡ŒéŸ³é¢‘ã€è§†è§‰å’Œå¤šæ¨¡æ€ä»»åŠ¡çš„æ¨ç†ã€‚\n",
    "\n",
    "```\n",
    "è¯·æŸ¥çœ‹[pipeline()](https://huggingface.co/docs/transformers/v4.44.2/zh/main_classes/pipelines#transformers.pipeline)æ–‡æ¡£ä»¥è·å–å·²æ”¯æŒçš„ä»»åŠ¡å’Œå¯ç”¨å‚æ•°çš„å®Œæ•´åˆ—è¡¨ã€‚\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Pipelineä½¿ç”¨\n",
    "\n",
    "è™½ç„¶æ¯ç§ä»»åŠ¡éƒ½æœ‰ä¸€ä¸ªå…³è”çš„ pipelineï¼Œä½†æˆ‘ä»¬å¯ä»¥ä½¿ç”¨é€šç”¨çš„ `pipeline()` æ–¹æ³•ï¼Œå…¶ä¸­åŒ…å«æ‰€æœ‰ç‰¹å®šä»»åŠ¡çš„ pipelinesï¼Œ**èƒ½å¤Ÿæ ¹æ®ä»»åŠ¡ç±»å‹è‡ªåŠ¨åŠ è½½ä¸€ä¸ªé»˜è®¤æ¨¡å‹å’Œä¸€ä¸ªèƒ½å¤Ÿè¿›è¡Œä»»åŠ¡æ¨ç†çš„é¢„å¤„ç†ç±»**ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬ä»¥ä½¿ç”¨ pipeline() è¿›è¡Œè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æˆ–è¯­éŸ³è½¬æ–‡æœ¬ä¸ºä¾‹ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. é¦–å…ˆï¼Œåˆ›å»ºä¸€ä¸ª pipeline() å¹¶æŒ‡å®šæ¨ç†ä»»åŠ¡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\", device=0) # ä»»åŠ¡ï¼šè‡ªåŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. å°†ä½ çš„è¾“å…¥ä¼ é€’ç»™ pipeline()ã€‚å¯¹äºè¯­éŸ³è¯†åˆ«ï¼Œé€šå¸¸æ˜¯è¾“å…¥ä¸€ä¸ªéŸ³é¢‘æ–‡ä»¶ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber(\"../resources/speech/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. å¦‚æœä½ åœ¨ MAC OS ä¸­è¿è¡Œï¼Œå¯èƒ½ä¼šé‡åˆ°é”™è¯¯ä¿¡æ¯ \"ffmpeg was not found but is required to load audio files from filename\"ã€‚è¿™è¡¨ç¤ºä½ çš„ç¯å¢ƒä¸­ç¼ºå°‘ `ffmpeg`ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºå¤„ç†éŸ³é¢‘å’Œè§†é¢‘æ–‡ä»¶çš„å¼ºå¤§å·¥å…·ã€‚åœ¨ macOS ä¸Šï¼Œä½ å¯ä»¥ä½¿ç”¨ Homebrew æ¥å®‰è£… ffmpegã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "brew install ffmpeg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœç»“æœä¸ç¬¦åˆä½ çš„é¢„æœŸï¼Œè¿˜å¯ä»¥åœ¨ Hub ä¸ŠæŸ¥çœ‹ä¸€äº›[æœ€å—æ¬¢è¿çš„è‡ªåŠ¨è¯­éŸ³è¯†åˆ«æ¨¡å‹](https://huggingface.co/models?pipeline_tag=automatic-speech-recognition&sort=trending)ï¼Œçœ‹çœ‹æ˜¯å¦å¯ä»¥è·å¾—æ›´å¥½çš„è½¬å½•ç»“æœã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬å°è¯•æ¥è‡ª OpenAI çš„ [Whisper large-v2](https://huggingface.co/openai/whisper-large) æ¨¡å‹ã€‚Whisperb æ¯” Wav2Vec2 æ™š2å¹´å‘å¸ƒï¼Œä½¿ç”¨æ¥è¿‘10å€çš„æ•°æ®è¿›è¡Œäº†è®­ç»ƒã€‚å› æ­¤ï¼Œå®ƒåœ¨å¤§å¤šæ•°ä¸‹æ¸¸åŸºå‡†æµ‹è¯•ä¸Šå‡»è´¥äº† Wav2Vec2ã€‚ å®ƒè¿˜å…·æœ‰é¢„æµ‹æ ‡ç‚¹å’Œå¤§å°å†™çš„é™„åŠ ä¼˜åŠ¿ï¼Œè€Œ Wav2Vec2 åˆ™æ— æ³•å®ç°è¿™äº›åŠŸèƒ½ã€‚\n",
    "\n",
    "è®©æˆ‘ä»¬åœ¨è¿™é‡Œå°è¯•ä¸€ä¸‹ï¼Œçœ‹çœ‹å®ƒçš„è¡¨ç°å¦‚ä½•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v2\")\n",
    "\n",
    "transcriber(\"../resources/speech/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç°åœ¨è¿™ä¸ªç»“æœçœ‹èµ·æ¥æ›´å‡†ç¡®äº†ï¼\n",
    "\n",
    "è¦è¿›è¡Œæ·±å…¥çš„ Wav2Vec2 ä¸ Whisper æ¯”è¾ƒï¼Œè¯·å‚é˜…[éŸ³é¢‘å˜æ¢å™¨è¯¾ç¨‹](https://huggingface.co/learn/audio-course/chapter5/asr_models)ã€‚ \n",
    "\n",
    "å»ºè®®åœ¨ Hub ä¸ŠæŸ¥çœ‹ä¸åŒè¯­è¨€çš„æ¨¡å‹ï¼Œä»¥åŠä¸“ä¸šé¢†åŸŸçš„æ¨¡å‹ç­‰ã€‚å¯ä»¥åœ¨ Hub ä¸Šç›´æ¥æŸ¥çœ‹å¹¶æ¯”è¾ƒæ¨¡å‹çš„ç»“æœï¼Œä»¥ç¡®å®šæ˜¯å¦é€‚åˆæˆ–å¤„ç†è¾¹ç¼˜æƒ…å†µæ˜¯å¦æ¯”å…¶ä»–æ¨¡å‹æ›´å¥½ã€‚\n",
    "\n",
    "å¦‚æœæ²¡æœ‰æ‰¾åˆ°é€‚ç”¨äºä½ çš„ç”¨ä¾‹çš„æ¨¡å‹ï¼Œå¯ä»¥é€šè¿‡[è®­ç»ƒ](https://huggingface.co/docs/transformers/v4.44.2/zh/training)æ¥è·å¾—é€‚åˆè‡ªå·±çš„æ¨¡å‹ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æœ‰å¤šä¸ªè¾“å…¥ï¼Œå¯ä»¥å°†è¾“å…¥ä½œä¸ºåˆ—è¡¨ä¼ é€’ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber(\n",
    "    [\n",
    "        \"../resources/speech/mlk.flac\",\n",
    "        \"../resources/speech/1.flac\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»ä¸€ä¸ªæ¨¡å‹åˆ‡æ¢åˆ°å¦ä¸€ä¸ªæ¨¡å‹éå¸¸çç¢çš„è¿‡ç¨‹ï¼Œä½†ä½¿ç”¨ Pipelines èƒ½å¤Ÿéå¸¸è¿…é€Ÿçš„åˆ‡æ¢æ¨¡å‹ï¼Œæ‰€ä»¥å®ƒéå¸¸é€‚åˆç”¨äºæµ‹è¯•ã€‚\n",
    "\n",
    "ä½†æ˜¯ï¼Œé€šè¿‡ä¸€äº›æ–¹æ³•è¿˜å¯ä»¥å°† Pipelines ç”¨äºå¤§å‹å·¥ä½œè´Ÿè½½è€Œä¸ä»…ä»…æ˜¯æµ‹è¯•ã€‚è¯¦ç»†è¯·æŸ¥çœ‹ä»¥ä¸‹æŒ‡å—ï¼Œæ·±å…¥æ¢è®¨å¦‚ä½•è¿­ä»£æ•´ä¸ªæ•°æ®é›†æˆ–åœ¨ Web æœåŠ¡å™¨ä¸­ä½¿ç”¨ Pipelinesï¼š\n",
    "\n",
    "1. [åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨æµæ°´çº¿](https://huggingface.co/docs/transformers/v4.44.2/zh/pipeline_tutorial#using-pipelines-on-a-dataset)\n",
    "2. [åœ¨WebæœåŠ¡å™¨ä¸­ä½¿ç”¨æµæ°´çº¿](https://huggingface.co/docs/transformers/v4.44.2/zh/pipeline_webserver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å‚æ•°\n",
    "\n",
    "`pipeline()` æ”¯æŒäº†è®¸å¤šå‚æ•°ï¼›æœ‰äº›æ˜¯é€‚ç”¨äºç‰¹å®šä»»åŠ¡çš„ï¼Œæœ‰äº›åˆ™æ˜¯é€‚ç”¨äºæ‰€æœ‰çš„ pipelineã€‚é€šå¸¸æƒ…å†µä¸‹ï¼Œä½ å¯ä»¥åœ¨ä»»ä½•åœ°æ–¹æŒ‡å®šå¯¹åº”çš„å‚æ•°ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", my_parameter=1)\n",
    "\n",
    "out = transcriber(...)  # è¿™å°†ä½¿ç”¨ `my_parameter=1`ã€‚\n",
    "out = transcriber(..., my_parameter=2)  # è¿™å°†è¦†ç›–å¹¶ä½¿ç”¨ `my_parameter=2`ã€‚\n",
    "out = transcriber(...)  # è¿™å°†è¿”å›åˆ°ä½¿ç”¨ `my_parameter=1`ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è®©æˆ‘ä»¬æŸ¥çœ‹å…¶ä¸­çš„ä¸‰ä¸ªé‡è¦å‚æ•°ï¼š**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### è®¾å¤‡\n",
    "\n",
    "å¦‚æœä½ è®¾ç½® `device=n`ï¼Œpipeline ä¼šè‡ªåŠ¨å°†æ¨¡å‹åŠ è½½åˆ°æŒ‡å®šçš„è®¾å¤‡ä¸Šï¼ˆåŒ…æ‹¬å­˜å‚¨æ¨¡å‹çš„æƒé‡ï¼‰ã€‚æ”¯æŒ PyTorch æˆ– Tensorflowã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", device=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ ä½¿ç”¨çš„æ˜¯ PyTorch ï¼Œä¸”è§‰å¾—æ¨¡å‹å¯¹äºå•ä¸ªGPUæ¥è¯´è¿‡äºåºå¤§ï¼Œåˆ™å¯ä»¥è®¾ç½® `device_map=\"auto\"` ä½¿å…¶è‡ªåŠ¨ç¡®å®šå¦‚ä½•åŠ è½½å’Œå­˜å‚¨æ¨¡å‹çš„æƒé‡ã€‚ä½¿ç”¨ `device_map` å‚æ•°éœ€è¦å®‰è£… `ğŸ¤— Accelerate` è½¯ä»¶åŒ…ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --upgrade accelerate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸‹ä»£ç ä¼šè‡ªåŠ¨åœ¨å„ä¸ªè®¾å¤‡ä¸ŠåŠ è½½å’Œå­˜å‚¨æ¨¡å‹æƒé‡ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¯·æ³¨æ„ï¼Œå¦‚æœè®¾ç½®äº† `device_map=\"auto\"`ï¼Œåœ¨å®ä¾‹åŒ– pipeline æ—¶åˆ™ä¸éœ€è¦è®¾ç½® `device=n` å‚æ•°ï¼Œå¦åˆ™å¯èƒ½ä¼šé‡åˆ°ä¸€äº›æ„å¤–çš„çŠ¶å†µï¼**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### æ‰¹é‡å¤§å°\n",
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œpipelines ä¸ä¼šè¿›è¡Œæ‰¹é‡æ¨ç†ï¼Œå› ä¸ºæ‰¹å¤„ç†ä¸ä¸€å®šæ›´å¿«ï¼Œåœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šæ›´æ…¢ï¼ŒåŸå› å¯æŸ¥é˜…[è¯¦ç»†è§£é‡Š](https://huggingface.co/docs/transformers/main_classes/pipelines#pipeline-batching)ã€‚\n",
    "\n",
    "æƒ³åœ¨ç”¨ä¾‹ä¸­ä½¿ç”¨æ‰¹é‡æ¨ç†ï¼Œå¦‚ä¸‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "audio_filenames = [os.path.join('../resources/speech', f\"{i}.flac\") for i in range(1, 5)]\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", device=0, batch_size=2)\n",
    "texts = transcriber(audio_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä»¥ä¸Šä»£ç ä¼šåœ¨æä¾›çš„4ä¸ªéŸ³é¢‘æ–‡ä»¶ä¸Šè¿è¡Œ pipelineï¼Œè®¾ç½® `batch_size=2` è¡¨ç¤º pipeline ä¼šå°†å®ƒä»¬ä»¥æ¯2ä¸ªä¸ºä¸€ç»„çš„æ‰¹æ¬¡ä¼ é€’ç»™æ¨¡å‹ï¼ˆè‹¥æ¨¡å‹åŠ è½½åœ¨GPUä¸Šï¼Œæ­¤æ—¶æ‰¹å¤„ç†å¯èƒ½æ›´æœ‰ä½œç”¨ï¼‰ï¼Œä¸éœ€è¦ç¼–å†™é¢å¤–çš„ä»£ç ã€‚\n",
    "\n",
    "**è¾“å‡ºçš„ç»“æœä¼šå§‹ç»ˆä¸æ²¡æœ‰æ‰¹å¤„ç†æ—¶æ”¶åˆ°çš„ç»“æœç›¸ä¸€è‡´ï¼Œå› ä¸ºæ‰¹é‡æ¨ç†åªæ˜¯ä¸€ç§å¸®åŠ©æ‚¨æ›´å¿«åœ°è¿è¡Œ pipeline çš„æ–¹å¼ï¼Œä½ å¯ä»¥å°†å®ƒç†è§£ä¸ºæ˜¯ pipeline çš„å¸¦å®½å¤§å°ã€‚**\n",
    "\n",
    "åŒæ—¶ï¼Œä½¿ç”¨ pipeline() èƒ½å¤Ÿå¸®åŠ©æˆ‘ä»¬å‡è½»å·¥ä½œé‡ï¼Œé™ä½æ‰¹å¤„ç†çš„å¤æ‚æ€§ã€‚å› ä¸ºå¯¹äºæŸäº› pipeline ï¼Œéœ€è¦å°†å•ä¸ªé¡¹ç›®ï¼ˆå¦‚é•¿éŸ³é¢‘æ–‡ä»¶ï¼‰åˆ†æˆå¤šä¸ªéƒ¨åˆ†åæ‰èƒ½ä¾›æ¨¡å‹å¤„ç†ã€‚**pipeline() ä¼šåœ¨éœ€è¦æ—¶è‡ªåŠ¨æ‰§è¡Œ chunk batchingï¼ˆåˆ†å—å¤„ç†ï¼‰ã€‚**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä»»åŠ¡ç‰¹å®šå‚æ•°\n",
    "\n",
    "transformers ä¸ºæ‰€æœ‰ä»»åŠ¡çš„ pipeline éƒ½æä¾›äº†ç‰¹å®šçš„å‚æ•°ï¼Œè¿™äº›å‚æ•°ä½¿å¾— pipeline æ›´å…·çµæ´»æ€§ï¼Œèƒ½å¤Ÿå¸®åŠ©ä½ æ›´å¥½åœ°å®Œæˆå·¥ä½œã€‚ \n",
    "\n",
    "ä¾‹å¦‚ï¼Œ`transformers.AutomaticSpeechRecognitionPipeline.call()` æ–¹æ³•å°±å…·æœ‰ä¸€ä¸ª `return_timestamps` å‚æ•°ï¼Œèƒ½å¤Ÿåœ¨æ¨¡å‹æ¨æ–­å‡ºæ–‡æœ¬çš„åŒæ—¶è¾“å‡ºå„ä¸ªå¥å­å‘éŸ³çš„æ—¶é—´æˆ³ï¼Œå¯¹äºéœ€è¦æ·»åŠ å­—å¹•çš„è§†é¢‘å¾ˆæœ‰å¸®åŠ©ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", return_timestamps=True)\n",
    "\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¯ä¸ªä»»åŠ¡éƒ½æœ‰è®¸å¤šå¯ç”¨çš„å‚æ•°ï¼Œå› æ­¤è¯·æŸ¥çœ‹æ¯ä¸ªä»»åŠ¡çš„APIå‚è€ƒã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œåœ¨ä¸ºæ•´éƒ¨ç”µå½±æˆ–é•¿è¾¾ä¸€å°æ—¶çš„è§†é¢‘æ·»åŠ å­—å¹•æ—¶ï¼Œéœ€è¦æ¨¡å‹å¤„ç†éå¸¸é•¿çš„éŸ³é¢‘æ–‡ä»¶ï¼Œä½†è¿™é€šå¸¸æ˜¯æ¨¡å‹æ— æ³•å•ç‹¬å¤„ç†çš„ï¼Œå¯ä»¥é€šè¿‡`AutomaticSpeechRecognitionPipeline` è®¾ç½® `chunk_length_s` å‚æ•°æ¥æ§åˆ¶éŸ³é¢‘å¤„ç†çš„åˆ†å—é•¿åº¦ï¼Œè¿™éå¸¸æœ‰å¸®åŠ©ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", chunk_length_s=30, return_timestamps=True)\n",
    "\n",
    "transcriber(\"https://huggingface.co/datasets/sanchit-gandhi/librispeech_long/resolve/main/audio.wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœä½ æ‰¾ä¸åˆ°ä»»ä½•ä¸€ä¸ªæœ‰å¸®åŠ©çš„å‚æ•°ï¼Œæ¬¢è¿[æå‡ºè¯·æ±‚](https://github.com/huggingface/transformers/issues/new?assignees=&labels=feature&template=feature-request.yml)ï¼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨æ•°æ®é›†ä¸Šä½¿ç”¨ pipelines\n",
    "\n",
    "pipelines ä¹Ÿå¯ä»¥å¯¹å¤§å‹æ•°æ®é›†è¿›è¡Œæ¨ç†ã€‚\n",
    "\n",
    "`yield` åœ¨Pythonä¸­æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„æ ‡è¯†ç¬¦ï¼Œç”¨äºå°†å‡½æ•°è½¬æ¢ä¸ºç”Ÿæˆå™¨ï¼ˆè¿­ä»£å™¨ï¼‰ã€‚ç”Ÿæˆå™¨å¯ä»¥ä¸€æ¬¡ç”Ÿæˆå¹¶è¿”å›ä¸€ä¸ªå€¼ï¼Œè€Œä¸æ˜¯ä¸€æ¬¡æ€§è¿”å›æ‰€æœ‰å€¼ï¼Œä»è€ŒèŠ‚çœå†…å­˜ã€‚\n",
    "\n",
    "å»ºè®®ä½¿ç”¨ç”Ÿæˆå™¨æ¥å®Œæˆè¿™ä¸€ä»»åŠ¡ï¼Œè¿™æ˜¯æœ€ç®€å•çš„æ–¹æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data():\n",
    "    for i in range(1000):\n",
    "        yield f\"My example {i}\"\n",
    "\n",
    "\n",
    "pipe = pipeline(model=\"openai-community/gpt2\", device=0)\n",
    "generated_characters = 0\n",
    "for out in pipe(data()):\n",
    "    generated_characters += len(out[0][\"generated_text\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipelines ä¼šè‡ªåŠ¨å°†ç”Ÿæˆå™¨ `data()` è¯†åˆ«è¾“å…¥ä¸ºå¯è¿­ä»£å¯¹è±¡ï¼Œå¹¶åœ¨GPUä¸Šå¤„ç†æ•°æ®çš„åŒæ—¶å¼€å§‹è·å–æ•°æ®ï¼ˆåœ¨åº•å±‚ä½¿ç”¨DataLoaderï¼‰ã€‚è¿™ä¸€ç‚¹éå¸¸é‡è¦ï¼Œå› ä¸ºæ‚¨ä¸å¿…ä¸ºæ•´ä¸ªæ•°æ®é›†åˆ†é…å†…å­˜ï¼Œå¯ä»¥å°½å¯èƒ½å¿«åœ°å°†æ•°æ®ä¼ é€åˆ°GPUã€‚\n",
    "\n",
    "ç”±äºæ‰¹å¤„ç†å¯ä»¥åŠ é€Ÿå¤„ç†ï¼Œå› æ­¤åœ¨è¿™é‡Œå°è¯•è°ƒæ•´ `batch_size` å‚æ•°å¯èƒ½ä¼šå¾ˆæœ‰ç”¨ã€‚\n",
    "\n",
    "è¿­ä»£æ•°æ®é›†çš„æœ€ç®€å•æ–¹æ³•å°±æ˜¯ä»ğŸ¤— `(Datasets)[https://github.com/huggingface/datasets/]` ä¸­åŠ è½½æ•°æ®é›†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KeyDataset æ˜¯ä¸€ä¸ªå®ç”¨å·¥å…·ï¼Œå®ƒæ”¯æŒä» datasets åº“ä¸­çš„æ•°æ®é›†ä¸­æå–ç‰¹å®šçš„å­—æ®µï¼ˆæˆ–é”®ï¼‰ä¼ é€’ç»™ pipelinesã€‚\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "pipe = pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n",
    "\n",
    "for out in pipe(KeyDataset(dataset, \"audio\")): # KeyDataset(dataset, \"audio\") è¡¨ç¤ºä» dataset ä¸­æå–é”®ä¸º \"audio\" çš„æ•°æ®ã€‚\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨WebæœåŠ¡å™¨ä¸Šä½¿ç”¨ pipelines\n",
    "\n",
    "[Using pipelines for a webserver](https://huggingface.co/docs/transformers/main/en/pipeline_webserver)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è§†è§‰ä»»åŠ¡ pipeline()\n",
    "\n",
    "å¯¹äºä¸åŒè§†è§‰ä»»åŠ¡çš„ pipeline() è°ƒç”¨æ–¹å¼éƒ½éå¸¸ç›¸ä¼¼ï¼ŒæŒ‡å®šè§†è§‰ä»»åŠ¡æ¨¡å‹å¹¶å°†å›¾åƒä¼ é€’ç»™åˆ†ç±»å™¨ã€‚\n",
    "\n",
    "è¾“å…¥çš„å›¾åƒå¯ä»¥æ˜¯é“¾æ¥ã€æœ¬åœ°è·¯å¾„æˆ–base64ç¼–ç çš„å›¾åƒã€‚ä¾‹å¦‚ï¼Œä¸‹é¢æ˜¾ç¤ºçš„æ˜¯å“ªç§å“ç§çš„çŒ«ï¼Ÿ\n",
    "\n",
    "![pipeline-cat-chonk.png](../resources/images/pipeline-cat-chonk.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\", device=0)\n",
    "preds = vision_classifier(\n",
    "    images=\"../resources/images/pipeline-cat-chonk.png\"\n",
    ")\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è¾“å‡ºç»“æœè§£é‡Š**\n",
    "æ¨¡å‹çš„è¾“å‡ºç»“æœ `preds` æ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å«ä¸¤ä¸ªé”®ï¼š`score` å’Œ `label`ã€‚\n",
    "- scoreï¼šè¡¨ç¤ºæ¨¡å‹é¢„æµ‹è¯¥ç±»åˆ«å¯¹åº”çš„ç½®ä¿¡åˆ†æ•°ï¼ŒèŒƒå›´é€šå¸¸åœ¨0åˆ°1ä¹‹é—´ï¼Œå€¼è¶Šæ¥è¿‘1è¡¨ç¤ºæ¨¡å‹è¶Šç¡®ä¿¡è¯¥ç±»åˆ«ã€‚\n",
    "- labelï¼šè¡¨ç¤ºé¢„æµ‹çš„ç±»åˆ«æ ‡ç­¾ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ pipeline()\n",
    "\n",
    "å¯¹äºä¸åŒNLPä»»åŠ¡çš„ pipeline() è°ƒç”¨æ–¹å¼éƒ½éå¸¸ç›¸ä¼¼ï¼Œä¸»è¦åŒºåˆ«åœ¨äºä»»åŠ¡ç±»å‹å’Œæ¨¡å‹çš„æŒ‡å®šã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# `facebook/bart-large-mnli`æ˜¯ä¸€ä¸ª`zero-shot-classification`æ¨¡å‹ï¼Œèƒ½å¤Ÿåœ¨æ²¡æœ‰é’ˆå¯¹ç‰¹å®šç±»åˆ«è¿›è¡Œä¸“é—¨è®­ç»ƒçš„æƒ…å†µä¸‹ï¼Œå¯¹æ–‡æœ¬è¿›è¡Œçµæ´»çš„åˆ†ç±»ã€‚\n",
    "classifier = pipeline(model=\"facebook/bart-large-mnli\", device=0)\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],# å®ƒèƒ½å¤Ÿå¯¹æ–‡æœ¬å†…å®¹è¿›è¡Œåˆ†ç±»ï¼Œæ‚¨å¯ä»¥è‡ªç”±åœ°å®šä¹‰ä»»ä½•æ ‡ç­¾ä½œä¸ºå€™é€‰ç±»åˆ«ã€‚\n",
    ")\n",
    "# æ¨¡å‹çš„è¾“å‡ºé€šå¸¸ä¼šåŒ…å«æ¯ä¸ªå€™é€‰æ ‡ç­¾çš„ç½®ä¿¡åˆ†æ•°ï¼Œè¡¨ç¤ºæ–‡æœ¬å±äºè¯¥æ ‡ç­¾çš„å¯èƒ½æ€§ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## å¤šæ¨¡æ€ pipeline()\n",
    "\n",
    "pipeline() æ”¯æŒå¤šæ¨¡æ€ä»»åŠ¡çš„è°ƒç”¨ã€‚\n",
    "\n",
    "å¯¹äºè§†è§‰é—®é¢˜å›ç­”ä»»åŠ¡ï¼ˆVQAï¼‰ï¼Œpipeline() ç»“åˆäº†æ–‡æœ¬å’Œå›¾åƒï¼Œä½¿å¾—å¯ä»¥å¤„ç†ä»»ä½•å›¾åƒå’Œä»»ä½•å…³äºè¯¥å›¾åƒçš„é—®é¢˜ã€‚\n",
    "\n",
    "ä¾‹å¦‚ï¼Œä½ æƒ³æé—®å…³äºè¯¥å›¾åƒçš„é—®é¢˜ï¼š\n",
    "\n",
    "![invoice image](../resources/images/invoice.png)ï¼š"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "è¦è¿è¡Œä¸‹é¢çš„ç¤ºä¾‹ï¼Œé™¤äº†å®‰è£…ğŸ¤— Transformers ä¹‹å¤–ï¼Œè¿˜éœ€è¦å®‰è£… `pytesseract` ã€‚\n",
    "\n",
    "**Mac OS**\n",
    "```shell\n",
    "brew install tesseract\n",
    "```\n",
    "**Windows**\n",
    "\n",
    "[Tesseract installer for Windows](https://github.com/UB-Mannheim/tesseract/wiki)\n",
    "\n",
    "**Python**\n",
    "```shell\n",
    "pip install pytesseract\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vqa = pipeline(model=\"impira/layoutlm-document-qa\", device=0)\n",
    "output = vqa(\n",
    "    image=\"../resources/images/invoice.png\",\n",
    "    question=\"What is the invoice number?\",\n",
    ")\n",
    "output[0][\"score\"] = round(output[0][\"score\"], 3)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## åœ¨å¤§æ¨¡å‹ä¸Šä½¿ç”¨ğŸ¤— accelerate å’Œ pipeline ï¼š\n",
    "\n",
    "ä½ å¯ä»¥è½»æ¾åœ°ä½¿ç”¨ `ğŸ¤— accelerate` åœ¨å¤§æ¨¡å‹ä¸Šè¿è¡Œ pipelineï¼\n",
    "\n",
    "é¦–å…ˆç¡®ä¿ä½ å·²ç»ä½¿ç”¨ `pip install accelerate` å®‰è£…äº† accelerateã€‚\n",
    "\n",
    "é¦–å…ˆä½¿ç”¨ `device_map=\"auto\"` åŠ è½½æ‚¨çš„æ¨¡å‹ï¼æˆ‘ä»¬å°†åœ¨ç¤ºä¾‹ä¸­ä½¿ç”¨æ¨¡å‹ `facebook/opt-1.3b`ã€‚\n",
    "\n",
    "**device_map=\"auto\"çš„ä½œç”¨**\n",
    "\n",
    "- è‡ªåŠ¨è®¾å¤‡åˆ†é…ï¼š\n",
    "    1. å½“ä½ è®¾ç½®device_map=\"auto\"æ—¶ï¼Œtransformersåº“ä¼šè‡ªåŠ¨æ£€æµ‹ä½ å½“å‰çš„ç¡¬ä»¶é…ç½®ï¼ˆåŒ…æ‹¬CPUå’ŒGPUçš„æ•°é‡åŠå†…å­˜å¤§å°ï¼‰ã€‚\n",
    "    2. æ ¹æ®æ£€æµ‹åˆ°çš„ç¡¬ä»¶é…ç½®ï¼Œå®ƒä¼šæ™ºèƒ½åœ°å°†æ¨¡å‹çš„ä¸åŒå±‚åˆ†é…åˆ°ä¸åŒçš„è®¾å¤‡ä¸Šã€‚ä¾‹å¦‚ï¼Œå¦‚æœæœ‰ä¸€ä¸ªå¤§å‹æ¨¡å‹å’Œä¸€ä¸ªGPUï¼Œå®ƒå¯èƒ½ä¼šå°†æ¨¡å‹çš„ä¸€éƒ¨åˆ†æ”¾åœ¨GPUä¸Šï¼Œå¦ä¸€éƒ¨åˆ†æ”¾åœ¨CPUä¸Šï¼Œä»¥é¿å…å†…å­˜æº¢å‡ºã€‚\n",
    "- ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼š\n",
    "    1. é€šè¿‡æ™ºèƒ½åˆ†é…ï¼Œå¯ä»¥æœ€å¤§é™åº¦åœ°åˆ©ç”¨å¯ç”¨çš„ç¡¬ä»¶èµ„æºï¼Œé¿å…å•ä¸ªè®¾å¤‡ï¼ˆå¦‚å•ä¸ªGPUï¼‰å› å†…å­˜ä¸è¶³è€Œæ— æ³•åŠ è½½æ•´ä¸ªæ¨¡å‹ã€‚\n",
    "    2. è¿™å¯¹äºå¤„ç†åƒ `facebook/opt-1.3b` è¿™æ ·çš„å¤§å‹æ¨¡å‹å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºè¿™äº›æ¨¡å‹å¯èƒ½éœ€è¦å¤§é‡çš„å†…å­˜ã€‚\n",
    "- æé«˜è®¡ç®—æ•ˆç‡ï¼š\n",
    "    è‡ªåŠ¨è®¾å¤‡åˆ†é…ä¸ä»…ä¼˜åŒ–äº†å†…å­˜ä½¿ç”¨ï¼Œè¿˜å¯ä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚ä¾‹å¦‚ï¼Œ**å°†è®¡ç®—å¯†é›†çš„éƒ¨åˆ†æ”¾åœ¨GPUä¸Šï¼Œè€Œå°†å…¶ä»–éƒ¨åˆ†æ”¾åœ¨CPUä¸Šï¼Œå¯ä»¥å……åˆ†åˆ©ç”¨ä¸åŒè®¾å¤‡çš„è®¡ç®—èƒ½åŠ›**ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "try:\n",
    "    import accelerate\n",
    "except ImportError:\n",
    "    raise ImportError(\"'accelerate' could not be resolved. Please install it by 'pip install accelerate'.\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n",
    "output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "å¦‚æœå®‰è£… `bitsandbytes` å¯ä»¥é€šè¿‡æ·»åŠ å‚æ•° `load_in_8bit=True` ä»¥8ä½ç²¾åº¦åŠ è½½æ¨¡å‹ã€‚\n",
    "\n",
    "`load_in_8bit=True`æ˜¯ä¸€ä¸ªç”¨äºæ¨¡å‹åŠ è½½çš„å‚æ•°ï¼Œå®ƒæŒ‡ç¤º transformers åº“**ä»¥8ä½ç²¾åº¦åŠ è½½æ¨¡å‹**ï¼ˆè€Œä¸æ˜¯æ ‡å‡†çš„16ä½æˆ–32ä½ç²¾åº¦ï¼‰ã€‚è¿™ä¸ªå‚æ•°é€šå¸¸ä¸`bitsandbytes`åº“ç»“åˆä½¿ç”¨ï¼Œä»¥å®ç°æ¨¡å‹çš„é‡åŒ–ï¼Œä»è€Œå‡å°‘å†…å­˜å ç”¨å’Œæé«˜æ¨ç†é€Ÿåº¦ã€‚è™½ç„¶8ä½é‡åŒ–é€šå¸¸ä¼šä¿ç•™æ¨¡å‹çš„ç»å¤§éƒ¨åˆ†æ€§èƒ½ï¼Œä½†åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½ä¼šç•¥å¾®é™ä½æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "try:\n",
    "    import accelerate\n",
    "except ImportError:\n",
    "    raise ImportError(\"'accelerate' could not be resolved. Please install it by 'pip install accelerate'.\")\n",
    "# pip install bitsandbytes\n",
    "try:\n",
    "    import bitsandbytes\n",
    "except ImportError:\n",
    "    raise ImportError(\"'bitsandbytes' could not be resolved. Please install it by 'pip install bitsandbytes'.\")\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", device_map=\"auto\", model_kwargs={\"load_in_8bit\": True})\n",
    "\n",
    "# ä½¿ç”¨éšæœºæŠ½æ ·\n",
    "output_sample = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)\n",
    "print(\"With do_sample=True:\", output_sample[0]['generated_text'])\n",
    "\n",
    "# ä¸ä½¿ç”¨éšæœºæŠ½æ ·\n",
    "output_greedy = pipe(\"This is a cool example!\", do_sample=False)\n",
    "print(\"With do_sample=False:\", output_greedy[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**do_sample éšæœºæŠ½æ ·**\n",
    "\n",
    "`do_sample`å‚æ•°å†³å®šäº†æ¨¡å‹åœ¨ç”Ÿæˆä¸‹ä¸€ä¸ªtokenæ—¶æ˜¯é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenï¼Œè¿˜æ˜¯ä»æ¦‚ç‡åˆ†å¸ƒä¸­éšæœºæŠ½å–ä¸€ä¸ªtokenã€‚\n",
    "- å½“`do_sample=True`æ—¶ï¼Œæ¨¡å‹ä¼šåœ¨æ¯ä¸ªæ—¶é—´æ­¥æ ¹æ®æ¦‚ç‡åˆ†å¸ƒéšæœºé€‰æ‹©ä¸‹ä¸€ä¸ªtokenã€‚è¿™ç§æ–¹æ³•å¯ä»¥å¢åŠ ç”Ÿæˆæ–‡æœ¬çš„å¤šæ ·æ€§å’Œåˆ›é€ æ€§ï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´è‡ªç„¶ã€æ›´ä¸é‡å¤ã€‚\n",
    "- å½“`do_sample=False`æ—¶ï¼Œæ¨¡å‹å§‹ç»ˆé€‰æ‹©æ¦‚ç‡æœ€é«˜çš„tokenä½œä¸ºä¸‹ä¸€ä¸ªtokenã€‚è¿™ç§æ–¹æ³•ç”Ÿæˆçš„æ–‡æœ¬é€šå¸¸æ›´ä¿å®ˆã€æ›´å¯é¢„æµ‹ï¼Œä½†å¯èƒ½ä¼šæ˜¾å¾—å•è°ƒã€ç¼ºä¹å¤šæ ·æ€§ã€‚\n",
    "\n",
    "**å‚æ•°ç»„åˆ**\n",
    "\n",
    "`do_sample`é€šå¸¸ä¸top_kå’Œtop_pï¼ˆä¹Ÿç§°ä¸ºnucleus samplingï¼‰ç­‰å‚æ•°ç»“åˆä½¿ç”¨ï¼Œä»¥è¿›ä¸€æ­¥æ§åˆ¶æŠ½æ ·è¿‡ç¨‹ã€‚\n",
    "- `top_k`ï¼šä¸ `do_sample=True` ç»“åˆä½¿ç”¨æ—¶ï¼Œè¡¨ç¤ºä»æ¦‚ç‡æœ€é«˜çš„ k ä¸ª token ä¸­è¿›è¡ŒéšæœºæŠ½æ ·ã€‚\n",
    "- `top_p`ï¼šä¸ `do_sample=True` ç»“åˆä½¿ç”¨æ—¶ï¼Œè¡¨ç¤ºä»ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ p çš„ token é›†åˆä¸­è¿›è¡ŒéšæœºæŠ½æ ·ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "åœ¨Hugging Faceçš„æ–‡æ¡£ä¸­ï¼Œ\"checkpoint\" æ˜¯ä¸€ä¸ªæ³›æŒ‡ï¼Œä»£è¡¨ä½ é€‰æ‹©çš„é¢„è®­ç»ƒæ¨¡å‹ã€‚åœ¨ä»£ç ä¸­ï¼Œä½ é€šè¿‡æ›¿æ¢ \"model\" å‚æ•°çš„å€¼æ¥é€‰æ‹©ä¸åŒçš„æ¨¡å‹ã€‚\n",
    "```\n",
    "\n",
    "è¯·æ³¨æ„ï¼Œæ‚¨å¯ä»¥å°† `checkpoint` æ›¿æ¢ä¸ºä»»ä½•æ”¯æŒå¤§æ¨¡å‹åŠ è½½çš„ Hugging Face æ¨¡å‹ï¼Œæ¯”å¦‚ `BLOOM`ã€‚åªéœ€å°†æ¨¡å‹ `facebook/opt-1.3b` æ›¿æ¢ä¸º `bigscience/bloom-1b1` å³å¯ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "# æ›¿æ¢æ¨¡å‹åç§°ä¸ºBLOOMæ¨¡å‹\n",
    "model_name = \"bigscience/bloom-1b1\"\n",
    "\n",
    "# åˆ›å»ºæ–‡æœ¬ç”Ÿæˆç®¡é“\n",
    "pipe = pipeline(\n",
    "    model=model_name,\n",
    "    device_map=\"auto\",\n",
    "    model_kwargs={\"load_in_8bit\": True}\n",
    ")\n",
    "\n",
    "# ç”Ÿæˆæ–‡æœ¬\n",
    "output = pipe(\n",
    "    \"This is a cool example!\",\n",
    "    do_sample=True,\n",
    "    top_p=0.95\n",
    ")\n",
    "\n",
    "# æ‰“å°ç”Ÿæˆçš„æ–‡æœ¬\n",
    "print(output[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers-playground-windows-ctv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
