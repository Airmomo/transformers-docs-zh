{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70d00e34",
   "metadata": {},
   "source": [
    "# Optimum-quanto â€”â€” PyTorch å¤šåŠŸèƒ½é‡åŒ–å·¥å…·åŒ…\n",
    "\n",
    "ä½ å¯ä»¥é€šè¿‡è¿™ä¸ª [ç¬”è®°æœ¬](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing) æ¥å°è¯•ä½¿ç”¨ optimum-quanto + transformersï¼\n",
    "\n",
    "[ğŸ¤— optimum-quanto](https://github.com/huggingface/optimum-quanto) æ˜¯ä¸€ä¸ªå¤šåŠŸèƒ½çš„ PyTorch é‡åŒ–å·¥å…·åŒ…ã€‚å®ƒä½¿ç”¨çš„é‡åŒ–æ–¹æ³•æ˜¯çº¿æ€§é‡åŒ–ã€‚Optimum-quanto æä¾›äº†ä»¥ä¸‹å‡ ä¸ªç‹¬ç‰¹åŠŸèƒ½ï¼š\n",
    "\n",
    "- æƒé‡é‡åŒ–ï¼ˆ`float8`, `int8`, `int4`, `int2`ï¼‰\n",
    "- æ¿€æ´»é‡åŒ–ï¼ˆ`float8`, `int8`ï¼‰\n",
    "- æ¨¡æ€æ— å…³ï¼ˆä¾‹å¦‚ CV, LLMï¼‰\n",
    "- è®¾å¤‡æ— å…³ï¼ˆä¾‹å¦‚ CUDA, XPU, MPS, CPUï¼‰\n",
    "- ä¸ `torch.compile` å…¼å®¹\n",
    "- å®¹æ˜“æ·»åŠ ç‰¹å®šè®¾å¤‡çš„è‡ªå®šä¹‰å†…æ ¸\n",
    "- æ”¯æŒé‡åŒ–æ„ŸçŸ¥è®­ç»ƒ\n",
    "\n",
    "åœ¨å¼€å§‹ä¹‹å‰ï¼Œè¯·ç¡®ä¿å®‰è£…äº†ä»¥ä¸‹åº“ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a15092",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install optimum-quanto accelerate transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308ba4ba",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ï¼Œä½ å¯ä»¥é€šè¿‡åœ¨ [from_pretrained()](/docs/transformers/v4.47.0/en/main_classes/model#transformers.PreTrainedModel.from_pretrained) æ–¹æ³•ä¸­ä¼ é€’ [QuantoConfig](/docs/transformers/v4.47.0/en/main_classes/quantization#transformers.QuantoConfig) å¯¹è±¡æ¥é‡åŒ–æ¨¡å‹ã€‚åªè¦æ¨¡å‹åŒ…å« `torch.nn.Linear` å±‚ï¼Œè¿™é€‚ç”¨äºä»»ä½•æ¨¡æ€çš„ä»»ä½•æ¨¡å‹ã€‚\n",
    "\n",
    "Transformers é›†æˆä»…æ”¯æŒæƒé‡é‡åŒ–ã€‚å¯¹äºæ›´å¤æ‚çš„æƒ…å†µï¼Œå¦‚æ¿€æ´»é‡åŒ–ã€æ ¡å‡†å’Œé‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼Œä½ åº”è¯¥ä½¿ç”¨ [optimum-quanto](https://github.com/huggingface/optimum-quanto) åº“ã€‚\n",
    "\n",
    "é»˜è®¤æƒ…å†µä¸‹ï¼Œæ— è®ºæƒé‡å®é™…å­˜å‚¨çš„æ•°æ®ç±»å‹ï¼ˆå¦‚ `torch.float16`ï¼‰ï¼Œæƒé‡éƒ½ä¼šä»¥å…¨ç²¾åº¦ï¼ˆ`torch.float32`ï¼‰åŠ è½½ã€‚è®¾ç½® `torch_dtype=\"auto\"` å¯ä»¥æ ¹æ®æ¨¡å‹çš„ `config.json` æ–‡ä»¶ä¸­å®šä¹‰çš„æ•°æ®ç±»å‹è‡ªåŠ¨åŠ è½½æœ€èŠ‚çœå†…å­˜çš„æ•°æ®ç±»å‹ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb30386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n",
    "\n",
    "model_id = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "quantization_config = QuantoConfig(weights=\"int8\")\n",
    "quantized_model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", device_map=\"cuda:0\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f7c129e",
   "metadata": {},
   "source": [
    "\n",
    "è¯·æ³¨æ„ï¼Œç›®å‰ Transformers å°šä¸æ”¯æŒåºåˆ—åŒ–ï¼Œä½†è¿™å³å°†å®ç°ï¼å¦‚æœä½ æƒ³è¦ä¿å­˜æ¨¡å‹ï¼Œå¯ä»¥ä½¿ç”¨ quanto åº“ã€‚\n",
    "\n",
    "Optimum-quanto åº“ä½¿ç”¨çº¿æ€§é‡åŒ–ç®—æ³•è¿›è¡Œé‡åŒ–ã€‚å°½ç®¡è¿™æ˜¯ä¸€ç§åŸºæœ¬çš„é‡åŒ–æŠ€æœ¯ï¼Œä½†æˆ‘ä»¬ä»ç„¶è·å¾—äº†éå¸¸å¥½çš„ç»“æœï¼è¯·æŸ¥çœ‹ä»¥ä¸‹åŸºå‡†æµ‹è¯•ï¼ˆllama-2-7b åœ¨å›°æƒ‘åº¦æŒ‡æ ‡ä¸Šçš„è¡¨ç°ï¼‰ã€‚ä½ å¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°æ›´å¤šåŸºå‡†æµ‹è¯•ï¼š[è¿™é‡Œ](https://github.com/huggingface/optimum-quanto/tree/main/bench/generation)\n",
    "\n",
    "![llama-2-7b-quanto-perplexity](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/quantization/NousResearch-Llama-2-7b-hf_Perplexity.png)\n",
    "\n",
    "è¯¥åº“è¶³å¤Ÿçµæ´»ï¼Œå¯ä»¥ä¸å¤§å¤šæ•° PTQ ä¼˜åŒ–ç®—æ³•å…¼å®¹ã€‚æœªæ¥è®¡åˆ’ä»¥æœ€æ— ç¼çš„æ–¹å¼é›†æˆæœ€å—æ¬¢è¿çš„ç®—æ³•ï¼ˆAWQ, Smoothquantï¼‰ã€‚"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
