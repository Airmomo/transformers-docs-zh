{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "371a02b4",
   "metadata": {},
   "source": [
    "# 量化技术简介\n",
    "\n",
    "量化技术旨在用更少的信息表示数据，同时尽量不损失太多的准确性。这通常意味着将数据类型转换为用更少位数表示相同信息的形式。例如，如果你的模型权重存储为32位浮点数，并将其量化为16位浮点数，模型大小将减半，这样更容易存储并减少内存使用。较低的精度还可以加速推理，因为用更少的位数进行计算所需的时间更短。\n",
    "\n",
    "想要在Transformers中添加新的量化方法吗？请阅读 [HfQuantizer](./contribute) 指南以了解如何操作！\n",
    "\n",
    "如果你是量化领域的初学者，我们建议你查看这些与DeepLearning.AI合作的适合初学者的量化课程：\n",
    "\n",
    "* [使用Hugging Face的量化基础](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)\n",
    "* [深入量化](https://www.deeplearning.ai/short-courses/quantization-in-depth/)\n",
    "\n",
    "## 何时使用哪种方法？\n",
    "\n",
    "社区已经开发了许多适用于不同场景的量化方法。使用Transformers时，可以根据你的具体需求运行任何已集成的方法，因为每种方法都有其优缺点。\n",
    "\n",
    "例如，一些量化方法需要使用数据集校准模型以实现更准确和“极端”的压缩（最多可压缩到1-2位），而其他方法则可以在不使用数据集的情况下即时量化。\n",
    "\n",
    "另一个需要考虑的参数是与目标设备的兼容性。你想在CPU、GPU还是Apple芯片上进行量化？\n",
    "\n",
    "简而言之，支持广泛的量化方法可以让你选择最适合你特定需求的最佳量化方法。\n",
    "\n",
    "使用下表来帮助你决定使用哪种量化方法。\n",
    "\n",
    "| 量化方法 | 即时量化 | CPU | CUDA GPU | RoCm GPU (AMD) | Metal (Apple Silicon) | 支持torch.compile() | 位数 | 支持微调（通过PEFT） | 可与🤗 transformers序列化 | 🤗 transformers支持 | 库链接 |\n",
    "| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |\n",
    "| [AQLM](./aqlm) | ❌  | ✔️  | ✔️  | ❌  | ❌  | ✔️  | 1 / 2 | ✔️  | ✔️  | ✔️  | https://github.com/Vahe1994/AQLM |\n",
    "| [AWQ](./awq) | ❌  | ❌  | ✔️  | ✔️  | ❌  | ?   | 4   | ✔️  | ✔️  | ✔️  | https://github.com/casper-hansen/AutoAWQ |\n",
    "| [bitsandbytes](./bitsandbytes) | ✔️  | ⚠️ \\* | ✔️  | ⚠️ \\* | ❌ \\*\\* | ❌ (即将支持！) | 4 / 8 | ✔️  | ✔️  | ✔️  | https://github.com/bitsandbytes-foundation/bitsandbytes |\n",
    "| [compressed-tensors](./compressed_tensors) | ❌  | ✔️  | ✔️  | ✔️  | ❌  | ❌  | 1 - 8 | ✔️  | ✔️  | ✔️  | https://github.com/neuralmagic/compressed-tensors |\n",
    "| [EETQ](./eetq) | ✔️  | ❌  | ✔️  | ❌  | ❌  | ?   | 8   | ✔️  | ✔️  | ✔️  | https://github.com/NetEase-FuXi/EETQ |\n",
    "| GGUF / GGML (llama.cpp) | ✔️  | ✔️  | ✔️  | ❌  | ✔️  | ❌  | 1 - 8 | ❌  | [见GGUF部分](../gguf) | [见GGUF部分](../gguf) | https://github.com/ggerganov/llama.cpp |\n",
    "| [GPTQ](./gptq) | ❌  | ❌  | ✔️  | ✔️  | ❌  | ❌  | 2 / 3 / 4 / 8 | ✔️  | ✔️  | ✔️  | https://github.com/AutoGPTQ/AutoGPTQ |\n",
    "| [HQQ](./hqq) | ✔️  | ✔️  | ✔️  | ❌  | ❌  | ✔️  | 1 - 8 | ✔️  | ❌  | ✔️  | https://github.com/mobiusml/hqq/ |\n",
    "| [Quanto](./quanto) | ✔️  | ✔️  | ✔️  | ❌  | ✔️  | ✔️  | 2 / 4 / 8 | ❌  | ❌  | ✔️  | https://github.com/huggingface/quanto |\n",
    "| [FBGEMM_FP8](./fbgemm_fp8.md) | ✔️  | ❌  | ✔️  | ❌  | ❌  | ❌  | 8   | ❌  | ✔️  | ✔️  | https://github.com/pytorch/FBGEMM |\n",
    "| [torchao](./torchao.md) | ✔️  |    | ✔️  | ❌  | 部分支持（仅int4权重） |    | 4 / 8 |    | ✔️❌ | ✔️  | https://github.com/pytorch/ao |\n",
    "\n",
    "\\* bitsandbytes正在重构以支持多个后端，不仅限于CUDA。目前，ROCm（AMD GPU）和Intel CPU的实现已经成熟，Intel XPU正在开发中，预计在今年第四季度或明年第一季度支持Apple Silicon。有关安装说明和最新的后端更新，请访问 [此链接](https://huggingface.co/docs/bitsandbytes/main/en/installation#multi-backend)。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
