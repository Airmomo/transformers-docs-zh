{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04d1b3c7",
   "metadata": {},
   "source": [
    "# EETQ —— 高效的 Tensor 量化方法\n",
    "\n",
    "[EETQ](https://github.com/NetEase-FuXi/EETQ) 是一个支持在 NVIDIA GPU 上进行 int8 每通道权重量化（weight-only quantization）的库。其高性能的 GEMM 和 GEMV 内核来自 FasterTransformer 和 TensorRT-LLM。使用该库时，无需校准数据集，也不需要预先量化模型。此外，由于使用了每通道量化，模型的精度下降几乎可以忽略不计。\n",
    "\n",
    "确保你已经从 [发布页面](https://github.com/NetEase-FuXi/EETQ/releases) 安装了 EETQ。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb8e79b",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install --no-cache-dir https://github.com/NetEase-FuXi/EETQ/releases/download/v1.0.0/EETQ-1.0.0+cu121+torch2.1.2-cp310-cp310-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de5f41ec",
   "metadata": {},
   "source": [
    "\n",
    "或者通过源代码安装：[https://github.com/NetEase-FuXi/EETQ](https://github.com/NetEase-FuXi/EETQ)。EETQ 需要 CUDA 能力 <= 8.9 且 >= 7.0。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f1db35",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "git clone https://github.com/NetEase-FuXi/EETQ.git\n",
    "cd EETQ/\n",
    "git submodule update --init --recursive\n",
    "pip install ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259f246f",
   "metadata": {},
   "source": [
    "\n",
    "未量化的模型可以通过 `from_pretrained` 方法进行量化。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e0ae96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, EetqConfig\n",
    "path = \"/path/to/model\"  # 模型路径\n",
    "quantization_config = EetqConfig(\"int8\")  # 量化配置\n",
    "model = AutoModelForCausalLM.from_pretrained(path, device_map=\"auto\", quantization_config=quantization_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40de40a9",
   "metadata": {},
   "source": [
    "\n",
    "量化的模型可以通过 `save_pretrained` 方法保存，并通过 `from_pretrained` 方法再次加载使用。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800dce17",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_path = \"/path/to/save/quantized/model\"  # 保存的路径\n",
    "model.save_pretrained(quant_path)\n",
    "model = AutoModelForCausalLM.from_pretrained(quant_path, device_map=\"auto\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
