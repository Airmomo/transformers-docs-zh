{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32365710",
   "metadata": {},
   "source": [
    "# HQQ —— 半二次量化\n",
    "\n",
    "半二次量化（Half-Quadratic Quantization, HQQ）通过快速稳健的优化实现了即时量化。它不需要校准数据，并且可以用于量化任何模型。更多详细信息请参阅 [官方包](https://github.com/mobiusml/hqq/)。\n",
    "\n",
    "### 安装\n",
    "\n",
    "我们建议您使用以下方法安装最新版本并构建相应的 CUDA 内核：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7b77ad",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install hqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7841da",
   "metadata": {},
   "source": [
    "\n",
    "### 量化模型\n",
    "\n",
    "要量化一个模型，您需要创建一个 [HqqConfig](/docs/transformers/v4.47.0/en/main_classes/quantization#transformers.HqqConfig)。有以下两种方法：\n",
    "\n",
    "#### 方法 1：所有线性层使用相同的量化配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296c50e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, HqqConfig\n",
    "\n",
    "# 所有线性层使用相同的量化配置\n",
    "quant_config = HqqConfig(nbits=8, group_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305d604d",
   "metadata": {},
   "source": [
    "\n",
    "#### 方法 2：具有相同标签的每个线性层使用专门的量化配置\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1450e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不同线性层使用不同的量化配置\n",
    "q4_config = {'nbits': 4, 'group_size': 64}\n",
    "q3_config = {'nbits': 3, 'group_size': 32}\n",
    "quant_config = HqqConfig(dynamic_config={\n",
    "  'self_attn.q_proj': q4_config,\n",
    "  'self_attn.k_proj': q4_config,\n",
    "  'self_attn.v_proj': q4_config,\n",
    "  'self_attn.o_proj': q4_config,\n",
    "\n",
    "  'mlp.gate_proj': q3_config,\n",
    "  'mlp.up_proj': q3_config,\n",
    "  'mlp.down_proj': q3_config,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8414345b",
   "metadata": {},
   "source": [
    "\n",
    "第二种方法特别适用于量化混合专家模型（Mixture-of-Experts, MoEs），因为专家在较低的量化设置下受影响较小。\n",
    "\n",
    "然后，您可以按以下方式量化模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c54209",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_id, \n",
    "    torch_dtype=torch.float16, \n",
    "    device_map=\"cuda\", \n",
    "    quantization_config=quant_config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67f5006e",
   "metadata": {},
   "source": [
    "\n",
    "## 优化运行时\n",
    "\n",
    "HQQ 支持多种后端，包括纯 PyTorch 和自定义反量化 CUDA 内核。这些后端适合旧 GPU 和 peft/QLoRA 训练。为了更快的推理，HQQ 支持 4 位融合内核（TorchAO 和 Marlin），在单个 4090 GPU 上可以达到每秒 200 个 token 的速度。有关如何使用这些后端的更多详细信息，请参阅 [HQQ 文档](https://github.com/mobiusml/hqq/?tab=readme-ov-file#backend)。"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
