{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71e076ec",
   "metadata": {},
   "source": [
    "# 故障排除\n",
    "\n",
    "有时会出现错误，但我们在这里帮助您！本指南涵盖了一些最常见的问题及其解决方法。然而，这并不是一个涵盖所有 🤗 Transformers 问题的详尽清单。如果您需要更多帮助，可以尝试以下方法：\n",
    "\n",
    "1. 在 [论坛](https://discuss.huggingface.co/)上寻求帮助。您可以将问题发布到特定的类别中，例如 [初学者](https://discuss.huggingface.co/c/beginners/5) 或 [🤗 Transformers](https://discuss.huggingface.co/c/transformers/9)。确保您的帖子描述清楚并包含可复现的代码，以提高解决问题的可能性！\n",
    "\n",
    "2. 如果是与库相关的bug，请在 🤗 Transformers 仓库中创建一个 [问题](https://github.com/huggingface/transformers/issues/new/choose)。尽量提供尽可能多的信息来描述bug，以便我们更好地找出问题所在并解决它。\n",
    "\n",
    "3. 如果您使用的是较旧版本的 🤗 Transformers，请查看 [迁移指南](migration)，因为不同版本之间引入了一些重要的更改。\n",
    "\n",
    "有关故障排除和获取帮助的更多详细信息，请参阅 Hugging Face 课程的 [第8章](https://huggingface.co/course/chapter8/1?fw=pt)。\n",
    "\n",
    "## 网络隔离环境\n",
    "\n",
    "某些云上的GPU实例或内部网络设置可能被防火墙隔离，导致连接错误。当您的脚本尝试下载模型权重或数据集时，下载会挂起，然后超时，并显示以下消息：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bc7f4fd9",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ValueError: 连接错误，我们在缓存路径中找不到请求的文件。\n",
    "请重试或确保您的互联网连接已打开。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87db9dac",
   "metadata": {},
   "source": [
    "\n",
    "在这种情况下，您应该尝试在[离线模式](installation#offline-mode)下运行 🤗 Transformers 以避免连接错误。\n",
    "\n",
    "## CUDA 内存不足\n",
    "\n",
    "没有适当的硬件，训练具有数百万参数的大模型可能会很有挑战性。当GPU内存不足时，您可能会遇到以下常见错误：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a39aea6f",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "CUDA 内存不足。尝试分配 256.00 MiB（GPU 0；总容量 11.17 GiB；已分配 9.70 GiB；空闲 179.81 MiB；PyTorch 总共预留 9.85 GiB）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac58a4c8",
   "metadata": {},
   "source": [
    "\n",
    "以下是一些减少内存使用的潜在解决方案：\n",
    "\n",
    "* 减少 [TrainingArguments](/docs/transformers/v4.46.3/en/main_classes/trainer#transformers.TrainingArguments) 中的 `per_device_train_batch_size` 值。\n",
    "* 尝试在 [TrainingArguments](/docs/transformers/v4.46.3/en/main_classes/trainer#transformers.TrainingArguments) 中使用 `gradient_accumulation_steps` 来有效增加整体批次大小。\n",
    "\n",
    "有关节省内存技巧的更多详细信息，请参阅 [性能指南](performance)。\n",
    "\n",
    "## 无法加载保存的 TensorFlow 模型\n",
    "\n",
    "TensorFlow 的 [model.save](https://www.tensorflow.org/tutorials/keras/save_and_load#save_the_entire_model) 方法会将整个模型（包括架构、权重和训练配置）保存在一个文件中。但是，当您再次加载模型文件时，可能会遇到错误，因为 🤗 Transformers 可能无法加载模型文件中的所有 TensorFlow 相关对象。为了避免保存和加载 TensorFlow 模型时出现问题，我们建议您：\n",
    "\n",
    "* 使用 `model.save_weights` 将模型权重保存为 `.h5` 文件扩展名，然后使用 `from_pretrained()` 重新加载模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c5d3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFPreTrainedModel\n",
    "from tensorflow import keras\n",
    "\n",
    "model.save_weights(\"some_folder/tf_model.h5\")\n",
    "model = TFPreTrainedModel.from_pretrained(\"some_folder\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ded3d",
   "metadata": {},
   "source": [
    "\n",
    "* 使用 `~TFPretrainedModel.save_pretrained` 保存模型，并使用 `from_pretrained()` 重新加载模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594dbf52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFPreTrainedModel\n",
    "\n",
    "model.save_pretrained(\"path_to/model\")\n",
    "model = TFPreTrainedModel.from_pretrained(\"path_to/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e47047",
   "metadata": {},
   "source": [
    "\n",
    "## 导入错误\n",
    "\n",
    "您可能会遇到的另一个常见错误是 `ImportError`，尤其是对于新发布的模型：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fc1ad7ce",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "ImportError: 无法从 'transformers' 中导入名称 'ImageGPTImageProcessor'（未知位置）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e54c73",
   "metadata": {},
   "source": [
    "\n",
    "对于这些类型的错误，请确保您安装了最新版本的 🤗 Transformers 以访问最新的模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbdab1cb",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0f6d8e",
   "metadata": {},
   "source": [
    "\n",
    "## CUDA 错误：设备端断言触发\n",
    "\n",
    "有时您可能会遇到一个通用的 CUDA 错误，提示设备代码中有错误：\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e71ef02e",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "RuntimeError: CUDA 错误：设备端断言触发"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23aa4b0",
   "metadata": {},
   "source": [
    "\n",
    "您应该首先在 CPU 上运行代码以获取更详细的错误消息。在代码开头添加以下环境变量以切换到 CPU：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a702fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7aeb2c",
   "metadata": {},
   "source": [
    "\n",
    "另一种选择是获取更好的 GPU 回溯信息。在代码开头添加以下环境变量以获取指向错误源的回溯：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecfdae7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b829af4",
   "metadata": {},
   "source": [
    "\n",
    "## 当填充标记未被屏蔽时输出不正确\n",
    "\n",
    "在某些情况下，如果 `input_ids` 包含填充标记，输出 `hidden_state` 可能不正确。为了演示这一点，加载一个模型和分词器。您可以访问模型的 `pad_token_id` 来查看其值。对于某些模型，`pad_token_id` 可能为 `None`，但您可以始终手动设置它。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319ed11",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-uncased\")\n",
    "model.config.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c7ed9b8",
   "metadata": {},
   "source": [
    "\n",
    "以下示例展示了未屏蔽填充标记时的输出：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27103833",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[7592, 2057, 2097, 2393, 9611, 2115], [7592, 0, 0, 0, 0, 0]])\n",
    "output = model(input_ids)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ebb4f4",
   "metadata": {},
   "source": [
    "\n",
    "以下是第二个序列的实际输出：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f7a018",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.tensor([[7592]])\n",
    "output = model(input_ids)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5fa426e",
   "metadata": {},
   "source": [
    "\n",
    "大多数情况下，您应该向模型提供 `attention_mask` 以忽略填充标记，以避免这种无声错误。现在，第二个序列的输出与其实际输出匹配：\n",
    "\n",
    "默认情况下，分词器会根据您使用的特定分词器的默认设置为您创建 `attention_mask`。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd85f584",
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = torch.tensor([[1, 1, 1, 1, 1, 1], [1, 0, 0, 0, 0, 0]])\n",
    "output = model(input_ids, attention_mask=attention_mask)\n",
    "print(output.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435f378c",
   "metadata": {},
   "source": [
    "\n",
    "🤗 Transformers 不会在提供填充标记时自动创建 `attention_mask` 以屏蔽填充标记，因为：\n",
    "\n",
    "* 有些模型没有填充标记。\n",
    "* 对于某些用途，用户希望模型关注填充标记。\n",
    "\n",
    "## ValueError: 无法识别此种类的 AutoModel 的配置类 XYZ\n",
    "\n",
    "通常，我们建议使用 [AutoModel](/docs/transformers/v4.46.3/en/model_doc/auto#transformers.AutoModel) 类来加载预训练模型。这个类可以根据给定检查点的配置自动推断并加载正确的架构。如果您在从检查点加载模型时看到此 `ValueError`，这意味着 Auto 类无法将给定检查点中的配置映射到您尝试加载的模型类型。最常见的原因是检查点不支持给定任务。例如，您会在以下示例中看到此错误，因为 GPT2 没有用于问答的任务模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3d82f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor, AutoModelForQuestionAnswering\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai-community/gpt2-medium\")\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"openai-community/gpt2-medium\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
