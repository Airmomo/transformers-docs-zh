{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7245d701",
   "metadata": {},
   "source": [
    "# 社区资源\n",
    "\n",
    "本页面汇总了社区中关于 🤗 Transformers 的相关资源。\n",
    "\n",
    "## 社区资源\n",
    "\n",
    "| 资源 | 描述 | 作者 |\n",
    "| :--- | :--- | ---: |\n",
    "| [Hugging Face Transformers 术语表闪卡](https://www.darigovresearch.com/huggingface-transformers-glossary-flashcards) | 基于 [Transformers 文档术语表](glossary)，通过 Anki 这个开源跨平台应用程序以易于学习/复习的形式制作的闪卡。查看这个 [介绍视频了解如何使用闪卡](https://www.youtube.com/watch?v=Dji_h7PILrw) | [Darigov Research](https://www.darigovresearch.com/) |\n",
    "\n",
    "## 社区笔记本\n",
    "\n",
    "| 笔记本 | 描述 | 作者 |     |\n",
    "| :--- | :--- | :--- | ---: |\n",
    "| [微调预训练 Transformer 生成歌词](https://github.com/AlekseyKorshuk/huggingartists) | 如何通过微调 GPT-2 模型来生成你喜欢的艺术家风格的歌词 | [Aleksey Korshuk](https://github.com/AlekseyKorshuk) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AlekseyKorshuk/huggingartists/blob/master/huggingartists-demo.ipynb) |\n",
    "| [在 Tensorflow 2 中训练 T5](https://github.com/snapthat/TF-T5-text-to-text) | 如何使用 Tensorflow 2 训练 T5 模型。本笔记本展示了使用 SQUAD 数据集实现的问答任务 | [Muhammad Harris](https://github.com/HarrisDePerceptron) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/snapthat/TF-T5-text-to-text/blob/master/snapthatT5/notebooks/TF-T5-Datasets%20Training.ipynb) |\n",
    "| [在 TPU 上训练 T5](https://github.com/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb) | 如何使用 Transformers 和 Nlp 在 SQUAD 数据集上训练 T5 模型 | [Suraj Patil](https://github.com/patil-suraj) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/T5_on_TPU.ipynb#scrollTo=QLGiFCDqvuil) |\n",
    "| [微调 T5 用于分类和多选任务](https://github.com/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) | 如何使用 PyTorch Lightning 以文本到文本格式微调 T5 模型，用于分类和多选任务 | [Suraj Patil](https://github.com/patil-suraj) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/exploring-T5/blob/master/t5_fine_tuning.ipynb) |\n",
    "| [在新数据集和语言上微调 DialoGPT](https://github.com/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) | 如何在新数据集上微调 DialoGPT 模型，用于开放式对话聊天机器人 | [Nathan Cooper](https://github.com/ncoop57) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ncoop57/i-am-a-nerd/blob/master/_notebooks/2020-05-12-chatbot-part-1.ipynb) |\n",
    "| [使用 Reformer 处理长序列建模](https://github.com/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) | 如何使用 Reformer 训练长达 500,000 个标记的序列 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/PyTorch_Reformer.ipynb) |\n",
    "| [微调 BART 用于摘要](https://github.com/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) | 如何使用 fastai 和 blurr 微调 BART 模型，用于摘要 | [Wayde Gilliam](https://ohmeow.com/) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ohmeow/ohmeow_website/blob/master/posts/2021-05-25-mbart-sequence-classification-with-blurr.ipynb) |\n",
    "| [微调预训练 Transformer 生成推文](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) | 如何通过微调 GPT-2 模型生成你喜欢的 Twitter 账户风格的推文 | [Boris Dayma](https://github.com/borisdayma) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/borisdayma/huggingtweets/blob/master/huggingtweets-demo.ipynb) |\n",
    "| [使用 Weights & Biases 优化 🤗 Hugging Face 模型](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) | 完整教程展示如何将 W&B 集成到 Hugging Face 中 | [Boris Dayma](https://github.com/borisdayma) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/wandb/examples/blob/master/colabs/huggingface/Optimize_Hugging_Face_models_with_Weights_%26_Biases.ipynb) |\n",
    "| [构建 Longformer 预训练模型](https://github.com/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) | 如何构建现有预训练模型的“长”版本 | [Iz Beltagy](https://beltagy.net) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/allenai/longformer/blob/master/scripts/convert_model_to_long.ipynb) |\n",
    "| [微调 Longformer 用于问答任务](https://github.com/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) | 如何微调 longformer 模型用于问答任务 | [Suraj Patil](https://github.com/patil-suraj) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patil-suraj/Notebooks/blob/master/longformer_qa_training.ipynb) |\n",
    "| [使用 🤗nlp 评估模型](https://github.com/patrickvonplaten/notebooks/blob/master/How_to_evaluate_Longformer_on_TriviaQA_using_NLP.ipynb) | 如何使用 `nlp` 评估 longformer 在 TriviaQA 上的性能 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1m7eTGlPmLRgoPkkA7rkhQdZ9ydpmsdLE?usp=sharing) |\n",
    "| [微调 T5 用于情感跨度提取](https://github.com/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) | 如何使用 PyTorch Lightning 以文本到文本格式微调 T5 模型，用于情感跨度提取 | [Lorenzo Ampil](https://github.com/enzoampil) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/enzoampil/t5-intro/blob/master/t5_qa_training_pytorch_span_extraction.ipynb) |\n",
    "| [微调 DistilBert 用于多类别分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) | 如何使用 PyTorch 微调 DistilBert 模型，用于多类别分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multiclass_classification.ipynb) |\n",
    "| [微调 BERT 用于多标签分类](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb) | 如何使用 PyTorch 微调 BERT 模型，用于多标签分类 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_multi_label_classification.ipynb) |\n",
    "| [微调 T5 用于摘要](https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) | 如何使用 PyTorch 微调 T5 模型用于摘要，并使用 WandB 跟踪实验 | [Abhishek Kumar Mishra](https://github.com/abhimishra91) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb) |\n",
    "| [使用动态填充/分桶加速 Transformer 微调](https://github.com/ELS-RD/transformers-notebook/blob/master/Divide_Hugging_Face_Transformers_training_time_by_2_or_more.ipynb) | 如何通过动态填充/分桶将微调速度提高两倍 | [Michael Benesty](https://github.com/pommedeterresautee) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1CBfRU1zbfu7-ijiOqAAQUA-RJaxfcJoO?usp=sharing) |\n",
    "| [使用双向自注意力层预训练 Reformer](https://github.com/patrickvonplaten/notebooks/blob/master/Reformer_For_Masked_LM.ipynb) | 如何训练具有双向自注意力层的 Reformer 模型 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1tzzh0i8PgDQGV3SMFUGxM7_gGae3K-uW?usp=sharing) |\n",
    "| [扩展和微调 Sci-BERT](https://github.com/lordtt13/word-embeddings/blob/master/COVID-19%20Research%20Data/COVID-SciBERT.ipynb) | 如何在 CORD 数据集上扩展 AllenAI 预训练的 SciBERT 模型并将其接入管道 | [Tanmay Thakur](https://github.com/lordtt13) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1rqAR40goxbAfez1xvF3hBJphSCsvXmh8) |\n",
    "| [使用 Trainer API 微调 BlenderBotSmall 用于摘要](https://github.com/lordtt13/transformers-experiments/blob/master/Custom%20Tasks/fine-tune-blenderbot_small-for-summarization.ipynb) | 如何在自定义数据集上使用 Trainer API 微调 BlenderBotSmall 用于摘要 | [Tanmay Thakur](https://github.com/lordtt13) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/19Wmupuls7mykSGyRN_Qo6lPQhgp56ymq?usp=sharing) |\n",
    "| [微调 Electra 并使用集成梯度解释预测](https://github.com/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) | 如何微调 Electra 模型用于情感分析，并使用 Captum 集成梯度解释预测 | [Eliza Szczechla](https://elsanns.github.io) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/electra_fine_tune_interpret_captum_ig.ipynb) |\n",
    "| [使用 Trainer 类微调非英语 GPT-2 模型](https://github.com/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) | 如何使用 Trainer 类微调非英语 GPT-2 模型 | [Philipp Schmid](https://www.philschmid.de) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/philschmid/fine-tune-GPT-2/blob/master/Fine_tune_a_non_English_GPT_2_Model_with_Huggingface.ipynb) |\n",
    "| [微调 DistilBERT 用于多标签分类任务](https://github.com/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) | 如何微调 DistilBERT 模型用于多标签分类任务 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/Transformers_scripts/blob/master/Transformers_multilabel_distilbert.ipynb) |\n",
    "| [微调 ALBERT 用于句子对分类](https://github.com/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) | 如何微调 ALBERT 模型或基于 BERT 的其他模型用于句子对分类任务 | [Nadir El Manouzi](https://github.com/NadirEM) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NadirEM/nlp-notebooks/blob/master/Fine_tune_ALBERT_sentence_pair_classification.ipynb) |\n",
    "| [微调 Roberta 用于情感分析](https://github.com/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) | 如何微调 Roberta 模型用于情感分析 | [Dhaval Taunk](https://github.com/DhavalTaunk08) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/DhavalTaunk08/NLP_scripts/blob/master/sentiment_analysis_using_roberta.ipynb) |\n",
    "| [评估生成的问题模型](https://github.com/flexudy-pipe/qugeev) | 你的 seq2seq 变换器模型生成的问题答案有多准确？ | [Pascal Zoleko](https://github.com/zolekode) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1bpsSqCQU-iw_5nNoRm_crPq6FRuJthq_?usp=sharing) |\n",
    "| [使用 DistilBERT 和 Tensorflow 分类文本](https://github.com/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) | 如何使用 TensorFlow 微调 DistilBERT 模型用于文本分类 | [Peter Bayerle](https://github.com/peterbayerle) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/peterbayerle/huggingface_notebook/blob/main/distilbert_tf.ipynb) |\n",
    "| [使用 BERT 进行编码器-解码器摘要（CNN/Dailymail）](https://github.com/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) | 如何使用 _google-bert/bert-base-uncased_ 检查点预热 _EncoderDecoderModel_，用于 CNN/Dailymail 的摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/BERT2BERT_for_CNN_Dailymail.ipynb) |\n",
    "| [使用 RoBERTa 进行编码器-解码器摘要（BBC XSum）](https://github.com/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) | 如何使用 _FacebookAI/roberta-base_ 检查点预热共享 _EncoderDecoderModel_，用于 BBC/XSum 的摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/RoBERTaShared_for_BBC_XSum.ipynb) |\n",
    "| [微调 TAPAS 用于顺序问答（SQA）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) | 如何使用 _tapas-base_ 检查点微调 _TapasForQuestionAnswering_，用于顺序问答（SQA）数据集 | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Fine_tuning_TapasForQuestionAnswering_on_SQA.ipynb) |\n",
    "| [评估 TAPAS 用于表格事实检查（TabFact）](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) | 如何使用 🤗 datasets 和 🤗 transformers 库组合评估使用 _tapas-base-finetuned-tabfact_ 检查点微调的 _TapasForSequenceClassification_ | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/TAPAS/Evaluating_TAPAS_on_the_Tabfact_test_set.ipynb) |\n",
    "| [微调 mBART 用于翻译](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) | 如何使用 Seq2SeqTrainer 微调 mBART 用于印地语到英语的翻译 | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/huggingface-tutorials/blob/main/translation_training.ipynb) |\n",
    "| [微调 LayoutLM 用于表格理解数据集 FUNSD](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) | 如何在 FUNSD 数据集上微调 _LayoutLMForTokenClassification_，用于从扫描文档中提取信息 | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForTokenClassification_on_FUNSD.ipynb) |\n",
    "| [微调 DistilGPT2 并生成文本](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) | 如何微调 DistilGPT2 并生成文本 | [Aakash Tripathi](https://github.com/tripathiaakash) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tripathiaakash/DistilGPT2-Tutorial/blob/main/distilgpt2_fine_tuning.ipynb) |\n",
    "| [微调 LED 用于长达 8K 个标记的摘要](https://github.com/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_%28LED%29_for_Summarization_on_pubmed.ipynb) | 如何在 pubmed 数据集上微调 LED 用于长距离摘要 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Fine_tune_Longformer_Encoder_Decoder_%28LED%29_for_Summarization_on_pubmed.ipynb) |\n",
    "| [在 Arxiv 上评估 LED](https://github.com/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) | 如何有效地在长距离摘要上评估 LED | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/LED_on_Arxiv.ipynb) |\n",
    "| [微调 LayoutLM 用于文档图像分类数据集 RVL-CDIP](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) | 如何在 RVL-CDIP 数据集上微调 _LayoutLMForSequenceClassification_，用于扫描文档分类 | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/LayoutLM/Fine_tuning_LayoutLMForSequenceClassification_on_RVL_CDIP.ipynb) |\n",
    "| [使用 GPT2 调整 Wav2Vec2 CTC 解码](https://github.com/voidful/huggingface_notebook/blob/main/xlsr_gpt.ipynb) | 如何使用语言模型调整 CTC 序列解码 | [Eric Lam](https://github.com/voidful) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1e_z5jQHYbO2YKEaUgzb1ww1WwiAyydAj?usp=sharing) |\n",
    "| [使用 Trainer 类微调 BART 用于双语摘要](https://github.com/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) | 如何使用 Trainer 类微调 BART 用于双语摘要 | [Eliza Szczechla](https://github.com/elsanns) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/elsanns/xai-nlp-notebooks/blob/master/fine_tune_bart_summarization_two_langs.ipynb) |\n",
    "| [在 Trivia QA 上评估 Big Bird](https://github.com/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) | 如何在 Trivia QA 上评估 BigBird 模型的长文档问答 | [Patrick von Platen](https://github.com/patrickvonplaten) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/patrickvonplaten/notebooks/blob/master/Evaluating_Big_Bird_on_TriviaQA.ipynb) |\n",
    "| [使用 Wav2Vec2 创建视频字幕](https://github.com/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) | 如何通过转录音频使用 Wav2Vec 从任何视频创建 YouTube 字幕 | [Niklas Muennighoff](https://github.com/Muennighoff) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Muennighoff/ytclipcc/blob/main/wav2vec_youtube_captions.ipynb) |\n",
    "| [使用 PyTorch Lightning 微调 Vision Transformer 用于 CIFAR-10](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) | 如何使用 HuggingFace Transformers、Datasets 和 PyTorch Lightning 微调 Vision Transformer（ViT）用于 CIFAR-10 | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_PyTorch_Lightning.ipynb) |\n",
    "| [使用 🤗 Trainer 微调 Vision Transformer 用于 CIFAR-10](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) | 如何使用 HuggingFace Transformers、Datasets 和 🤗 Trainer 微调 Vision Transformer（ViT）用于 CIFAR-10 | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/VisionTransformer/Fine_tuning_the_Vision_Transformer_on_CIFAR_10_with_the_%F0%9F%A4%97_Trainer.ipynb) |\n",
    "| [在 Open Entity 数据集上评估 LUKE](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) | 如何在 Open Entity 数据集上评估 _LukeForEntityClassification_ | [Ikuya Yamada](https://github.com/ikuyamada) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_open_entity.ipynb) |\n",
    "| [在 TACRED 数据集上评估 LUKE](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) | 如何在 TACRED 数据集上评估 _LukeForEntityPairClassification_ | [Ikuya Yamada](https://github.com/ikuyamada) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_tacred.ipynb) |\n",
    "| [在 CoNLL-2003 数据集上评估 LUKE](https://github.com/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) | 如何在 CoNLL-2003 数据集上评估 _LukeForEntitySpanClassification_ | [Ikuya Yamada](https://github.com/ikuyamada) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/studio-ousia/luke/blob/master/notebooks/huggingface_conll_2003.ipynb) |\n",
    "| [在 PubMed 数据集上评估 BigBird-Pegasus](https://github.com/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) | 如何在 PubMed 数据集上评估 _BigBirdPegasusForConditionalGeneration_ | [Vasudev Gupta](https://github.com/vasudevgupta7) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/vasudevgupta7/bigbird/blob/main/notebooks/bigbird_pegasus_evaluation.ipynb) |\n",
    "| [使用 Wav2Vec2 进行语音情感分类](https://github.com/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) | 如何使用预训练的 Wav2Vec2 模型在 MEGA 数据集上进行情感分类 | [Mehrdad Farahani](https://github.com/m3hrdadfi) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/m3hrdadfi/soxan/blob/main/notebooks/Emotion_recognition_in_Greek_speech_using_Wav2Vec2.ipynb) |\n",
    "| [使用 DETR 检测图像中的对象](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_%28with_DetrFeatureExtractor%29.ipynb) | 如何使用训练好的 _DetrForObjectDetection_ 模型检测图像中的对象并可视化注意力 | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/DETR_minimal_example_%28with_DetrFeatureExtractor%29.ipynb) |\n",
    "| [在自定义目标检测数据集上微调 DETR](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_%28balloon%29.ipynb) | 如何在自定义目标检测数据集上微调 _DetrForObjectDetection_ | [Niels Rogge](https://github.com/nielsrogge) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/NielsRogge/Transformers-Tutorials/blob/master/DETR/Fine_tuning_DetrForObjectDetection_on_custom_dataset_%28balloon%29.ipynb) |\n",
    "| [微调 T5 用于命名实体识别](https://github.com/ToluClassics/Notebooks/blob/main/T5_Ner_Finetuning.ipynb) | 如何在命名实体识别任务上微调 _T5_ | [Ogundepo Odunayo](https://github.com/ToluClassics) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1obr78FY_cBmWY5ODViCmzdY6O1KB65Vc?usp=sharing) |\n",
    "| [使用 QLoRA 和 PEFT 微调开源大模型](https://github.com/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) | 如何使用 [QLoRA](https://github.com/artidoro/qlora) 和 [PEFT](https://huggingface.co/docs/peft/en/index) 以节省内存的方式微调大模型，同时使用 [MLflow](https://mlflow.org/docs/latest/llms/transformers/index.html) 管理实验跟踪 | [Yuki Watanabe](https://github.com/B-Step62) | [![在 Colab 中打开](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mlflow/mlflow/blob/master/docs/source/llms/transformers/tutorials/fine-tuning/transformers-peft.ipynb) |"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
