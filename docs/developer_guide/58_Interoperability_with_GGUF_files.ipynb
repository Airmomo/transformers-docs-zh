{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed8e0f1f",
   "metadata": {},
   "source": [
    "# GGUF 和 Transformer 的交互\n",
    "\n",
    "GGUF 文件格式用于存储与 [GGML](https://github.com/ggerganov/ggml) 及其依赖库（如非常流行的 [llama.cpp](https://github.com/ggerganov/llama.cpp) 或 [whisper.cpp](https://github.com/ggerganov/whisper.cpp)）一起进行推理的模型。\n",
    "\n",
    "这是一种由 [Hugging Face Hub 支持的文件格式](https://huggingface.co/docs/hub/en/gguf)，具有快速检查文件中张量和元数据的功能。\n",
    "\n",
    "这种文件格式设计为“单文件格式”，通常一个文件中包含配置属性、分词器词汇表和其他属性，以及要加载到模型中的所有张量。这些文件根据文件的量化类型有不同的格式。我们在这里简要介绍一些常见的格式：[量化类型](https://huggingface.co/docs/hub/en/gguf#quantization-types)。\n",
    "\n",
    "## Transformer 中的支持\n",
    "\n",
    "我们在 `transformers` 中添加了加载 `gguf` 文件的能力，以提供对 gguf 模型进一步训练或微调的能力，并在完成后再将这些模型转换回 `gguf` 格式，以便在 `ggml` 生态系统中使用。加载模型时，我们会先将其去量化为 fp32，然后再加载权重以供 PyTorch 使用。\n",
    "\n",
    "> **注意**：当前的支持仍处于探索性阶段，我们欢迎贡献以巩固不同量化类型和模型架构的支持。\n",
    "\n",
    "目前支持的模型架构和量化类型如下：\n",
    "\n",
    "### 支持的量化类型\n",
    "\n",
    "初始支持的量化类型是根据在 Hub 上共享的流行量化文件决定的。\n",
    "\n",
    "- F32\n",
    "- F16\n",
    "- BF16\n",
    "- Q4_0\n",
    "- Q4_1\n",
    "- Q5_0\n",
    "- Q5_1\n",
    "- Q8_0\n",
    "- Q2_K\n",
    "- Q3_K\n",
    "- Q4_K\n",
    "- Q5_K\n",
    "- Q6_K\n",
    "- IQ1_S\n",
    "- IQ1_M\n",
    "- IQ2_XXS\n",
    "- IQ2_XS\n",
    "- IQ2_S\n",
    "- IQ3_XXS\n",
    "- IQ3_S\n",
    "- IQ4_XS\n",
    "- IQ4_NL\n",
    "\n",
    "> **注意**：为了支持 gguf 去量化，需要安装 `gguf>=0.10.0`。\n",
    "\n",
    "### 支持的模型架构\n",
    "\n",
    "目前支持的模型架构是在 Hub 上非常流行的架构，包括：\n",
    "\n",
    "- LLaMa\n",
    "- Mistral\n",
    "- Qwen2\n",
    "- Qwen2Moe\n",
    "- Phi3\n",
    "- Bloom\n",
    "- Falcon\n",
    "- StableLM\n",
    "- GPT2\n",
    "- Starcoder2\n",
    "\n",
    "## 示例用法\n",
    "\n",
    "要在 `transformers` 中加载 `gguf` 文件，您需要在 `from_pretrained` 方法中指定 `gguf_file` 参数，用于加载分词器和模型。以下是如何加载分词器和模型的示例，它们可以从同一个文件中加载：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a500970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF\"\n",
    "filename = \"tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, gguf_file=filename)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, gguf_file=filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469226b4",
   "metadata": {},
   "source": [
    "\n",
    "现在，您可以在 PyTorch 生态系统中访问完整的去量化模型，并结合其他多种工具使用。\n",
    "\n",
    "为了将模型重新转换为 `gguf` 文件，我们推荐使用 llama.cpp 中的 [`convert-hf-to-gguf.py` 脚本](https://github.com/ggerganov/llama.cpp/blob/master/convert_hf_to_gguf.py)。\n",
    "\n",
    "以下是完成上述脚本以保存模型并将其导出为 `gguf` 文件的方法：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbef2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save_pretrained('directory')\n",
    "model.save_pretrained('directory')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7cee79c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ebb2f2",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!python ${path_to_llama_cpp}/convert-hf-to-gguf.py ${directory}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
