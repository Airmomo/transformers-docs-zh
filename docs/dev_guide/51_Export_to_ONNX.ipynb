{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "361f05de",
   "metadata": {},
   "source": [
    "# 将模型导出为 ONNX 格式\n",
    "\n",
    "在生产环境中部署 🤗 Transformers 模型时，通常需要将模型导出为一种序列化格式，以便在专用运行时和硬件上加载和执行。\n",
    "\n",
    "🤗 Optimum 是 Transformers 的一个扩展，通过其 `exporters` 模块，可以将模型从 PyTorch 或 TensorFlow 导出为序列化格式，如 ONNX 和 TFLite。🤗 Optimum 还提供了一组性能优化工具，以在目标硬件上以最大效率训练和运行模型。\n",
    "\n",
    "本指南演示了如何使用 🤗 Optimum 将 🤗 Transformers 模型导出为 ONNX 格式。有关将模型导出为 TFLite 的指南，请参阅 [导出到 TFLite 页面](tflite)。\n",
    "\n",
    "## 导出到 ONNX\n",
    "\n",
    "[ONNX（开放神经网络交换）](http://onnx.ai) 是一个开放标准，定义了一组通用的操作符和一个通用的文件格式，用于在各种框架（包括 PyTorch 和 TensorFlow）中表示深度学习模型。当模型导出为 ONNX 格式时，这些操作符用于构建一个计算图（通常称为中间表示），该图表示数据在神经网络中的流动。\n",
    "\n",
    "通过公开具有标准化操作符和数据类型的图，ONNX 使得在框架之间切换变得容易。例如，在 PyTorch 中训练的模型可以导出为 ONNX 格式，然后在 TensorFlow 中导入（反之亦然）。\n",
    "\n",
    "一旦导出为 ONNX 格式，模型可以：\n",
    "\n",
    "* 通过[图优化](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/optimization)和[量化](https://huggingface.co/docs/optimum/onnxruntime/usage_guides/quantization)等技术进行推理优化。\n",
    "* 通过 [`ORTModelForXXX` 类](https://huggingface.co/docs/optimum/onnxruntime/package_reference/modeling_ort) 使用 ONNX Runtime 运行，这些类遵循与 🤗 Transformers 中相同的 `AutoModel` API。\n",
    "* 通过[优化推理管道](https://huggingface.co/docs/optimum/main/en/onnxruntime/usage_guides/pipelines)运行，其 API 与 🤗 Transformers 中的 [pipeline()](/docs/transformers/v4.46.3/en/main_classes/pipelines#transformers.pipeline) 函数相同。\n",
    "\n",
    "🤗 Optimum 通过利用配置对象来支持 ONNX 导出。这些配置对象为多种模型架构提供了现成的支持，并且设计为易于扩展到其他架构。\n",
    "\n",
    "有关现成配置的列表，请参阅 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/onnx/overview)。\n",
    "\n",
    "有两种方法可以将 🤗 Transformers 模型导出为 ONNX，这里我们展示两种方法：\n",
    "\n",
    "* 通过 CLI 使用 🤗 Optimum 导出。\n",
    "* 通过 `optimum.onnxruntime` 使用 🤗 Optimum 导出。\n",
    "\n",
    "### 通过 CLI 导出 🤗 Transformers 模型到 ONNX\n",
    "\n",
    "要将 🤗 Transformers 模型导出为 ONNX，首先安装一个额外的依赖项：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb57098",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install optimum[exporters]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3725ec4b",
   "metadata": {},
   "source": [
    "\n",
    "要查看所有可用参数，请参阅 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/export_a_model#exporting-a-model-to-onnx-using-the-cli)，或在命令行中查看帮助：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09556d42",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "optimum-cli export onnx --help"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6d1c89",
   "metadata": {},
   "source": [
    "\n",
    "要从 🤗 Hub 导出模型的检查点，例如 `distilbert/distilbert-base-uncased-distilled-squad`，运行以下命令：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b1fda4",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squad distilbert_base_uncased_squad_onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba08754",
   "metadata": {},
   "source": [
    "\n",
    "你应该会看到日志指示进度，并显示保存的 `model.onnx` 文件的位置，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f53e3f6",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n",
    "    -[✓] ONNX model output names match reference model (start_logits, end_logits)\n",
    "    - Validating ONNX Model output \"start_logits\":\n",
    "        -[✓] (2, 16) matches (2, 16)\n",
    "        -[✓] all values close (atol: 0.0001)\n",
    "    - Validating ONNX Model output \"end_logits\":\n",
    "        -[✓] (2, 16) matches (2, 16)\n",
    "        -[✓] all values close (atol: 0.0001)\n",
    "The ONNX export succeeded and the exported model was saved at: distilbert_base_uncased_squad_onnx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a848cfd2",
   "metadata": {},
   "source": [
    "\n",
    "上面的示例说明了从 🤗 Hub 导出检查点。当导出本地模型时，首先确保你将模型的权重和分词器文件保存在同一目录（`local_path`）中。使用 CLI 时，将 `local_path` 传递给 `model` 参数，而不是 🤗 Hub 上的检查点名称，并提供 `--task` 参数。你可以在 [🤗 Optimum 文档](https://huggingface.co/docs/optimum/exporters/task_manager) 中查看支持的任务列表。如果未提供 `task` 参数，它将默认为模型架构，而不带任何任务特定的头部。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73459331",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "optimum-cli export onnx --model local_path --task question-answering distilbert_base_uncased_squad_onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3109d2b6",
   "metadata": {},
   "source": [
    "\n",
    "生成的 `model.onnx` 文件可以在支持 ONNX 标准的[许多加速器](https://onnx.ai/supported-tools.html#deployModel)上运行。例如，我们可以使用 [ONNX Runtime](https://onnxruntime.ai/) 加载和运行模型，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a3e97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
    "model = ORTModelForQuestionAnswering.from_pretrained(\"distilbert_base_uncased_squad_onnx\")\n",
    "inputs = tokenizer(\"What am I using?\", \"Using DistilBERT with ONNX Runtime!\", return_tensors=\"pt\")\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065c7e44",
   "metadata": {},
   "source": [
    "\n",
    "对于 Hub 上的 TensorFlow 检查点，过程是相同的。例如，这里是如何从 [Keras 组织](https://huggingface.co/keras-io) 导出纯 TensorFlow 检查点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abbeb3a",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "optimum-cli export onnx --model keras-io/transformers-qa distilbert_base_cased_squad_onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24911c64",
   "metadata": {},
   "source": [
    "\n",
    "### 通过 optimum.onnxruntime 导出 🤗 Transformers 模型到 ONNX\n",
    "\n",
    "除了 CLI，你还可以通过编程方式将 🤗 Transformers 模型导出为 ONNX，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8d529bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from optimum.onnxruntime import ORTModelForSequenceClassification\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilbert_base_uncased_squad\"\n",
    "save_directory = \"onnx/\"\n",
    "\n",
    "# 从 transformers 加载模型并导出为 ONNX\n",
    "ort_model = ORTModelForSequenceClassification.from_pretrained(model_checkpoint, export=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 保存 ONNX 模型和分词器\n",
    "ort_model.save_pretrained(save_directory)\n",
    "tokenizer.save_pretrained(save_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0798bf4c",
   "metadata": {},
   "source": [
    "\n",
    "### 导出不支持架构的模型\n",
    "\n",
    "如果你想通过添加对当前无法导出的模型的支持来贡献，你应该首先检查它是否在 [`optimum.exporters.onnx`](https://huggingface.co/docs/optimum/exporters/onnx/overview) 中受支持，如果未受支持，请直接[贡献到 🤗 Optimum](https://huggingface.co/docs/optimum/exporters/onnx/usage_guides/contribute)。\n",
    "\n",
    "### 通过 transformers.onnx 导出模型\n",
    "\n",
    "`transformers.onnx` 不再维护，请按照上述方法使用 🤗 Optimum 导出模型。此部分将在未来版本中删除。\n",
    "\n",
    "要将 🤗 Transformers 模型通过 `transformers.onnx` 导出为 ONNX，请安装额外的依赖项：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c94a66",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install transformers[onnx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed16618c",
   "metadata": {},
   "source": [
    "\n",
    "使用 `transformers.onnx` 包作为 Python 模块，使用现成的配置导出检查点：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af0152d",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m transformers.onnx --model=distilbert/distilbert-base-uncased onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcea9cff",
   "metadata": {},
   "source": [
    "\n",
    "这将导出由 `--model` 参数定义的检查点的 ONNX 图。传递 🤗 Hub 上的任何检查点或本地存储的检查点。生成的 `model.onnx` 文件可以在支持 ONNX 标准的许多加速器上运行。例如，使用 ONNX Runtime 加载和运行模型，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09036113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from onnxruntime import InferenceSession\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
    "session = InferenceSession(\"onnx/model.onnx\")\n",
    "# ONNX Runtime 期望输入为 NumPy 数组\n",
    "inputs = tokenizer(\"Using DistilBERT with ONNX Runtime!\", return_tensors=\"np\")\n",
    "outputs = session.run(output_names=[\"last_hidden_state\"], input_feed=dict(inputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfc21396",
   "metadata": {},
   "source": [
    "\n",
    "所需的输出名称（如 `[\"last_hidden_state\"]`）可以通过查看每个模型的 ONNX 配置来获得。例如，对于 DistilBERT，我们有：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c5f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.models.distilbert import DistilBertConfig, DistilBertOnnxConfig\n",
    "\n",
    "config = DistilBertConfig()\n",
    "onnx_config = DistilBertOnnxConfig(config)\n",
    "print(list(onnx_config.outputs.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf901d6",
   "metadata": {},
   "source": [
    "\n",
    "对于 Hub 上的 TensorFlow 检查点，过程是相同的。例如，导出纯 TensorFlow 检查点，如下所示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203c9afa",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m transformers.onnx --model=keras-io/transformers-qa onnx/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864bf284",
   "metadata": {},
   "source": [
    "\n",
    "要导出本地存储的模型，请将模型的权重和分词器文件保存在同一目录（例如 `local-pt-checkpoint`）中，然后通过将 `transformers.onnx` 包的 `--model` 参数指向所需目录来将其导出为 ONNX：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a61cb51",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "python -m transformers.onnx --model=local-pt-checkpoint onnx/"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
