{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad4fd05",
   "metadata": {},
   "source": [
    "# 使用 🤗 Tokenizers 中的分词器\n",
    "\n",
    "[PreTrainedTokenizerFast](/docs/transformers/v4.46.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 依赖于 [🤗 Tokenizers](https://huggingface.co/docs/tokenizers) 库。从 🤗 Tokenizers 库中获取的分词器可以非常简单地加载到 🤗 Transformers 中。\n",
    "\n",
    "在详细介绍之前，我们先通过几行代码创建一个简单的分词器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# 创建一个 BPE 分词器，并指定未知令牌 (unk_token) 为 \"[UNK]\"\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# 创建一个 BPE 训练器，并定义特殊令牌\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# 设置预分词器为空白分词器\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# 定义要训练的文件列表\n",
    "files = [...]\n",
    "\n",
    "# 使用训练器训练分词器\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d016ce",
   "metadata": {},
   "source": [
    "\n",
    "现在我们已经训练了一个分词器，可以在当前运行时继续使用它，或者将其保存为 JSON 文件以供将来重用。\n",
    "\n",
    "## 直接从分词器对象加载\n",
    "\n",
    "让我们看看如何在 🤗 Transformers 库中利用这个分词器对象。[PreTrainedTokenizerFast](/docs/transformers/v4.46.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 类允许通过传递已实例化的分词器对象来轻松实例化：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 将分词器对象传递给 PreTrainedTokenizerFast\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff2c41",
   "metadata": {},
   "source": [
    "\n",
    "现在，这个对象可以使用 🤗 Transformers 分词器共享的所有方法！更多详细信息请参阅 [分词器页面](main_classes/tokenizer)。\n",
    "\n",
    "## 从 JSON 文件加载\n",
    "\n",
    "为了从 JSON 文件加载分词器，我们首先需要保存分词器：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aa7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将分词器保存为 JSON 文件\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303ee58",
   "metadata": {},
   "source": [
    "\n",
    "保存文件的路径可以传递给 [PreTrainedTokenizerFast](/docs/transformers/v4.46.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) 初始化方法，使用 `tokenizer_file` 参数：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# 从 JSON 文件加载分词器\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa275b",
   "metadata": {},
   "source": [
    "\n",
    "现在，这个对象可以使用 🤗 Transformers 分词器共享的所有方法！更多详细信息请参阅 [分词器页面](main_classes/tokenizer)。\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
