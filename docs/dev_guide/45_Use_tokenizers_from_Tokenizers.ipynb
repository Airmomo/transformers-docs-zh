{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ad4fd05",
   "metadata": {},
   "source": [
    "# ä½¿ç”¨ ğŸ¤— Tokenizers ä¸­çš„åˆ†è¯å™¨\n",
    "\n",
    "[PreTrainedTokenizerFast](/docs/transformers/v4.46.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) ä¾èµ–äº [ğŸ¤— Tokenizers](https://huggingface.co/docs/tokenizers) åº“ã€‚ä» ğŸ¤— Tokenizers åº“ä¸­è·å–çš„åˆ†è¯å™¨å¯ä»¥éå¸¸ç®€å•åœ°åŠ è½½åˆ° ğŸ¤— Transformers ä¸­ã€‚\n",
    "\n",
    "åœ¨è¯¦ç»†ä»‹ç»ä¹‹å‰ï¼Œæˆ‘ä»¬å…ˆé€šè¿‡å‡ è¡Œä»£ç åˆ›å»ºä¸€ä¸ªç®€å•çš„åˆ†è¯å™¨ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b4b14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import BPE\n",
    "from tokenizers.trainers import BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª BPE åˆ†è¯å™¨ï¼Œå¹¶æŒ‡å®šæœªçŸ¥ä»¤ç‰Œ (unk_token) ä¸º \"[UNK]\"\n",
    "tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "\n",
    "# åˆ›å»ºä¸€ä¸ª BPE è®­ç»ƒå™¨ï¼Œå¹¶å®šä¹‰ç‰¹æ®Šä»¤ç‰Œ\n",
    "trainer = BpeTrainer(special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"])\n",
    "\n",
    "# è®¾ç½®é¢„åˆ†è¯å™¨ä¸ºç©ºç™½åˆ†è¯å™¨\n",
    "tokenizer.pre_tokenizer = Whitespace()\n",
    "\n",
    "# å®šä¹‰è¦è®­ç»ƒçš„æ–‡ä»¶åˆ—è¡¨\n",
    "files = [...]\n",
    "\n",
    "# ä½¿ç”¨è®­ç»ƒå™¨è®­ç»ƒåˆ†è¯å™¨\n",
    "tokenizer.train(files, trainer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d016ce",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨æˆ‘ä»¬å·²ç»è®­ç»ƒäº†ä¸€ä¸ªåˆ†è¯å™¨ï¼Œå¯ä»¥åœ¨å½“å‰è¿è¡Œæ—¶ç»§ç»­ä½¿ç”¨å®ƒï¼Œæˆ–è€…å°†å…¶ä¿å­˜ä¸º JSON æ–‡ä»¶ä»¥ä¾›å°†æ¥é‡ç”¨ã€‚\n",
    "\n",
    "## ç›´æ¥ä»åˆ†è¯å™¨å¯¹è±¡åŠ è½½\n",
    "\n",
    "è®©æˆ‘ä»¬çœ‹çœ‹å¦‚ä½•åœ¨ ğŸ¤— Transformers åº“ä¸­åˆ©ç”¨è¿™ä¸ªåˆ†è¯å™¨å¯¹è±¡ã€‚[PreTrainedTokenizerFast](/docs/transformers/v4.46.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) ç±»å…è®¸é€šè¿‡ä¼ é€’å·²å®ä¾‹åŒ–çš„åˆ†è¯å™¨å¯¹è±¡æ¥è½»æ¾å®ä¾‹åŒ–ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f57b9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# å°†åˆ†è¯å™¨å¯¹è±¡ä¼ é€’ç»™ PreTrainedTokenizerFast\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aff2c41",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥ä½¿ç”¨ ğŸ¤— Transformers åˆ†è¯å™¨å…±äº«çš„æ‰€æœ‰æ–¹æ³•ï¼æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜… [åˆ†è¯å™¨é¡µé¢](main_classes/tokenizer)ã€‚\n",
    "\n",
    "## ä» JSON æ–‡ä»¶åŠ è½½\n",
    "\n",
    "ä¸ºäº†ä» JSON æ–‡ä»¶åŠ è½½åˆ†è¯å™¨ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¿å­˜åˆ†è¯å™¨ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9aa7c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°†åˆ†è¯å™¨ä¿å­˜ä¸º JSON æ–‡ä»¶\n",
    "tokenizer.save(\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303ee58",
   "metadata": {},
   "source": [
    "\n",
    "ä¿å­˜æ–‡ä»¶çš„è·¯å¾„å¯ä»¥ä¼ é€’ç»™ [PreTrainedTokenizerFast](/docs/transformers/v4.46.2/en/main_classes/tokenizer#transformers.PreTrainedTokenizerFast) åˆå§‹åŒ–æ–¹æ³•ï¼Œä½¿ç”¨ `tokenizer_file` å‚æ•°ï¼š\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21b1146a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "# ä» JSON æ–‡ä»¶åŠ è½½åˆ†è¯å™¨\n",
    "fast_tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"tokenizer.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70aa275b",
   "metadata": {},
   "source": [
    "\n",
    "ç°åœ¨ï¼Œè¿™ä¸ªå¯¹è±¡å¯ä»¥ä½¿ç”¨ ğŸ¤— Transformers åˆ†è¯å™¨å…±äº«çš„æ‰€æœ‰æ–¹æ³•ï¼æ›´å¤šè¯¦ç»†ä¿¡æ¯è¯·å‚é˜… [åˆ†è¯å™¨é¡µé¢](main_classes/tokenizer)ã€‚\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
